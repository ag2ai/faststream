# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/011_ConsumerLoop.ipynb.

# %% auto 0
__all__ = ['logger', 'AsyncConsume', 'AsyncConsumeMeta', 'SyncConsume', 'SyncConsumeMeta', 'ConsumeCallable', 'EventMetadata',
           'sanitize_kafka_config', 'aiokafka_consumer_loop']

# %% ../../nbs/011_ConsumerLoop.ipynb 1
from asyncio import iscoroutinefunction  # do not use the version from inspect
from typing import *
from functools import wraps, partial
from dataclasses import dataclass

import anyio
import asyncer
from aiokafka import AIOKafkaConsumer
from aiokafka.structs import ConsumerRecord, TopicPartition
from anyio.streams.memory import MemoryObjectReceiveStream
from pydantic import BaseModel
from pydantic.main import ModelMetaclass

from .logger import get_logger
from .meta import delegates, export

# %% ../../nbs/011_ConsumerLoop.ipynb 5
logger = get_logger(__name__)

# %% ../../nbs/011_ConsumerLoop.ipynb 8
@dataclass
@export("fastkafka")
class EventMetadata:
    """A class for encapsulating Kafka record metadata.

    Args:
        topic: The topic this record is received from
        partition: The partition from which this record is received
        offset: The position of this record in the corresponding Kafka partition
        timestamp: The timestamp of this record
        timestamp_type: The timestamp type of this record
        key: The key (or `None` if no key is specified)
        value: The value
        serialized_key_size: The size of the serialized, uncompressed key in bytes
        serialized_value_size: The size of the serialized, uncompressed value in bytes
        headers: The headers
    """

    topic: str
    partition: int
    offset: int
    timestamp: int
    timestamp_type: int
    key: Optional[bytes]
    value: Optional[bytes]
    checksum: int
    serialized_key_size: int
    serialized_value_size: int
    headers: Sequence[Tuple[str, bytes]]

# %% ../../nbs/011_ConsumerLoop.ipynb 9
AsyncConsume = Callable[[BaseModel], Awaitable[None]]
AsyncConsumeMeta = Callable[[BaseModel, EventMetadata], Awaitable[None]]
SyncConsume = Callable[[BaseModel], None]
SyncConsumeMeta = Callable[[BaseModel, EventMetadata], None]

ConsumeCallable = Union[AsyncConsume, AsyncConsumeMeta, SyncConsume, SyncConsumeMeta]

# %% ../../nbs/011_ConsumerLoop.ipynb 10
def _callback_parameters_wrapper(
    callback: Union[AsyncConsume, AsyncConsumeMeta]
) -> AsyncConsumeMeta:
    """Wraps an async callback and filters the arguments to pass based on if the function accepts EventMetadata as argument

    Args:
        callback: async callable that will be wrapped

    Returns:
        Wrapped callback with filtered params
    """

    async def _params_wrap(
        msg: BaseModel,
        meta: EventMetadata,
        callback: Union[AsyncConsume, AsyncConsumeMeta] = callback,
    ) -> None:
        types = list(get_type_hints(callback).values())
        args: List[Union[BaseModel, EventMetadata]] = [msg]
        if EventMetadata in types:
            args.insert(types.index(EventMetadata), meta)
        await callback(*args)  # type: ignore

    return _params_wrap

# %% ../../nbs/011_ConsumerLoop.ipynb 14
def _create_safe_callback(
    callback: Union[AsyncConsume, AsyncConsumeMeta]
) -> AsyncConsumeMeta:
    """Wraps an async callback into a safe callback that catches any Exception and loggs them as warnings

    Args:
        callback: async callable that will be wrapped into a safe callback

    Returns:
        Wrapped callback into a safe callback that handles exceptions
    """

    async def _safe_callback(
        msg: BaseModel,
        meta: EventMetadata,
        callback: Union[AsyncConsume, AsyncConsumeMeta] = callback,
    ) -> None:
        try:
            await _callback_parameters_wrapper(callback)(msg, meta)
        except Exception as e:
            logger.warning(
                f"_safe_callback(): exception caugth {e.__repr__()} while awaiting '{callback}({msg})'"
            )

    return _safe_callback

# %% ../../nbs/011_ConsumerLoop.ipynb 19
def _prepare_callback(callback: ConsumeCallable) -> AsyncConsumeMeta:
    """
    Prepares a callback to be used in the consumer loop.
        1. If callback is sync, asyncify it
        2. Wrap the callback into a safe callback for exception handling

    Params:
        callback: async callable that will be prepared for use in consumer

    Returns:
        Prepared callback
    """
    async_callback: Union[AsyncConsume, AsyncConsumeMeta] = (
        callback if iscoroutinefunction(callback) else asyncer.asyncify(callback)  # type: ignore
    )
    return _create_safe_callback(async_callback)

# %% ../../nbs/011_ConsumerLoop.ipynb 21
async def _stream_msgs(  # type: ignore
    msgs: Dict[TopicPartition, bytes],
    send_stream: anyio.streams.memory.MemoryObjectSendStream[Any],
) -> None:
    """
    Decodes and streams the message and topic to the send_stream.

    Params:
        msgs:
        send_stream:
    """
    for topic_partition, topic_msgs in msgs.items():
        topic = topic_partition.topic
        try:
            await send_stream.send(topic_msgs)
        except Exception as e:
            logger.warning(
                f"_stream_msgs(): Unexpected exception '{e.__repr__()}' caught and ignored for topic='{topic_partition.topic}', partition='{topic_partition.partition}' and messages: {topic_msgs!r}"
            )


def _decode_streamed_msgs(  # type: ignore
    msgs: List[ConsumerRecord], msg_type: BaseModel
) -> List[BaseModel]:
    decoded_msgs = [msg_type.parse_raw(msg.value.decode("utf-8")) for msg in msgs]
    return decoded_msgs

# %% ../../nbs/011_ConsumerLoop.ipynb 27
async def _streamed_records(
    receive_stream: MemoryObjectReceiveStream,
) -> AsyncGenerator[Any, Any]:
    async for records_per_topic in receive_stream:
        for records in records_per_topic:
            for record in records:
                yield record


@delegates(AIOKafkaConsumer.getmany)
async def _aiokafka_consumer_loop(  # type: ignore
    consumer: AIOKafkaConsumer,
    *,
    topic: str,
    decoder_fn: Callable[[bytes, ModelMetaclass], Any],
    callback: ConsumeCallable,
    max_buffer_size: int = 100_000,
    msg_type: Type[BaseModel],
    is_shutting_down_f: Callable[[], bool],
    **kwargs: Any,
) -> None:
    """
    Consumer loop for infinite pooling of the AIOKafka consumer for new messages. Calls consumer.getmany()
    and after the consumer return messages or times out, messages are decoded and streamed to defined callback.

    Params:
        topic: Topic to subscribe
        decoder_fn: Function to decode the messages consumed from the topic
        callbacks: Dict of callbacks mapped to their respective topics
        timeout_ms: Time to timeut the getmany request by the consumer
        max_buffer_size: Maximum number of unconsumed messages in the callback buffer
        msg_types: Dict of message types mapped to their respective topics
        is_shutting_down_f: Function for controlling the shutdown of consumer loop
    """

    prepared_callback = _prepare_callback(callback)

    async def process_message_callback(
        receive_stream: MemoryObjectReceiveStream[Any],
        callback: Callable[
            [BaseModel, EventMetadata], Awaitable[None]
        ] = prepared_callback,
        msg_type: Type[BaseModel] = msg_type,
        topic: str = topic,
        decoder_fn: Callable[[bytes, ModelMetaclass], Any] = decoder_fn,
    ) -> None:
        async with receive_stream:
            try:
                async for record in _streamed_records(receive_stream):
                    try:
                        msg = record.value
                        decoded_msg = decoder_fn(msg, msg_type)
                        await callback(
                            decoded_msg,
                            EventMetadata(
                                topic=record.topic,
                                partition=record.partition,
                                offset=record.offset,
                                timestamp=record.timestamp,
                                timestamp_type=record.timestamp_type,
                                value=record.value,
                                checksum=record.checksum,
                                key=record.key,
                                serialized_key_size=record.serialized_key_size,
                                serialized_value_size=record.serialized_value_size,
                                headers=record.headers,
                            ),
                        )
                    except Exception as e:
                        logger.warning(
                            f"process_message_callback(): Unexpected exception '{e.__repr__()}' caught and ignored for topic='{topic}' and message: {msg}"
                        )
            except Exception as e:
                logger.warning(
                    f"process_message_callback(): Unexpected exception '{e.__repr__()}' caught and ignored for topic='{topic}'"
                )

    send_stream, receive_stream = anyio.create_memory_object_stream(
        max_buffer_size=max_buffer_size
    )

    async with anyio.create_task_group() as tg:
        tg.start_soon(process_message_callback, receive_stream)
        async with send_stream:
            while not is_shutting_down_f():
                msgs = await consumer.getmany(**kwargs)
                try:
                    await send_stream.send(msgs.values())
                except Exception as e:
                    logger.warning(
                        f"_aiokafka_consumer_loop(): Unexpected exception '{e}' caught and ignored for messages: {msgs}"
                    )
            logger.info(
                f"_aiokafka_consumer_loop(): Consumer loop shutting down, waiting for send_stream to drain..."
            )

# %% ../../nbs/011_ConsumerLoop.ipynb 32
def sanitize_kafka_config(**kwargs: Any) -> Dict[str, Any]:
    """Sanitize Kafka config"""
    return {k: "*" * len(v) if "pass" in k.lower() else v for k, v in kwargs.items()}

# %% ../../nbs/011_ConsumerLoop.ipynb 34
@delegates(AIOKafkaConsumer)
@delegates(_aiokafka_consumer_loop, keep=True)
async def aiokafka_consumer_loop(
    topic: str,
    decoder_fn: Callable[[bytes, ModelMetaclass], Any],
    *,
    timeout_ms: int = 100,
    max_buffer_size: int = 100_000,
    callback: ConsumeCallable,
    msg_type: Type[BaseModel],
    is_shutting_down_f: Callable[[], bool],
    **kwargs: Any,
) -> None:
    """Consumer loop for infinite pooling of the AIOKafka consumer for new messages. Creates and starts AIOKafkaConsumer
    and runs _aio_kafka_consumer loop fo infinite poling of the consumer for new messages.

    Args:
        topic: name of the topic to subscribe to
        decoder_fn: Function to decode the messages consumed from the topic
        callback: callback function to be called after decoding and parsing a consumed message
        timeout_ms: Time to timeut the getmany request by the consumer
        max_buffer_size: Maximum number of unconsumed messages in the callback buffer
        msg_type: Type with `parse_json` method used for parsing a decoded message
        is_shutting_down_f: Function for controlling the shutdown of consumer loop
    """
    logger.info(f"aiokafka_consumer_loop() starting...")
    try:
        consumer = AIOKafkaConsumer(
            **kwargs,
        )
        logger.info(
            f"aiokafka_consumer_loop(): Consumer created using the following parameters: {sanitize_kafka_config(**kwargs)}"
        )

        await consumer.start()
        logger.info("aiokafka_consumer_loop(): Consumer started.")
        consumer.subscribe([topic])
        logger.info("aiokafka_consumer_loop(): Consumer subscribed.")

        try:
            await _aiokafka_consumer_loop(
                consumer=consumer,
                topic=topic,
                decoder_fn=decoder_fn,
                max_buffer_size=max_buffer_size,
                timeout_ms=timeout_ms,
                callback=callback,
                msg_type=msg_type,
                is_shutting_down_f=is_shutting_down_f,
            )
        finally:
            await consumer.stop()
            logger.info(f"aiokafka_consumer_loop(): Consumer stopped.")
            logger.info(f"aiokafka_consumer_loop() finished.")
    except Exception as e:
        logger.error(
            f"aiokafka_consumer_loop(): unexpected exception raised: '{e.__repr__()}'"
        )
        raise e
