# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/998_kafka_broker_helpers.ipynb.

# %% auto 0
__all__ = ['get_zookeeper_config_string', 'write_config', 'zookeeper', 'get_kafka_config_string', 'kafka_broker',
           'create_kafka_test_env']

# %% ../nbs/998_kafka_broker_helpers.ipynb 1
import asyncio
from contextlib import asynccontextmanager, contextmanager
from time import sleep
from tempfile import TemporaryDirectory, NamedTemporaryFile
from pathlib import Path
from fastcore.meta import delegates
from typing import *

import asyncer

from .server import terminate_asyncio_process

# %% ../nbs/998_kafka_broker_helpers.ipynb 4
def get_zookeeper_config_string(
    data_dir: str,  # the directory where the snapshot is stored.
    client_port: int = 2181,  # the port at which the clients will connect
    maxClientCnxns: int = 0,  # disable the per-ip limit on the number of connections since this is a non-production config
    admin_enableServer: bool = False,  # Disable the adminserver by default to avoid port conflicts.
    admin_serverPort: int = 8080,  # Set the port to something non-conflicting if choosing to enable this
) -> str:
    zookeeper_config = f"""dataDir={data_dir}
clientPort={client_port}
maxClientCnxns={maxClientCnxns}
admin.enableServer={"true" if admin_enableServer else "false"}
admin.serverPort={admin_serverPort}
"""

    return zookeeper_config

# %% ../nbs/998_kafka_broker_helpers.ipynb 6
@contextmanager
def write_config(config: str):
    try:
        with TemporaryDirectory() as config_dir:
            temp_config = Path(config_dir) / "configuration.config"
            with open(temp_config, "w") as config_file:
                config_file.write(config)
            yield temp_config
    except Exception as e:
        print(f"Exception raised {e=}")

# %% ../nbs/998_kafka_broker_helpers.ipynb 8
@asynccontextmanager
@delegates(get_zookeeper_config_string)
async def zookeeper(
    zookeeper_script_path: str = "/work/kafka_2.13-3.3.1/bin/zookeeper-server-start.sh",
    **kwargs,
) -> Generator[asyncio.subprocess.Process, None, None]:
    config_generator = write_config(get_zookeeper_config_string(**kwargs))
    config_path = config_generator.__enter__()
    proc = await asyncio.create_subprocess_exec(
        zookeeper_script_path,
        config_path,
        stdout=asyncio.subprocess.PIPE,
        stdin=asyncio.subprocess.PIPE,
    )
    try:
        yield proc
    except Exception as e:
        print(f"Exception raised {e=}")
    finally:
        await terminate_asyncio_process(proc)
        config_generator.__exit__(None, None, None)

# %% ../nbs/998_kafka_broker_helpers.ipynb 10
def get_kafka_config_string(
    log_dirs: str, zookeeper_connect: str = "localhost:2181", listener_port: int = 9092
) -> str:
    kafka_config = f"""broker.id=0

############################# Socket Server Settings #############################

# The address the socket server listens on. If not configured, the host name will be equal to the value of
# java.net.InetAddress.getCanonicalHostName(), with PLAINTEXT listener name, and port 9092.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
listeners=PLAINTEXT://:{listener_port}

# Listener name, hostname and port the broker will advertise to clients.
# If not set, it uses the value for "listeners".
#advertised.listeners=PLAINTEXT://your.host.name:9092

# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3

# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600


############################# Log Basics #############################

# A comma separated list of directories under which to store log files
log.dirs={log_dirs}

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=1

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

# The number of messages to accept before forcing a flush of data to disk
log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
log.flush.interval.ms=1000

# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according to the retention policies
log.retention.check.interval.ms=300000

# Zookeeper connection string (see zookeeper docs for details).
zookeeper.connect={zookeeper_connect}

# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=18000

# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
group.initial.rebalance.delay.ms=0
"""

    return kafka_config

# %% ../nbs/998_kafka_broker_helpers.ipynb 11
@asynccontextmanager
@delegates(get_kafka_config_string)
async def kafka_broker(
    kafka_script_path="/work/kafka_2.13-3.3.1/bin/kafka-server-start.sh", **kwargs
) -> Generator[asyncio.subprocess.Process, None, None]:
    config_generator = write_config(get_kafka_config_string(**kwargs))
    config_path = config_generator.__enter__()
    proc = await asyncio.create_subprocess_exec(
        kafka_script_path,
        config_path,
        stdout=asyncio.subprocess.PIPE,
        stdin=asyncio.subprocess.PIPE,
    )
    try:
        yield proc
    except Exception as e:
        print(f"Exception raised {e=}")
    finally:
        await terminate_asyncio_process(proc)
        config_generator.__exit__(None, None, None)

# %% ../nbs/998_kafka_broker_helpers.ipynb 13
@asynccontextmanager
async def create_kafka_test_env(zookeeper_port: int, kafka_port: int) -> str:
    try:
        with TemporaryDirectory() as tmp:
            tmp_dir_path = Path(tmp)
            async with zookeeper(
                data_dir=tmp_dir_path / "zookeeper", client_port=zookeeper_port
            ) as zookeeper_proc:
                sleep(5)
                async with kafka_broker(
                    log_dirs=tmp_dir_path / "kafka",
                    listener_port=kafka_port,
                    zookeeper_connect=f"localhost:{zookeeper_port}",
                ) as kafka_broker_proc:
                    sleep(5)
                    yield f"localhost:{kafka_port}"
    except Exception as e:
        print(f"Exception raised {e=}")
    finally:
        pass
