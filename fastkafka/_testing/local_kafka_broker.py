# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/001_LocalKafkaBroker.ipynb.

# %% auto 0
__all__ = ['logger', 'create_consumer_record', 'ConsumerMetadata', 'LocalKafkaBroker']

# %% ../../nbs/001_LocalKafkaBroker.ipynb 1
import uuid
from collections import namedtuple
from dataclasses import dataclass
from contextlib import contextmanager
import asyncio

from typing import *
import fastkafka._application.app
from aiokafka import AIOKafkaConsumer, AIOKafkaProducer
from aiokafka.structs import ConsumerRecord, TopicPartition

import fastkafka._application.app
from .._components.meta import copy_func, patch, delegates, classcontextmanager
from .._components.logger import get_logger

# %% ../../nbs/001_LocalKafkaBroker.ipynb 3
logger = get_logger(__name__)

# %% ../../nbs/001_LocalKafkaBroker.ipynb 5
def create_consumer_record(topic: str, msg: bytes) -> ConsumerRecord:  # type: ignore
    record = ConsumerRecord(
        topic=topic,
        partition=0,
        offset=0,
        timestamp=0,
        timestamp_type=0,
        key=None,
        value=msg,
        checksum=0,
        serialized_key_size=0,
        serialized_value_size=0,
        headers=[],
    )
    return record

# %% ../../nbs/001_LocalKafkaBroker.ipynb 7
@dataclass
class ConsumerMetadata:
    topic: str
    offset: int

# %% ../../nbs/001_LocalKafkaBroker.ipynb 9
@classcontextmanager()
class LocalKafkaBroker:
    def __init__(self, topics: List[str]):
        self.data: Dict[str, List[ConsumerRecord]] = {topic: list() for topic in topics}  # type: ignore
        self.consumers_metadata: Dict[str, List[ConsumerMetadata]] = {}
        self.is_started: bool = False

    def connect(self) -> uuid.UUID:
        return uuid.uuid4()

    def subscribe(self, actor_id: str, *, auto_offest_reset: str, topic: str) -> None:
        consumer_metadata = self.consumers_metadata.get(actor_id, list())
        consumer_metadata.append(
            ConsumerMetadata(
                topic, len(self.data[topic]) if auto_offest_reset == "latest" else 0
            )
        )
        self.consumers_metadata[actor_id] = consumer_metadata

    def unsubscribe(self, actor_id: str) -> None:
        try:
            del self.consumers_metadata[actor_id]
        except KeyError:
            logger.warning(f"No subscription with {actor_id=} found!")

    def produce(
        self, actor_id: str, *, topic: str, msg: bytes, key: Optional[bytes]
    ) -> ConsumerRecord:  # type: ignore
        record = create_consumer_record(topic, msg)
        self.data[topic].append(record)
        return record

    def consume(  # type: ignore
        self, actor_id: str
    ) -> Dict[TopicPartition, List[ConsumerRecord]]:
        msgs: Dict[TopicPartition, List[ConsumerRecord]] = {}  # type: ignore

        try:
            consumer_metadata = self.consumers_metadata[actor_id]
        except KeyError:
            logger.warning(f"No subscription with {actor_id=} found!")
            return msgs

        for metadata in consumer_metadata:
            try:
                msgs[TopicPartition(metadata.topic, 0)] = self.data[metadata.topic][
                    metadata.offset :
                ]
                metadata.offset = len(self.data[metadata.topic])
            except KeyError:
                raise RuntimeError(
                    f"{metadata.topic=} not found, did you pass it to LocalKafkaBroker on init to be created?"
                )
        return msgs

    def lifecycle(self) -> "LocalKafkaBroker":
        raise NotImplementedError()

    async def _start(self) -> str:
        logger.info("LocalKafkaBroker._start() called")
        self.__enter__()
        return "localbroker:0"

    async def _stop(self) -> None:
        logger.info("LocalKafkaBroker._stop() called")
        self.__exit__()

# %% ../../nbs/001_LocalKafkaBroker.ipynb 14
@patch
@delegates(AIOKafkaConsumer.start)
async def start(self: ConsumerMock, **kwargs: Any) -> None:
    logger.info("AIOKafkaConsumer patched start() called()")
    if self.id is not None:
        raise RuntimeError(
            "Consumer start() already called! Run consumer stop() before running start() again"
        )
    self.id = self.broker.connect()

# %% ../../nbs/001_LocalKafkaBroker.ipynb 17
@patch
@delegates(AIOKafkaConsumer.subscribe)
def subscribe(self: ConsumerMock, topics: List[str], **kwargs: Any) -> None:
    logger.info("AIOKafkaConsumer patched subscribe() called")
    if self.id is None:
        raise RuntimeError("Consumer start() not called! Run consumer start() first")
    logger.info(f"AIOKafkaConsumer.subscribe(), subscribing to: {topics}")
    [
        self.broker.subscribe(
            self.id, topic=topic, auto_offest_reset=self.auto_offset_reset
        )
        for topic in topics
    ]

# %% ../../nbs/001_LocalKafkaBroker.ipynb 20
@patch
@delegates(AIOKafkaConsumer.stop)
async def stop(self: ConsumerMock, **kwargs: Any) -> None:  # type: ignore
    logger.info("AIOKafkaConsumer patched stop() called")
    if self.id is None:
        raise RuntimeError("Consumer start() not called! Run consumer start() first")
    self.broker.unsubscribe(self.id)

# %% ../../nbs/001_LocalKafkaBroker.ipynb 23
@patch
@delegates(AIOKafkaConsumer.getmany)
async def getmany(
    self: ConsumerMock, **kwargs: Any
) -> Dict[TopicPartition, List[ConsumerRecord]]:
    return self.broker.consume(self.id)  # type: ignore

# %% ../../nbs/001_LocalKafkaBroker.ipynb 28
@patch
@delegates(AIOKafkaProducer.start)
async def start(self: ProducerMock, **kwargs: Any) -> None:
    logger.info("AIOKafkaProducer patched start() called()")
    if self.id is not None:
        raise RuntimeError(
            "Producer start() already called! Run producer stop() before running start() again"
        )
    self.id = self.broker.connect()

# %% ../../nbs/001_LocalKafkaBroker.ipynb 31
@patch
@delegates(AIOKafkaProducer.stop)
async def stop(self: ProducerMock, **kwargs: Any) -> None:
    logger.info("AIOKafkaProducer patched stop() called")
    if self.id is None:
        raise RuntimeError("Producer start() not called! Run producer start() first")

# %% ../../nbs/001_LocalKafkaBroker.ipynb 34
@patch
@delegates(AIOKafkaProducer.send)
async def send(
    self: ProducerMock,
    topic: str,
    msg: bytes,
    key: Optional[bytes] = None,
    **kwargs: Any,
) -> None:
    if self.id is None:
        raise RuntimeError("Producer start() not called! Run producer start() first")
    record = self.broker.produce(self.id, topic=topic, msg=msg, key=key)

    async def _f(record: ConsumerRecord = record) -> ConsumerRecord:
        return record

    return asyncio.create_task(_f())

# %% ../../nbs/001_LocalKafkaBroker.ipynb 37
@patch
@contextmanager
def lifecycle(self: LocalKafkaBroker) -> None:
    logger.info(
        "LocalKafkaProducer._patch_consumers_and_producers(): Patching consumers and producers!"
    )
    try:
        logger.info("Local kafka broker starting")
        old_consumer = fastkafka._application.app.AIOKafkaConsumer
        old_producer = fastkafka._application.app.AIOKafkaProducer
        fastkafka._application.app.AIOKafkaConsumer = ConsumerMock(self)
        fastkafka._application.app.AIOKafkaProducer = ProducerMock(self)
        self.is_started = True
        yield self
    finally:
        logger.info("Local kafka broker stopping")
        fastkafka._application.app.AIOKafkaConsumer = old_consumer
        fastkafka._application.app.AIOKafkaProducer = old_producer
        self.is_started = False
