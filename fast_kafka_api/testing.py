# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/999_Test_Utils.ipynb.

# %% auto 0
__all__ = ['logger', 'kafka_server_url', 'kafka_server_port', 'kafka_config', 'in_notebook', 'true_after', 'create_testing_topic',
           'create_and_fill_testing_topic', 'nb_safe_seed', 'mock_AIOKafkaProducer_send', 'change_dir',
           'run_script_and_cancel']

# %% ../nbs/999_Test_Utils.ipynb 1
def in_notebook():
    try:
        from IPython import get_ipython

        if "IPKernelApp" not in get_ipython().config:
            return False
    except ImportError:
        return False
    except AttributeError:
        return False
    return True

# %% ../nbs/999_Test_Utils.ipynb 3
import asyncio
import contextlib
import hashlib
import os
import random
import shlex
import textwrap

# [B404:blacklist] Consider possible security implications associated with the subprocess module.
import subprocess  # nosec
import time
import unittest
import unittest.mock
from contextlib import asynccontextmanager, contextmanager
from fastcore.meta import delegates
from datetime import datetime, timedelta
from pathlib import Path
from tempfile import TemporaryDirectory
from typing import (
    Any,
    Callable,
    Dict,
    Generator,
    List,
    Optional,
    Tuple,
    AsyncIterator,
    Union,
)
from pydantic import BaseModel

from aiokafka import AIOKafkaConsumer, AIOKafkaProducer
from confluent_kafka.admin import AdminClient, NewTopic

if in_notebook():
    from tqdm.notebook import tqdm, trange
else:
    from tqdm import tqdm, trange

from ._components.logger import get_logger, supress_timestamps
from ._components.helpers import delegates_using_docstring
from .helpers import create_admin_client, create_missing_topics

# %% ../nbs/999_Test_Utils.ipynb 6
logger = get_logger(__name__)

# %% ../nbs/999_Test_Utils.ipynb 8
kafka_server_url = (
    os.environ["KAFKA_HOSTNAME"] if "KAFKA_HOSTNAME" in os.environ else "localhost"
)
kafka_server_port = os.environ["KAFKA_PORT"] if "KAFKA_PORT" in os.environ else "9092"

kafka_config = {
    "bootstrap.servers": f"{kafka_server_url}:{kafka_server_port}",
    # "group.id": f"{kafka_server_url}:{kafka_server_port}_group"
}

# %% ../nbs/999_Test_Utils.ipynb 9
def true_after(seconds: float) -> Callable[[], bool]:
    """Function returning True after a given number of seconds"""
    t = datetime.now()

    def _true_after(seconds: float = seconds, t: datetime = t) -> bool:
        return (datetime.now() - t) > timedelta(seconds=seconds)

    return _true_after

# %% ../nbs/999_Test_Utils.ipynb 11
@contextmanager
@delegates_using_docstring(create_missing_topics)
def create_testing_topic(
    topic_prefix: str, *, seed: Optional[int] = None, **kwargs: Dict[str, Any]
) -> Generator[str, None, None]:
    """Create testing topic

    Args:
        topic_prefix: topic name prefix which will be augumented with a randomly generated sufix
        seed: seed used to generate radnom sufix

    Returns:
        Generator returning the generated name of the created topic

    Example:
        ```python
        from os import environ
        from fast_kafka_api.testing import create_testing_topic, create_admin_client

        kafka_server_url = environ["KAFKA_HOSTNAME"]
        kafka_config = {"bootstrap.servers": f"{kafka_server_url}:9092"}

        with create_testing_topic("my_topic_", num_partitions=1, **aiokafka_config) as topic:
            # Check if topic is created and exists in topic list
            kafka_admin = create_admin_client(**aiokafka_config)
            existing_topics = kafka_admin.list_topics().topics.keys()
            assert topic in existing_topics

        # Check if topic is deleted after exiting context
        existing_topics = kafka_admin.list_topics().topics.keys()
        assert topic not in existing_topics
        ```
    """
    # create random topic name
    random.seed(seed)
    # [B311:blacklist] Standard pseudo-random generators are not suitable for security/cryptographic purposes.
    suffix = str(random.randint(0, 10**10))  # nosec

    topic = topic_prefix + suffix.zfill(3)

    # delete topic if it already exists
    admin = create_admin_client(**kwargs)
    existing_topics = admin.list_topics().topics.keys()
    if topic in existing_topics:
        logger.warning(f"topic {topic} exists, deleting it...")
        fs = admin.delete_topics(topics=[topic])
        results = {k: f.result() for k, f in fs.items()}
        while topic in admin.list_topics().topics.keys():
            time.sleep(1)
    try:
        # create topic if needed
        create_missing_topics([topic], **kwargs)
        while topic not in admin.list_topics().topics.keys():
            time.sleep(1)
        yield topic

    finally:
        pass
        # cleanup if needed again
        fs = admin.delete_topics(topics=[topic])
        while topic in admin.list_topics().topics.keys():
            time.sleep(1)

# %% ../nbs/999_Test_Utils.ipynb 14
@asynccontextmanager
@delegates(create_testing_topic)
@delegates_using_docstring(AIOKafkaProducer)
async def create_and_fill_testing_topic(
    msgs: List[bytes], *, seed: int, **kwargs: Dict[str, str]
) -> AsyncIterator[str]:
    """
    Args:
        msgs:
        seed:
    """

    with create_testing_topic("my_topic_", seed=seed, **kwargs) as topic:

        producer = AIOKafkaProducer(**kwargs)
        logger.info(f"Producer {producer} created.")

        await producer.start()
        logger.info(f"Producer {producer} started.")
        try:
            fx = [
                producer.send(
                    topic,
                    msg,
                    key=f"{i % 17}".encode("utf-8"),
                )
                for i, msg in enumerate(msgs)
            ]
            await producer.flush()
            sent_msgs = [await f for f in fx]
            msg_statuses = [await s for s in sent_msgs]
            logger.info(f"Sent messages: len(sent_msgs)={len(sent_msgs)}")

            yield topic
        finally:
            await producer.stop()
            logger.info(f"Producer {producer} stoped.")

# %% ../nbs/999_Test_Utils.ipynb 29
def nb_safe_seed(s: str) -> Callable[[int], int]:
    """Gets a unique seed function for a notebook

    Params:
        s: name of the notebook used to initialize the seed function

    Returns:
        A unique seed function
    """
    init_seed = int(hashlib.sha256(s.encode("utf-8")).hexdigest(), 16) % (10**8)

    def _get_seed(x: int = 0, *, init_seed: int = init_seed) -> int:
        return init_seed + x

    return _get_seed

# %% ../nbs/999_Test_Utils.ipynb 31
@contextmanager
def mock_AIOKafkaProducer_send() -> Generator[unittest.mock.Mock, None, None]:
    """Mocks **send** method of **AIOKafkaProducer**"""
    with unittest.mock.patch("__main__.AIOKafkaProducer.send") as mock:

        async def _f():
            pass

        mock.return_value = asyncio.create_task(_f())

        yield mock

# %% ../nbs/999_Test_Utils.ipynb 32
@contextlib.contextmanager
def change_dir(d: str) -> Generator[None, None, None]:
    curdir = os.getcwd()
    os.chdir(d)
    try:
        yield
    finally:
        os.chdir(curdir)

# %% ../nbs/999_Test_Utils.ipynb 34
def run_script_and_cancel(
    *, script: str, script_file: str, cmd: str, cancel_after: int
) -> Tuple[int, bytes]:
    with TemporaryDirectory() as d:
        consumer_script = Path(d) / script_file

        with open(consumer_script, "a+") as file:
            file.write(script)

        # os.chdir(d)
        with change_dir(d):
            proc = subprocess.Popen(  # nosec: [B603:subprocess_without_shell_equals_true] subprocess call - check for execution of untrusted input.
                shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT
            )
            time.sleep(cancel_after)
            proc.terminate()
            output, _ = proc.communicate()

        return (proc.returncode, output)
