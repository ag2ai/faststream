# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/000_FastKafkaAPI.ipynb.

# %% auto 0
__all__ = ['logger', 'KafkaErrorMsg', 'FastKafkaAPI', 'consumers_async_loop']

# %% ../nbs/000_FastKafkaAPI.ipynb 1
from typing import *
from typing import get_type_hints

from enum import Enum
from pathlib import Path
import json
import yaml
from copy import deepcopy
from os import environ
from datetime import datetime, timedelta
import tempfile
from fastcore.foundation import patch
from contextlib import contextmanager, asynccontextmanager
import time

import anyio
import asyncio
from asyncio import iscoroutinefunction  # do not use the version from inspect
import httpx
from fastapi import FastAPI
from fastapi import status, Depends, HTTPException, Request, Response
from fastapi.openapi.docs import get_swagger_ui_html, get_redoc_html
from fastapi.openapi.utils import get_openapi
from fastapi.responses import FileResponse, RedirectResponse
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from pydantic import Field, HttpUrl, EmailStr, PositiveInt
from pydantic.schema import schema
from pydantic.json import timedelta_isoformat

import confluent_kafka
from confluent_kafka import Producer, Consumer
from confluent_kafka.admin import AdminClient, NewTopic
from confluent_kafka import Message, KafkaError
import asyncer

import fast_kafka_api.logger

fast_kafka_api.logger.should_supress_timestamps = True

import fast_kafka_api
from .confluent_kafka import AIOProducer
from .confluent_kafka import create_missing_topics
from fast_kafka_api.asyncapi import (
    KafkaMessage,
    export_async_spec,
    _get_msg_cls_for_method,
)
from fast_kafka_api.asyncapi import (
    KafkaBroker,
    ContactInfo,
    KafkaServiceInfo,
    KafkaBrokers,
)
from .logger import get_logger
from .testing import true_after

# %% ../nbs/000_FastKafkaAPI.ipynb 2
logger = get_logger(__name__)

# %% ../nbs/000_FastKafkaAPI.ipynb 3
logger = get_logger(__name__, level=0)

# %% ../nbs/000_FastKafkaAPI.ipynb 7
class KafkaErrorMsg(KafkaMessage):
    topic: str = Field(..., description="topic where exception occurred")
    raw_msg: Optional[bytes] = Field(None, description="raw message string")
    error: str = Field(..., description="exception triggered by the message")

# %% ../nbs/000_FastKafkaAPI.ipynb 8
def _get_topic_name(
    f_or_topic: Union[Callable[[KafkaMessage], None], str], prefix: str = "on_"
):
    if isinstance(f_or_topic, str):
        topic: str = f_or_topic
    elif callable(f_or_topic):
        topic = f_or_topic.__name__
        if not topic.startswith(prefix) or len(topic) <= len(prefix):
            raise ValueError(f"Function name '{topic}' must start with {prefix}")
        topic = topic[len(prefix) :]

    return topic

# %% ../nbs/000_FastKafkaAPI.ipynb 10
class FastKafkaAPI(FastAPI):
    def __init__(
        self,
        *,
        title: str = "FastKafkaAPI",
        contact: Optional[Dict[str, Union[str, Any]]] = None,
        kafka_brokers: Optional[Dict[str, Any]] = None,
        kafka_config: Dict[str, Any],
        root_path: Optional[Union[Path, str]] = None,
        num_partitions: Optional[int] = None,
        replication_factor: Optional[int] = None,
        **kwargs,
    ):
        self._kafka_config = kafka_config
        self.num_partitions = num_partitions
        self.replication_factor = replication_factor

        if root_path is None:
            root_path = Path(".")
        self._root_path = Path(root_path)

        if kafka_brokers is None:
            kafka_brokers = {
                "localhost": KafkaBroker(
                    url="https://localhost",
                    description="Local (dev) Kafka broker",
                    port="9092",
                )
            }
        if contact is None:
            contact = dict(
                name="author", url="https://www.google.com", email="noreply@gmail.com"
            )

        super().__init__(title=title, contact=contact, **kwargs)

        self._store: Dict[
            str,
            Dict[
                str,
                Union[
                    Callable[[KafkaMessage], Any],
                    Callable[[KafkaMessage, Message], Any],
                ],
            ],
        ] = {
            "consumers": {},
            "producers": {},
        }
        self._on_error_topic: Optional[str] = None

        contact_info = ContactInfo(**contact)  # type: ignore
        self._kafka_service_info = KafkaServiceInfo(
            title=self.title,
            version=self.version,
            description=self.description,
            contact=contact_info,
        )
        self._kafka_brokers = KafkaBrokers(brokers=kafka_brokers)

        self._confluent_producer: Optional[AIOProducer] = None

        self._asyncapi_path = self._root_path / "asyncapi"
        (self._asyncapi_path / "docs").mkdir(exist_ok=True, parents=True)
        (self._asyncapi_path / "spec").mkdir(exist_ok=True, parents=True)
        self.mount(
            "/asyncapi",
            StaticFiles(directory=self._asyncapi_path / "docs"),
            name="asyncapi",
        )

        self._is_shutting_down: bool = False
        self._kafka_consumer_tasks: List[asyncio.Task[Any]] = []
        self._kafka_producer_tasks: List[asyncio.Task[Any]] = []

        @self.get("/", include_in_schema=False)
        def redirect_root_to_asyncapi():
            return RedirectResponse("/asyncapi")

        @self.get("/asyncapi", include_in_schema=False)
        async def redirect_asyncapi_docs():
            return RedirectResponse("/asyncapi/index.html")

        @self.get("/asyncapi.yml", include_in_schema=False)
        async def download_asyncapi_yml():
            return FileResponse(self._asyncapi_path / "spec" / "asyncapi.yml")

        @self.on_event("startup")
        async def __on_startup(app=self):
            app._on_startup()

        @self.on_event("shutdown")
        async def __on_shutdown(app=self):
            await app._on_shutdown()

    async def _on_startup(self) -> None:
        raise NotImplemented

    async def _on_shutdown(self) -> None:
        raise NotImplemented

    def _add_topic(
        self,
        *,
        store_key: str,
        topic: str,
        f: Callable[[KafkaMessage], Any],
        on_error: bool,
    ):
        """Stores function `f` under key `topic` in `store`

        Params:
            store_key: either `consumers` or `producers`
            topic: the name of the topic
            f: callback function called on receiving a message for consumers or on delivery report for producers
            on_error: True for at most one producer topic used for outputting errors

        Raises:
            ValueError:
                - if store_key not one of either `consumers` or `producers`,
                - if key `topic` is already in `self._store[store_key]`, or
                - if `on_error` is already set
        """
        raise NotImplementedError

    def _register_kafka_callback(
        self,
        *,
        store_key: str,
        f_or_topic: Union[Callable[[KafkaMessage], None], str],
        prefix: str,
        on_error: bool = False,
    ) -> Callable[[KafkaMessage], None]:
        raise NotImplementedError

    def consumes(
        self, f_or_topic: Union[Callable[[KafkaMessage], None], str]
    ) -> Callable[[KafkaMessage], None]:
        """Decorator registering the callback called when a message is received in a topic.

        This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.

        Params:
            f_or_topic: either a function or name of the topic. In the case of function, its name is
                used to infer the name of the topic ("on_topic_name" -> "topic_name")

        Returns:
            A function returning the same function

        """
        return self._register_kafka_callback(
            store_key="consumers", f_or_topic=f_or_topic, on_error=False  # type: ignore
        )

    def produces(
        self,
        f_or_topic: Union[Callable[[KafkaMessage], None], str],
        on_error: bool = False,
    ) -> Callable[[KafkaMessage], None]:
        """Decorator registering the callback called when delivery report for a produced message is received

        This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.

        Params:
            f_or_topic: either a function or name of the topic. In the case of function, its name is
                used to infer the name of the topic ("on_topic_name" -> "topic_name")

        Returns:
            A function returning the same function

        """
        return self._register_kafka_callback(
            store_key="producers", f_or_topic=f_or_topic, on_error=on_error  # type: ignore
        )

    def produces_on_error(
        self, f_or_topic: Union[Callable[[KafkaMessage], None], str]
    ) -> Callable[[KafkaMessage], None]:
        return self._register_kafka_callback(
            store_key="producers", f_or_topic=f_or_topic, on_error=True  # type: ignore
        )

    def produce(
        self,
        topic: str,
        msg: KafkaMessage,
        on_delivery: Optional[Callable[[KafkaMessage, Message], None]] = None,
    ):
        return self.produce_raw(
            topic=topic, raw_msg=msg.json().encode("utf-8"), on_delivery=on_delivery
        )

    def produce_raw(
        self,
        topic: str,
        raw_msg: Union[str, bytes],
        on_delivery: Optional[Callable[[KafkaMessage, Message], None]] = None,
    ) -> "asyncio.Future[Any]":
        raise NotImplementedError

# %% ../nbs/000_FastKafkaAPI.ipynb 13
@patch
def _add_topic(
    self: FastKafkaAPI,
    *,
    store_key: str,
    topic: str,
    f: Callable[[KafkaMessage], Any],
    on_error: bool = False,
):
    if store_key not in ["consumers", "producers"]:
        raise ValueError(
            f"store_key must be one of 'consumers', 'producers', it is '{store_key}' instead"
        )
    if on_error:
        if store_key == "consumers":
            raise ValueError(
                "`on_error` can be true only when `store_key` is `producers`"
            )
        if self._on_error_topic is not None:
            raise ValueError(
                f"`on_error` must be unique, it is already set to '{self._on_error_topic}'"
            )
        self._on_error_topic = topic

    if topic in self._store["consumers"].keys():
        raise ValueError(f"Topic '{topic}' is already in consumers.")
    if topic in self._store["producers"].keys():
        raise ValueError(f"Topic '{topic}' is already in producers.")
    self._store[store_key][topic] = f

# %% ../nbs/000_FastKafkaAPI.ipynb 15
def _get_first_func_arg_type(f: Callable[[Any], Any]) -> Type[Any]:
    return list(get_type_hints(f).values())[0]

# %% ../nbs/000_FastKafkaAPI.ipynb 17
@patch
def _register_kafka_callback(
    self: FastKafkaAPI,
    *,
    store_key: str,
    f_or_topic: Union[Callable[[KafkaMessage], None], str],
    prefix: str = "on_",
    on_error: bool = False,
) -> Callable[[KafkaMessage], None]:

    topic = _get_topic_name(f_or_topic=f_or_topic, prefix=prefix)

    if isinstance(f_or_topic, str):

        def _decorator(
            on_topic: Callable[[KafkaMessage], Any],
            store_key: str = store_key,
            topic: str = topic,
            on_error: bool = on_error,
        ) -> Callable[[KafkaMessage], Any]:
            first_arg_type = _get_first_func_arg_type(on_topic)
            if on_error and first_arg_type != KafkaErrorMsg:
                raise ValueError(
                    f"The first argument of a decorator handling errors must be KafkaErrorMsg, it is '{first_arg_type}' instead"
                )
            self._add_topic(
                store_key=store_key, topic=topic, f=on_topic, on_error=on_error
            )
            return on_topic

        return _decorator  # type: ignore
    elif callable(f_or_topic):
        first_arg_type = _get_first_func_arg_type(f_or_topic)
        if on_error and first_arg_type != KafkaErrorMsg:
            raise ValueError(
                f"The first argument of a decorator handling errors must be KafkaErrorMsg, it is '{first_arg_type}' instead"
            )
        self._add_topic(
            store_key=store_key, topic=topic, f=f_or_topic, on_error=on_error
        )
        return f_or_topic
    else:
        raise ValueError(
            f"Called on object of type {type(f_or_topic)}, should be called on 'str' or 'callable' only."
        )

# %% ../nbs/000_FastKafkaAPI.ipynb 22
async def _consumer_pooling_step(
    *,
    async_poll_f: Callable[[float], Optional[Message]],
    timeout: float,
    topic: str,
    on_event_callback: Callable[[KafkaMessage], None],
    on_error_callback: Optional[Callable[[KafkaErrorMsg], None]] = None,
    msg_type: Type[KafkaMessage],
) -> None:
    logger.debug("_consumer_pooling_step()")
    #     print(f"iscoroutinefunction(async_poll_f)={iscoroutinefunction(async_poll_f)},")
    if not iscoroutinefunction(async_poll_f):
        raise ValueError(
            f"async_poll_f ({async_poll_f}) must be coroutine, but it isn't."
        )
    if not iscoroutinefunction(on_event_callback):
        raise ValueError(
            f"on_event_callback ({on_event_callback}) must be coroutine, but it isn't."
        )

    try:
        # we convert the blocking poll() function into asynchronous one, while executing poll() in a worker thread
        msg = await async_poll_f(timeout=timeout)  # type: ignore
        if msg is None:
            logger.debug(
                f"consumers_async_loop(topic={topic}): no messages for the topic {topic} due to no message available."
            )
        elif msg.error() is not None:
            logger.warning(
                f"consumers_async_loop(topic={topic}): no messages for the topic {topic} due to error: {msg.error()}"
            )
            if on_error_callback is not None:
                kafka_err_msg = KafkaErrorMsg(
                    topic=topic,
                    raw_msg=None,
                    error=msg.error(),
                )
                on_error_callback(kafka_err_msg)

        else:
            #             msg_type = _get_first_func_arg_type(on_event_callback)
            logger.debug(
                f"consumers_async_loop(topic={topic}): message received for the topic {topic}: {msg.value()}, {on_event_callback}, msg_type={msg_type},"
            )
            msg_object = msg_type.parse_raw(msg.value().decode("utf-8"))
            logger.debug(
                f"consumers_async_loop(topic={topic}): calling {on_event_callback}({msg_object})"
            )
            await on_event_callback(msg_object)

    except Exception as e:
        import traceback

        logger.warning(
            f"consumers_async_loop(topic={topic}): Exception in inner try raised: {e}"
            + "\n"
            + traceback.format_exc()
        )

        if on_error_callback is not None:
            kafka_err_msg = KafkaErrorMsg(
                topic=topic,
                raw_msg=msg.value().decode("utf-8"),
                error=str(e),
            )
            on_error_callback(kafka_err_msg)

# %% ../nbs/000_FastKafkaAPI.ipynb 28
async def _consumers_async_loop(
    *,
    topic: str,
    on_event_callback: Callable[[KafkaMessage], Any],
    is_shutting_down_f: Callable[[], bool],
    config: Dict[str, str],
    timeout: float = 1.0,
    app: FastKafkaAPI,
):
    logger.info(
        f"consumers_async_loop(topic={topic}, config={config}, timeout={timeout}) starting."
    )
    try:
        c: Consumer = None

        c = Consumer(config)
        logger.info(
            f"consumers_async_loop(topic={topic}): Kafka Consumer for topic created."
        )

        c.subscribe([topic])
        logger.info(
            f"consumers_async_loop(topic={topic}): Kafka Consumer subscribed to topic."
        )

        # we convert the blocking poll() function into asynchronous one (it executes poll() in a worker thread)
        async_poll_f = asyncer.asyncify(c.poll)

        # convert on_event_callback to coroutine if needed
        async_on_event_callback = on_event_callback
        if not iscoroutinefunction(async_on_event_callback):
            async_on_event_callback = asyncer.asyncify(async_on_event_callback)
        msg_type = _get_first_func_arg_type(on_event_callback)

        def on_error_callback(error_msg: KafkaErrorMsg, app=app) -> None:
            app.produce(topic=app._on_error_topic, msg=error_msg)

        while True:
            if is_shutting_down_f():
                logger.info(f"consumers_async_loop(topic={topic}) shutting down...")
                break

            await _consumer_pooling_step(
                async_poll_f=async_poll_f,
                timeout=timeout,
                topic=topic,
                on_event_callback=async_on_event_callback,
                on_error_callback=on_error_callback,
                msg_type=msg_type,
            )

    except Exception as e:
        logger.error(
            f"consumers_async_loop(topic={topic}): Exception in outer try raised: {e}"
        )

    finally:
        if c is not None:
            c.close()
            logger.info(f"consumers_async_loop(topic={topic}): Kafka Consumer closed.")

    logger.info(f"consumers_async_loop(topic={topic}) exiting.")

# %% ../nbs/000_FastKafkaAPI.ipynb 33
def consumers_async_loop(
    *,
    app: FastKafkaAPI,
    timeout: float = 1.0,
    is_shutting_down_f: Callable[[], bool],
):
    config: Dict[str, str] = app._kafka_config

    # Used to create missing topics
    topics = sorted(
        set(app._store["consumers"].keys()).union(set(app._store["producers"].keys()))
    )

    kafka_admin = AdminClient(config)
    logger.info(f"consumers_async_loop(): Kafka admin created {kafka_admin}.")
    create_missing_topics(
        admin=kafka_admin,
        topic_names=topics,
        num_partitions=app.num_partitions,
        replication_factor=app.replication_factor,
    )
    logger.info(f"consumers_async_loop(): Kafka topics {topics} created if needed.")

    tx = [
        asyncio.create_task(
            _consumers_async_loop(
                app=app,
                topic=topic,
                on_event_callback=on_event_callback,  # type: ignore
                config=config,
                timeout=timeout,
                is_shutting_down_f=is_shutting_down_f,
            )
        )
        for topic, on_event_callback in app._store["consumers"].items()
    ]

    return tx

# %% ../nbs/000_FastKafkaAPI.ipynb 35
@patch
def _on_startup(self: FastKafkaAPI) -> None:
    export_async_spec(
        consumers=self._store["consumers"],  # type: ignore
        producers=self._store["producers"],  # type: ignore
        kafka_brokers=self._kafka_brokers,
        kafka_service_info=self._kafka_service_info,
        asyncapi_path=self._asyncapi_path,
    )

    self._is_shutting_down = False

    def is_shutting_down_f(self: FastKafkaAPI = self) -> bool:
        return self._is_shutting_down

    self._kafka_consumer_tasks = consumers_async_loop(
        app=self,
        is_shutting_down_f=is_shutting_down_f,
    )

    self._confluent_producer = AIOProducer(self._kafka_config)
    logger.info("AIOProducer created.")


@patch
async def _on_shutdown(self: FastKafkaAPI) -> None:
    self._is_shutting_down = True
    await asyncio.wait(self._kafka_consumer_tasks)
    self._confluent_producer.close()  # type: ignore
    logger.info("AIOProducer closed.")

    self._is_shutting_down = False

# %% ../nbs/000_FastKafkaAPI.ipynb 39
@patch
def produce_raw(
    self: FastKafkaAPI,
    topic: str,
    raw_msg: Union[str, bytes],
    on_delivery: Optional[Callable[[KafkaMessage, Message], None]] = None,
) -> "asyncio.Future[Any]":

    if isinstance(raw_msg, str):
        raw_msg = raw_msg.encode("utf-8")

    if on_delivery is None:
        on_delivery = self._store["producers"][topic]  # type: ignore

    if iscoroutinefunction(on_delivery):
        raise ValueError("coroutines not supported for callbacks yet")

    p: AIOProducer = self._confluent_producer  # type: ignore

    def _delivery_report(
        kafka_err: KafkaError,
        kafka_msg: Message,
        self=self,
        topic=topic,
        raw_msg=raw_msg,
        on_delivery=on_delivery,
    ):
        msg_cls: KafkaMessage
        if kafka_err is not None:
            logger.info(f"produce_raw() topic={topic} raw_msg={raw_msg} delivery error")
            if self._on_error_topic is not None:
                on_error = self._store["producers"][self._on_error_topic]
                msg_cls = _get_msg_cls_for_method(on_error)
                on_error(
                    msg_cls("Message delivery failed: {}".format(kafka_err)), kafka_err  # type: ignore
                )
        else:
            logger.info(f"produce_raw() topic={topic} raw_msg={raw_msg} delivered")
            msg_cls = _get_msg_cls_for_method(on_delivery)
            on_delivery(msg_cls.parse_raw(raw_msg), kafka_msg)

    return p.produce(topic, raw_msg, on_delivery=_delivery_report)

# %% ../nbs/000_FastKafkaAPI.ipynb 41
@patch
def test_run(self: FastKafkaAPI, f: Callable[[], Any], timeout: int = 30):
    async def _loop(app: FastKafkaAPI = self, f: Callable[[], Any] = f):
        logger.info(f"test_run(): starting")
        try:
            async with anyio.create_task_group() as tg:
                with anyio.move_on_after(timeout) as scope:
                    app._on_startup()  # type: ignore

                    if iscoroutinefunction(f):
                        logger.info(
                            f"test_run(app={app}, f={f}): Calling coroutine {f}"
                        )
                        retval = await f()
                    else:
                        logger.info(f"test_run(app={app}, f={f}): Calling function {f}")
                        retval = await asyncer.asyncify(f)()

                return retval
        except Exception as e:
            logger.error(f"test_run(): exception caugth {e}")
            raise e
        finally:
            logger.info(f"test_run(app={app}, f={f}): shutting down the app")
            await app._on_shutdown()
            logger.info(f"test_run(app={app}, f={f}): finished")

    return asyncer.runnify(_loop)()

# %% ../nbs/000_FastKafkaAPI.ipynb 43
@patch
@asynccontextmanager
async def testing_ctx(self: FastKafkaAPI, timeout: int = 30):
    logger.info(f"test_context(): starting")
    try:
        async with anyio.create_task_group() as tg:
            with anyio.move_on_after(timeout) as scope:
                self._on_startup()  # type: ignore

                yield

    except Exception as e:
        logger.error(f"test_context(): exception caugth {e}")
        raise e
    finally:
        logger.info(f"test_context(self={self}): shutting down the app")
        await self._on_shutdown()
        logger.info(f"test_context(self={self}): finished")
