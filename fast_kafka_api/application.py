# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/FastKafkaAPI_rework.ipynb.

# %% auto 0
__all__ = ['logger', 'ProduceCallable', 'TopicCallable', 'KafkaErrorMsg', 'FastKafkaAPI', 'populate_consumers']

# %% ../nbs/FastKafkaAPI_rework.ipynb 1
from typing import *
from typing import get_type_hints

from enum import Enum
from pathlib import Path
import json
import yaml
from copy import deepcopy
from os import environ
from datetime import datetime, timedelta
import tempfile
from contextlib import contextmanager, asynccontextmanager
import time

from fastcore.foundation import patch
from fastcore.meta import delegates

import anyio
import asyncio
from asyncio import iscoroutinefunction  # do not use the version from inspect
import httpx
from fastapi import FastAPI
from fastapi import status, Depends, HTTPException, Request, Response
from fastapi.openapi.docs import get_swagger_ui_html, get_redoc_html
from fastapi.openapi.utils import get_openapi
from fastapi.responses import FileResponse, RedirectResponse
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from pydantic import Field, HttpUrl, EmailStr, PositiveInt
from pydantic.schema import schema
from pydantic.json import timedelta_isoformat

import confluent_kafka
from confluent_kafka import Producer
from confluent_kafka.admin import AdminClient, NewTopic
from confluent_kafka import Message, KafkaError
import asyncer

import fast_kafka_api.logger

fast_kafka_api.logger.should_supress_timestamps = True

import fast_kafka_api
from .confluent_kafka import AIOProducer
from .confluent_kafka import create_missing_topics
from fast_kafka_api.asyncapi import (
    KafkaMessage,
    export_async_spec,
    _get_msg_cls_for_method,
)
from fast_kafka_api.asyncapi import (
    KafkaBroker,
    ContactInfo,
    KafkaServiceInfo,
    KafkaBrokers,
)
from .logger import get_logger
from .testing import true_after

# %% ../nbs/FastKafkaAPI_rework.ipynb 2
logger = get_logger(__name__)

# %% ../nbs/FastKafkaAPI_rework.ipynb 3
logger = get_logger(__name__, level=0)

# %% ../nbs/FastKafkaAPI_rework.ipynb 7
class KafkaErrorMsg(KafkaMessage):
    topic: str = Field(..., description="topic where exception occurred")
    raw_msg: Optional[bytes] = Field(None, description="raw message string")
    error: str = Field(..., description="exception triggered by the message")


ProduceCallable = Callable[[str, KafkaMessage], None]
TopicCallable = Callable[[KafkaMessage, ProduceCallable], None]

# %% ../nbs/FastKafkaAPI_rework.ipynb 8
def _get_topic_name(on_topic: TopicCallable, prefix: str = "on_") -> str:
    topic = on_topic.__name__
    if not topic.startswith(prefix) or len(topic) <= len(prefix):
        raise ValueError(f"Function name '{topic}' must start with {prefix}")
    topic = topic[len(prefix) :]

    return topic

# %% ../nbs/FastKafkaAPI_rework.ipynb 10
class FastKafkaAPI(FastAPI):
    def __init__(
        self,
        *,
        title: str = "FastKafkaAPI",
        contact: Optional[Dict[str, Union[str, Any]]] = None,
        kafka_brokers: Optional[Dict[str, Any]] = None,
        kafka_config: Dict[str, Any],
        root_path: Optional[Union[Path, str]] = None,
        num_partitions: Optional[int] = None,
        replication_factor: Optional[int] = None,
        **kwargs,
    ):
        self._kafka_config = kafka_config
        self.num_partitions = num_partitions
        self.replication_factor = replication_factor

        if root_path is None:
            root_path = Path(".")
        self._root_path = Path(root_path)

        if kafka_brokers is None:
            kafka_brokers = {
                "localhost": KafkaBroker(
                    url="https://localhost",
                    description="Local (dev) Kafka broker",
                    port="9092",
                )
            }
        if contact is None:
            contact = dict(
                name="author", url="https://www.google.com", email="noreply@gmail.com"
            )

        super().__init__(title=title, contact=contact, **kwargs)

        self._store: Dict[str, Dict[str, Tuple[TopicCallable, Dict[str, Any]]],] = {
            "consumers": {},
            "producers": {},
        }
        self._on_error_topic: Optional[str] = None

        contact_info = ContactInfo(**contact)  # type: ignore
        self._kafka_service_info = KafkaServiceInfo(
            title=self.title,
            version=self.version,
            description=self.description,
            contact=contact_info,
        )
        self._kafka_brokers = KafkaBrokers(brokers=kafka_brokers)

        self._confluent_producer: Optional[AIOProducer] = None

        self._asyncapi_path = self._root_path / "asyncapi"
        (self._asyncapi_path / "docs").mkdir(exist_ok=True, parents=True)
        (self._asyncapi_path / "spec").mkdir(exist_ok=True, parents=True)
        self.mount(
            "/asyncapi",
            StaticFiles(directory=self._asyncapi_path / "docs"),
            name="asyncapi",
        )

        self._is_shutting_down: bool = False
        self._kafka_consumer_tasks: List[asyncio.Task[Any]] = []
        self._kafka_producer_tasks: List[asyncio.Task[Any]] = []

        @self.get("/", include_in_schema=False)
        def redirect_root_to_asyncapi():
            return RedirectResponse("/asyncapi")

        @self.get("/asyncapi", include_in_schema=False)
        async def redirect_asyncapi_docs():
            return RedirectResponse("/asyncapi/index.html")

        @self.get("/asyncapi.yml", include_in_schema=False)
        async def download_asyncapi_yml():
            return FileResponse(self._asyncapi_path / "spec" / "asyncapi.yml")

        @self.on_event("startup")
        async def __on_startup(app=self):
            app._on_startup()

        @self.on_event("shutdown")
        async def __on_shutdown(app=self):
            await app._on_shutdown()

    async def _on_startup(self) -> None:
        raise NotImplemented

    async def _on_shutdown(self) -> None:
        raise NotImplemented

    def _add_topic(
        self,
        *,
        store_key: str,
        topic: str,
        f: Callable[[KafkaMessage], Any],
        on_error: bool,
        **kwargs,
    ):
        """Stores function `f` under key `topic` in `store`

        Params:
            store_key: either `consumers` or `producers`
            topic: the name of the topic
            f: callback function called on receiving a message for consumers or on delivery report for producers
            on_error: True for at most one producer topic used for outputting errors

        Raises:
            ValueError:
                - if store_key not one of either `consumers` or `producers`,
                - if key `topic` is already in `self._store[store_key]`, or
                - if `on_error` is already set
        """
        raise NotImplementedError

    def _register_kafka_callback(
        self,
        *,
        store_key: str,
        topic: Optional[str] = None,
        prefix: str = "on_",
        on_error: bool = False,
        **kwargs,
    ) -> TopicCallable:
        """ """
        raise NotImplementedError

    def consumes(
        self,
        topic: Optional[str] = None,
        prefix: str = "on_",
        **kwargs,
    ) -> TopicCallable:
        """Decorator registering the callback called when a message is received in a topic.

        This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.

        Params:
            topic: todo

        Returns:
            A function returning the same function

        """
        return self._register_kafka_callback(
            store_key="consumers", topic=topic, prefix=prefix, on_error=False, **kwargs
        )

    def produces(
        self,
        topic: Optional[str] = None,
        prefix: str = "on_",
        on_error: bool = False,
        **kwargs,
    ) -> Callable[[KafkaMessage], None]:
        """Decorator registering the callback called when delivery report for a produced message is received

        This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.

        Params:
            f_or_topic: either a function or name of the topic. In the case of function, its name is
                used to infer the name of the topic ("on_topic_name" -> "topic_name")

        Returns:
            A function returning the same function

        """
        return self._register_kafka_callback(
            store_key="producers",
            topic=topic,
            prefix=prefix,
            on_error=on_error,
            **kwargs,
        )

    def produces_on_error(
        self,
        topic: Optional[str] = None,
        prefix: str = "on_",
        **kwargs,
    ) -> Callable[[KafkaMessage], None]:
        return self._register_kafka_callback(
            store_key="producers", topic=topic, prefix=prefix, on_error=True, **kwargs
        )

    def produce(
        self,
        topic: str,
        msg: KafkaMessage,
        on_delivery: Optional[Callable[[KafkaMessage, Message], None]] = None,
    ):
        return self.produce_raw(
            topic=topic, raw_msg=msg.json().encode("utf-8"), on_delivery=on_delivery
        )

    def produce_raw(
        self,
        topic: str,
        raw_msg: Union[str, bytes],
        on_delivery: Optional[Callable[[KafkaMessage, Message], None]] = None,
    ) -> "asyncio.Future[Any]":
        raise NotImplementedError

# %% ../nbs/FastKafkaAPI_rework.ipynb 13
@patch
def _add_topic(
    self: FastKafkaAPI,
    *,
    store_key: str,
    topic: str,
    f: Callable[[KafkaMessage], Any],
    on_error: bool = False,
    **kwargs,
):
    if store_key not in ["consumers", "producers"]:
        raise ValueError(
            f"store_key must be one of 'consumers', 'producers', it is '{store_key}' instead"
        )
    if on_error:
        if store_key == "consumers":
            raise ValueError(
                "`on_error` can be true only when `store_key` is `producers`"
            )
        if self._on_error_topic is not None:
            raise ValueError(
                f"`on_error` must be unique, it is already set to '{self._on_error_topic}'"
            )
        self._on_error_topic = topic

    if topic in self._store["consumers"].keys():
        raise ValueError(f"Topic '{topic}' is already in consumers.")
    if topic in self._store["producers"].keys():
        raise ValueError(f"Topic '{topic}' is already in producers.")
    self._store[store_key][topic] = (f, kwargs)

# %% ../nbs/FastKafkaAPI_rework.ipynb 15
def _get_first_func_arg_type(f: Callable[[Any], Any]) -> Type[Any]:
    return list(get_type_hints(f).values())[0]

# %% ../nbs/FastKafkaAPI_rework.ipynb 17
@patch
def _register_kafka_callback(
    self: FastKafkaAPI,
    *,
    store_key: str,
    topic: Optional[str] = None,
    prefix: str = "on_",
    on_error: bool = False,
    **kwargs,
) -> Callable[[KafkaMessage, Callable[[str, KafkaMessage], None]], None]:
    """ """

    def _decorator(
        on_topic: Callable[[KafkaMessage, Callable[[str, KafkaMessage], None]], None],
        store_key: str = store_key,
        topic: str = topic,
        on_error: bool = on_error,
        kwargs=kwargs,
    ) -> Callable[[KafkaMessage, Callable[[str, KafkaMessage], None]], None]:
        if topic is None:
            topic = _get_topic_name(on_topic=on_topic, prefix=prefix)

        first_arg_type = _get_first_func_arg_type(on_topic)
        if on_error and first_arg_type != KafkaErrorMsg:
            raise ValueError(
                f"The first argument of a decorator handling errors must be KafkaErrorMsg, it is '{first_arg_type}' instead"
            )
        self._add_topic(
            store_key=store_key, topic=topic, f=on_topic, on_error=on_error, **kwargs
        )
        return on_topic

    return _decorator


#     elif callable(f_or_topic):
#         first_arg_type = _get_first_func_arg_type(f_or_topic)
#         if on_error and first_arg_type != KafkaErrorMsg:
#             raise ValueError(
#                 f"The first argument of a decorator handling errors must be KafkaErrorMsg, it is '{first_arg_type}' instead"
#             )
#         self._add_topic(
#             store_key=store_key, topic=topic, f=f_or_topic, on_error=on_error
#         )
#         return f_or_topic
#     else:
#         raise ValueError(
#             f"Called on object of type {type(f_or_topic)}, should be called on 'str' or 'callable' only."
#         )

# %% ../nbs/FastKafkaAPI_rework.ipynb 24
def populate_consumers(
    *,
    app: FastKafkaAPI,
    is_shutting_down_f: Callable[[], bool],
) -> List[asyncio.Task]:
    config: Dict[str, Any] = app._kafka_config
    # something like this, consumer_config right now is a decorator function
    # do we change the logic of _add_topic function or pass the consumer configuration another way?
    tx = [
        asyncio.create_task(
            aiokafka_consumer_loop(
                topics=[topic],
                bootstrap_servers=config["bootstrap.servers"],
                auto_offset_reset=consumer_config.get(
                    "auto_offset_reset", config["auto_offset_reset"]
                ),
                max_poll_records=consumer_config.get(
                    "max_poll_records", config["max_poll_records"]
                ),
                max_buffer_size=consumer_config.get(
                    "max_buffer_size", config["max_buffer_size"]
                ),
                callbacks={topic: consumer_config["callback"]},
                produce=consumer_config.get("produce", config["produce"]),
                msg_types={topic: consumer_config["msg_type"]},
                is_shutting_down_f=is_shutting_down_f,
            )
        )
        for topic, consumer_config in app._store["consumers"].items()
    ]

    return tx

# %% ../nbs/FastKafkaAPI_rework.ipynb 25
@patch
def _on_startup(self: FastKafkaAPI) -> None:
    export_async_spec(
        consumers=self._store["consumers"],  # type: ignore
        producers=self._store["producers"],  # type: ignore
        kafka_brokers=self._kafka_brokers,
        kafka_service_info=self._kafka_service_info,
        asyncapi_path=self._asyncapi_path,
    )

    self._is_shutting_down = False

    def is_shutting_down_f(self: FastKafkaAPI = self) -> bool:
        return self._is_shutting_down

    self._kafka_consumer_tasks = populate_consumers(
        app=self,
        is_shutting_down_f=is_shutting_down_f,
    )

    self._confluent_producer = AIOProducer(self._kafka_config)
    logger.info("AIOProducer created.")


@patch
async def _on_shutdown(self: FastKafkaAPI) -> None:
    self._is_shutting_down = True
    await asyncio.wait(self._kafka_consumer_tasks)
    self._confluent_producer.close()  # type: ignore
    logger.info("AIOProducer closed.")

    self._is_shutting_down = False

# %% ../nbs/FastKafkaAPI_rework.ipynb 29
@patch
def produce_raw(
    self: FastKafkaAPI,
    topic: str,
    raw_msg: Union[str, bytes],
    on_delivery: Optional[Callable[[KafkaMessage, Message], None]] = None,
) -> "asyncio.Future[Any]":

    if isinstance(raw_msg, str):
        raw_msg = raw_msg.encode("utf-8")

    if on_delivery is None:
        on_delivery = self._store["producers"][topic]  # type: ignore

    if iscoroutinefunction(on_delivery):
        raise ValueError("coroutines not supported for callbacks yet")

    p: AIOProducer = self._confluent_producer  # type: ignore

    def _delivery_report(
        kafka_err: KafkaError,
        kafka_msg: Message,
        self=self,
        topic=topic,
        raw_msg=raw_msg,
        on_delivery=on_delivery,
    ):
        msg_cls: KafkaMessage
        if kafka_err is not None:
            logger.info(f"produce_raw() topic={topic} raw_msg={raw_msg} delivery error")
            if self._on_error_topic is not None:
                on_error = self._store["producers"][self._on_error_topic]
                msg_cls = _get_msg_cls_for_method(on_error)
                on_error(
                    msg_cls("Message delivery failed: {}".format(kafka_err)), kafka_err  # type: ignore
                )
        else:
            logger.info(f"produce_raw() topic={topic} raw_msg={raw_msg} delivered")
            msg_cls = _get_msg_cls_for_method(on_delivery)
            on_delivery(msg_cls.parse_raw(raw_msg), kafka_msg)

    return p.produce(topic, raw_msg, on_delivery=_delivery_report)

# %% ../nbs/FastKafkaAPI_rework.ipynb 31
@patch
def test_run(self: FastKafkaAPI, f: Callable[[], Any], timeout: int = 30):
    async def _loop(app: FastKafkaAPI = self, f: Callable[[], Any] = f):
        logger.info(f"test_run(): starting")
        try:
            async with anyio.create_task_group() as tg:
                with anyio.move_on_after(timeout) as scope:
                    app._on_startup()  # type: ignore

                    if iscoroutinefunction(f):
                        logger.info(
                            f"test_run(app={app}, f={f}): Calling coroutine {f}"
                        )
                        retval = await f()
                    else:
                        logger.info(f"test_run(app={app}, f={f}): Calling function {f}")
                        retval = await asyncer.asyncify(f)()

                return retval
        except Exception as e:
            logger.error(f"test_run(): exception caugth {e}")
            raise e
        finally:
            logger.info(f"test_run(app={app}, f={f}): shutting down the app")
            await app._on_shutdown()
            logger.info(f"test_run(app={app}, f={f}): finished")

    return asyncer.runnify(_loop)()

# %% ../nbs/FastKafkaAPI_rework.ipynb 33
@patch
@asynccontextmanager
async def testing_ctx(self: FastKafkaAPI, timeout: int = 30):
    logger.info(f"test_context(): starting")
    try:
        async with anyio.create_task_group() as tg:
            with anyio.move_on_after(timeout) as scope:
                self._on_startup()  # type: ignore

                yield

    except Exception as e:
        logger.error(f"test_context(): exception caugth {e}")
        raise e
    finally:
        logger.info(f"test_context(self={self}): shutting down the app")
        await self._on_shutdown()
        logger.info(f"test_context(self={self}): finished")
