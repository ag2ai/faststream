{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff734a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _application.app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import asyncio\n",
    "import functools\n",
    "import inspect\n",
    "import json\n",
    "import types\n",
    "from asyncio import iscoroutinefunction  # do not use the version from inspect\n",
    "from collections import namedtuple\n",
    "from datetime import datetime, timedelta\n",
    "from inspect import signature\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from unittest.mock import AsyncMock, MagicMock\n",
    "\n",
    "import anyio\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "from fastcore.foundation import patch\n",
    "from fastcore.meta import delegates\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import fastkafka._components.logger\n",
    "\n",
    "fastkafka._components.logger.should_supress_timestamps = True\n",
    "\n",
    "import fastkafka\n",
    "from fastkafka._components.aiokafka_consumer_loop import (\n",
    "    aiokafka_consumer_loop,\n",
    "    sanitize_kafka_config,\n",
    ")\n",
    "from fastkafka._components.aiokafka_producer_manager import AIOKafkaProducerManager\n",
    "from fastkafka._components.asyncapi import (\n",
    "    ConsumeCallable,\n",
    "    ContactInfo,\n",
    "    KafkaBroker,\n",
    "    KafkaBrokers,\n",
    "    KafkaServiceInfo,\n",
    "    export_async_spec,\n",
    ")\n",
    "from fastkafka._components.helpers import filter_using_signature\n",
    "from fastkafka._components.logger import get_logger\n",
    "from fastkafka._components.producer_decorator import ProduceCallable, producer_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddeaf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkafka._components.logger import supress_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf16b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "supress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedeee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import unittest.mock\n",
    "\n",
    "import pytest\n",
    "import yaml\n",
    "from pydantic import EmailStr, Field, HttpUrl\n",
    "\n",
    "from fastkafka.testing import LocalKafkaBroker, mock_AIOKafkaProducer_send, true_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9177ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "# allows async calls in notebooks\n",
    "\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211534a4",
   "metadata": {},
   "source": [
    "### Constructor utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@delegates(AIOKafkaConsumer)  # type: ignore\n",
    "@delegates(AIOKafkaProducer, keep=True)  # type: ignore\n",
    "def _get_kafka_config(\n",
    "    **kwargs,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Get kafka config\n",
    "    Args:\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "            ``host[:port]`` strings that the producer should contact to\n",
    "            bootstrap initial cluster metadata. This does not have to be the\n",
    "            full node list.  It just needs to have at least one broker that will\n",
    "            respond to a Metadata API Request. Default port is 9092. If no\n",
    "            servers are specified, will default to ``localhost:9092``.\n",
    "        client_id (str): a name for this client. This string is passed in\n",
    "            each request to servers and can be used to identify specific\n",
    "            server-side log entries that correspond to this client.\n",
    "            Default: ``aiokafka-producer-#`` (appended with a unique number\n",
    "            per instance)\n",
    "        key_serializer (Callable): used to convert user-supplied keys to bytes\n",
    "            If not :data:`None`, called as ``f(key),`` should return\n",
    "            :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        value_serializer (Callable): used to convert user-supplied message\n",
    "            values to :class:`bytes`. If not :data:`None`, called as\n",
    "            ``f(value)``, should return :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        acks (Any): one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
    "            the producer requires the leader to have received before considering a\n",
    "            request complete. This controls the durability of records that are\n",
    "            sent. The following settings are common:\n",
    "\n",
    "            * ``0``: Producer will not wait for any acknowledgment from the server\n",
    "              at all. The message will immediately be added to the socket\n",
    "              buffer and considered sent. No guarantee can be made that the\n",
    "              server has received the record in this case, and the retries\n",
    "              configuration will not take effect (as the client won't\n",
    "              generally know of any failures). The offset given back for each\n",
    "              record will always be set to -1.\n",
    "            * ``1``: The broker leader will write the record to its local log but\n",
    "              will respond without awaiting full acknowledgement from all\n",
    "              followers. In this case should the leader fail immediately\n",
    "              after acknowledging the record but before the followers have\n",
    "              replicated it then the record will be lost.\n",
    "            * ``all``: The broker leader will wait for the full set of in-sync\n",
    "              replicas to acknowledge the record. This guarantees that the\n",
    "              record will not be lost as long as at least one in-sync replica\n",
    "              remains alive. This is the strongest available guarantee.\n",
    "\n",
    "            If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
    "            :data:`True` defaults to ``acks=all``\n",
    "        compression_type (str): The compression type for all data generated by\n",
    "            the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
    "            or :data:`None`.\n",
    "            Compression is of full batches of data, so the efficacy of batching\n",
    "            will also impact the compression ratio (more batching means better\n",
    "            compression). Default: :data:`None`.\n",
    "        max_batch_size (int): Maximum size of buffered data per partition.\n",
    "            After this amount :meth:`send` coroutine will block until batch is\n",
    "            drained.\n",
    "            Default: 16384\n",
    "        linger_ms (int): The producer groups together any records that arrive\n",
    "            in between request transmissions into a single batched request.\n",
    "            Normally this occurs only under load when records arrive faster\n",
    "            than they can be sent out. However in some circumstances the client\n",
    "            may want to reduce the number of requests even under moderate load.\n",
    "            This setting accomplishes this by adding a small amount of\n",
    "            artificial delay; that is, if first request is processed faster,\n",
    "            than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
    "            Default: 0 (i.e. no delay).\n",
    "        partitioner (Callable): Callable used to determine which partition\n",
    "            each message is assigned to. Called (after key serialization):\n",
    "            ``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
    "            The default partitioner implementation hashes each non-None key\n",
    "            using the same murmur2 algorithm as the Java client so that\n",
    "            messages with the same key are assigned to the same partition.\n",
    "            When a key is :data:`None`, the message is delivered to a random partition\n",
    "            (filtered to partitions with available leaders only, if possible).\n",
    "        max_request_size (int): The maximum size of a request. This is also\n",
    "            effectively a cap on the maximum record size. Note that the server\n",
    "            has its own cap on record size which may be different from this.\n",
    "            This setting will limit the number of record batches the producer\n",
    "            will send in a single request to avoid sending huge requests.\n",
    "            Default: 1048576.\n",
    "        metadata_max_age_ms (int): The period of time in milliseconds after\n",
    "            which we force a refresh of metadata even if we haven't seen any\n",
    "            partition leadership changes to proactively discover any new\n",
    "            brokers or partitions. Default: 300000\n",
    "        request_timeout_ms (int): Produce request timeout in milliseconds.\n",
    "            As it's sent as part of\n",
    "            :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
    "            call), maximum waiting time can be up to ``2 *\n",
    "            request_timeout_ms``.\n",
    "            Default: 40000.\n",
    "        retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
    "            errors. Default: 100.\n",
    "        api_version (str): specify which kafka API version to use.\n",
    "            If set to ``auto``, will attempt to infer the broker version by\n",
    "            probing various APIs. Default: ``auto``\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "            Default: ``PLAINTEXT``.\n",
    "        ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
    "            for wrapping socket connections. Directly passed into asyncio's\n",
    "            :meth:`~asyncio.loop.create_connection`. For more\n",
    "            information see :ref:`ssl_auth`.\n",
    "            Default: :data:`None`\n",
    "        connections_max_idle_ms (int): Close idle connections after the number\n",
    "            of milliseconds specified by this config. Specifying :data:`None` will\n",
    "            disable idle checks. Default: 540000 (9 minutes).\n",
    "        enable_idempotence (bool): When set to :data:`True`, the producer will\n",
    "            ensure that exactly one copy of each message is written in the\n",
    "            stream. If :data:`False`, producer retries due to broker failures,\n",
    "            etc., may write duplicates of the retried message in the stream.\n",
    "            Note that enabling idempotence acks to set to ``all``. If it is not\n",
    "            explicitly set by the user it will be chosen. If incompatible\n",
    "            values are set, a :exc:`ValueError` will be thrown.\n",
    "            New in version 0.5.0.\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "            are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_oauth_token_provider (: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
    "            OAuthBearer token provider instance. (See\n",
    "            :mod:`kafka.oauth.abstract`).\n",
    "            Default: :data:`None`\n",
    "        group_id (str or None): name of the consumer group to join for dynamic\n",
    "            partition assignment (if enabled), and to use for fetching and\n",
    "            committing offsets. If None, auto-partition assignment (via\n",
    "            group coordinator) and offset commits are disabled.\n",
    "            Default: None\n",
    "        key_deserializer (Callable): Any callable that takes a\n",
    "            raw message key and returns a deserialized key.\n",
    "        value_deserializer (Callable, Optional): Any callable that takes a\n",
    "            raw message value and returns a deserialized value.\n",
    "        fetch_min_bytes (int): Minimum amount of data the server should\n",
    "            return for a fetch request, otherwise wait up to\n",
    "            `fetch_max_wait_ms` for more data to accumulate. Default: 1.\n",
    "        fetch_max_bytes (int): The maximum amount of data the server should\n",
    "            return for a fetch request. This is not an absolute maximum, if\n",
    "            the first message in the first non-empty partition of the fetch\n",
    "            is larger than this value, the message will still be returned\n",
    "            to ensure that the consumer can make progress. NOTE: consumer\n",
    "            performs fetches to multiple brokers in parallel so memory\n",
    "            usage will depend on the number of brokers containing\n",
    "            partitions for the topic.\n",
    "            Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).\n",
    "        fetch_max_wait_ms (int): The maximum amount of time in milliseconds\n",
    "            the server will block before answering the fetch request if\n",
    "            there isn't sufficient data to immediately satisfy the\n",
    "            requirement given by fetch_min_bytes. Default: 500.\n",
    "        max_partition_fetch_bytes (int): The maximum amount of data\n",
    "            per-partition the server will return. The maximum total memory\n",
    "            used for a request ``= #partitions * max_partition_fetch_bytes``.\n",
    "            This size must be at least as large as the maximum message size\n",
    "            the server allows or else it is possible for the producer to\n",
    "            send messages larger than the consumer can fetch. If that\n",
    "            happens, the consumer can get stuck trying to fetch a large\n",
    "            message on a certain partition. Default: 1048576.\n",
    "        max_poll_records (int): The maximum number of records returned in a\n",
    "            single call to :meth:`.getmany`. Defaults ``None``, no limit.\n",
    "        auto_offset_reset (str): A policy for resetting offsets on\n",
    "            :exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest\n",
    "            available message, ``latest`` will move to the most recent, and\n",
    "            ``none`` will raise an exception so you can handle this case.\n",
    "            Default: ``latest``.\n",
    "        enable_auto_commit (bool): If true the consumer's offset will be\n",
    "            periodically committed in the background. Default: True.\n",
    "        auto_commit_interval_ms (int): milliseconds between automatic\n",
    "            offset commits, if enable_auto_commit is True. Default: 5000.\n",
    "        check_crcs (bool): Automatically check the CRC32 of the records\n",
    "            consumed. This ensures no on-the-wire or on-disk corruption to\n",
    "            the messages occurred. This check adds some overhead, so it may\n",
    "            be disabled in cases seeking extreme performance. Default: True\n",
    "        partition_assignment_strategy (list): List of objects to use to\n",
    "            distribute partition ownership amongst consumer instances when\n",
    "            group management is used. This preference is implicit in the order\n",
    "            of the strategies in the list. When assignment strategy changes:\n",
    "            to support a change to the assignment strategy, new versions must\n",
    "            enable support both for the old assignment strategy and the new\n",
    "            one. The coordinator will choose the old assignment strategy until\n",
    "            all members have been updated. Then it will choose the new\n",
    "            strategy. Default: [:class:`.RoundRobinPartitionAssignor`]\n",
    "        max_poll_interval_ms (int): Maximum allowed time between calls to\n",
    "            consume messages (e.g., :meth:`.getmany`). If this interval\n",
    "            is exceeded the consumer is considered failed and the group will\n",
    "            rebalance in order to reassign the partitions to another consumer\n",
    "            group member. If API methods block waiting for messages, that time\n",
    "            does not count against this timeout. See `KIP-62`_ for more\n",
    "            information. Default 300000\n",
    "        rebalance_timeout_ms (int): The maximum time server will wait for this\n",
    "            consumer to rejoin the group in a case of rebalance. In Java client\n",
    "            this behaviour is bound to `max.poll.interval.ms` configuration,\n",
    "            but as ``aiokafka`` will rejoin the group in the background, we\n",
    "            decouple this setting to allow finer tuning by users that use\n",
    "            :class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults\n",
    "            to ``session_timeout_ms``\n",
    "        session_timeout_ms (int): Client group session and failure detection\n",
    "            timeout. The consumer sends periodic heartbeats\n",
    "            (`heartbeat.interval.ms`) to indicate its liveness to the broker.\n",
    "            If no hearts are received by the broker for a group member within\n",
    "            the session timeout, the broker will remove the consumer from the\n",
    "            group and trigger a rebalance. The allowed range is configured with\n",
    "            the **broker** configuration properties\n",
    "            `group.min.session.timeout.ms` and `group.max.session.timeout.ms`.\n",
    "            Default: 10000\n",
    "        heartbeat_interval_ms (int): The expected time in milliseconds\n",
    "            between heartbeats to the consumer coordinator when using\n",
    "            Kafka's group management feature. Heartbeats are used to ensure\n",
    "            that the consumer's session stays active and to facilitate\n",
    "            rebalancing when new consumers join or leave the group. The\n",
    "            value must be set lower than `session_timeout_ms`, but typically\n",
    "            should be set no higher than 1/3 of that value. It can be\n",
    "            adjusted even lower to control the expected time for normal\n",
    "            rebalances. Default: 3000\n",
    "        consumer_timeout_ms (int): maximum wait timeout for background fetching\n",
    "            routine. Mostly defines how fast the system will see rebalance and\n",
    "            request new data for new partitions. Default: 200\n",
    "        exclude_internal_topics (bool): Whether records from internal topics\n",
    "            (such as offsets) should be exposed to the consumer. If set to True\n",
    "            the only way to receive records from an internal topic is\n",
    "            subscribing to it. Requires 0.10+ Default: True\n",
    "        isolation_level (str): Controls how to read messages written\n",
    "            transactionally.\n",
    "\n",
    "            If set to ``read_committed``, :meth:`.getmany` will only return\n",
    "            transactional messages which have been committed.\n",
    "            If set to ``read_uncommitted`` (the default), :meth:`.getmany` will\n",
    "            return all messages, even transactional messages which have been\n",
    "            aborted.\n",
    "\n",
    "            Non-transactional messages will be returned unconditionally in\n",
    "            either mode.\n",
    "\n",
    "            Messages will always be returned in offset order. Hence, in\n",
    "            `read_committed` mode, :meth:`.getmany` will only return\n",
    "            messages up to the last stable offset (LSO), which is the one less\n",
    "            than the offset of the first open transaction. In particular any\n",
    "            messages appearing after messages belonging to ongoing transactions\n",
    "            will be withheld until the relevant transaction has been completed.\n",
    "            As a result, `read_committed` consumers will not be able to read up\n",
    "            to the high watermark when there are in flight transactions.\n",
    "            Further, when in `read_committed` the seek_to_end method will\n",
    "            return the LSO. See method docs below. Default: ``read_uncommitted``\n",
    "        sasl_oauth_token_provider (~aiokafka.abc.AbstractTokenProvider): OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).\n",
    "            Default: None\n",
    "    \"\"\"\n",
    "    allowed_keys = set(signature(_get_kafka_config).parameters.keys())\n",
    "    if not set(kwargs.keys()) <= allowed_keys:\n",
    "        unallowed_keys = \", \".join(\n",
    "            sorted([f\"'{x}'\" for x in set(kwargs.keys()).difference(allowed_keys)])\n",
    "        )\n",
    "        raise ValueError(f\"Unallowed key arguments passed: {unallowed_keys}\")\n",
    "    retval = kwargs.copy()\n",
    "\n",
    "    # todo: check this values\n",
    "    config_defaults = {\n",
    "        \"bootstrap_servers\": \"localhost:9092\",\n",
    "        \"auto_offset_reset\": \"earliest\",\n",
    "        \"max_poll_records\": 100,\n",
    "        #         \"max_buffer_size\": 10_000,\n",
    "    }\n",
    "    for key, value in config_defaults.items():\n",
    "        if key not in retval:\n",
    "            retval[key] = value\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(_get_kafka_config, AIOKafkaConsumer).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac331ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature(_get_kafka_config).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _get_kafka_config() == {\n",
    "    \"bootstrap_servers\": \"localhost:9092\",\n",
    "    \"auto_offset_reset\": \"earliest\",\n",
    "    \"max_poll_records\": 100,\n",
    "}\n",
    "\n",
    "assert _get_kafka_config(max_poll_records=1_000) == {\n",
    "    \"bootstrap_servers\": \"localhost:9092\",\n",
    "    \"auto_offset_reset\": \"earliest\",\n",
    "    \"max_poll_records\": 1_000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b17b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(ValueError) as e:\n",
    "    _get_kafka_config(random_key=1_000, whatever=\"whocares\")\n",
    "assert e.value.args == (\"Unallowed key arguments passed: 'random_key', 'whatever'\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3477cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_kafka_brokers(kafka_brokers: Optional[Dict[str, Any]] = None) -> KafkaBrokers:\n",
    "    \"\"\"Get Kafka brokers\n",
    "\n",
    "    Args:\n",
    "        kafka_brokers: Kafka brokers\n",
    "\n",
    "    \"\"\"\n",
    "    if kafka_brokers is None:\n",
    "        retval: KafkaBrokers = KafkaBrokers(\n",
    "            brokers={\n",
    "                \"localhost\": KafkaBroker(\n",
    "                    url=\"https://localhost\",\n",
    "                    description=\"Local (dev) Kafka broker\",\n",
    "                    port=\"9092\",\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        retval = KafkaBrokers(\n",
    "            brokers={\n",
    "                k: KafkaBroker.parse_raw(\n",
    "                    v.json() if hasattr(v, \"json\") else json.dumps(v)\n",
    "                )\n",
    "                for k, v in kafka_brokers.items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    _get_kafka_brokers(None).json()\n",
    "    == '{\"brokers\": {\"localhost\": {\"url\": \"https://localhost\", \"description\": \"Local (dev) Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n",
    ")\n",
    "\n",
    "assert (\n",
    "    _get_kafka_brokers(dict(localhost=dict(url=\"localhost\"))).json()\n",
    "    == '{\"brokers\": {\"localhost\": {\"url\": \"localhost\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n",
    ")\n",
    "\n",
    "assert (\n",
    "    _get_kafka_brokers(\n",
    "        dict(localhost=dict(url=\"localhost\"), staging=dict(url=\"staging.airt.ai\"))\n",
    "    ).json()\n",
    "    == '{\"brokers\": {\"localhost\": {\"url\": \"localhost\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}, \"staging\": {\"url\": \"staging.airt.ai\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_topic_name(\n",
    "    topic_callable: Union[ConsumeCallable, ProduceCallable], prefix: str = \"on_\"\n",
    ") -> str:\n",
    "    \"\"\"Get topic name\n",
    "    Args:\n",
    "        topic_callable: a function\n",
    "        prefix: prefix of the name of the function followed by the topic name\n",
    "\n",
    "    Returns:\n",
    "        The name of the topic\n",
    "    \"\"\"\n",
    "    topic = topic_callable.__name__\n",
    "    if not topic.startswith(prefix) or len(topic) <= len(prefix):\n",
    "        raise ValueError(f\"Function name '{topic}' must start with {prefix}\")\n",
    "    topic = topic[len(prefix) :]\n",
    "\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_topic_name_1():\n",
    "    pass\n",
    "\n",
    "\n",
    "assert _get_topic_name(on_topic_name_1) == \"topic_name_1\"\n",
    "\n",
    "assert _get_topic_name(on_topic_name_1, prefix=\"on_topic_\") == \"name_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_contact_info(\n",
    "    name: str = \"Author\",\n",
    "    url: str = \"https://www.google.com\",\n",
    "    email: str = \"noreply@gmail.com\",\n",
    ") -> ContactInfo:\n",
    "    return ContactInfo(name=name, url=url, email=email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e311f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _get_contact_info() == ContactInfo(\n",
    "    name=\"Author\",\n",
    "    url=HttpUrl(url=\"https://www.google.com\", scheme=\"http\"),\n",
    "    email=\"noreply@gmail.com\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c37353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class FastKafka:\n",
    "    @delegates(_get_kafka_config)  # type: ignore\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        title: Optional[str] = None,\n",
    "        description: Optional[str] = None,\n",
    "        version: Optional[str] = None,\n",
    "        contact: Optional[Dict[str, str]] = None,\n",
    "        kafka_brokers: Optional[Dict[str, Any]] = None,\n",
    "        root_path: Optional[Union[Path, str]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Creates FastKafka application\n",
    "\n",
    "        Args:\n",
    "            title: optional title for the documentation. If None,\n",
    "                the title will be set to empty string\n",
    "            description: optional description for the documentation. If\n",
    "                None, the description will be set to empty string\n",
    "            version: optional version for the documentation. If None,\n",
    "                the version will be set to empty string\n",
    "            contact: optional contact for the documentation. If None, the\n",
    "                contact will be set to placeholder values:\n",
    "                name='Author' url=HttpUrl('https://www.google.com', ) email='noreply@gmail.com'\n",
    "            kafka_brokers: dictionary describing kafka brokers used for\n",
    "                generating documentation\n",
    "            root_path: path to where documentation will be created\n",
    "            bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "                ``host[:port]`` strings that the producer should contact to\n",
    "                bootstrap initial cluster metadata. This does not have to be the\n",
    "                full node list.  It just needs to have at least one broker that will\n",
    "                respond to a Metadata API Request. Default port is 9092. If no\n",
    "                servers are specified, will default to ``localhost:9092``.\n",
    "            client_id (str): a name for this client. This string is passed in\n",
    "                each request to servers and can be used to identify specific\n",
    "                server-side log entries that correspond to this client.\n",
    "                Default: ``aiokafka-producer-#`` (appended with a unique number\n",
    "                per instance)\n",
    "            key_serializer (Callable): used to convert user-supplied keys to bytes\n",
    "                If not :data:`None`, called as ``f(key),`` should return\n",
    "                :class:`bytes`.\n",
    "                Default: :data:`None`.\n",
    "            value_serializer (Callable): used to convert user-supplied message\n",
    "                values to :class:`bytes`. If not :data:`None`, called as\n",
    "                ``f(value)``, should return :class:`bytes`.\n",
    "                Default: :data:`None`.\n",
    "            acks (Any): one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
    "                the producer requires the leader to have received before considering a\n",
    "                request complete. This controls the durability of records that are\n",
    "                sent. The following settings are common:\n",
    "\n",
    "                * ``0``: Producer will not wait for any acknowledgment from the server\n",
    "                  at all. The message will immediately be added to the socket\n",
    "                  buffer and considered sent. No guarantee can be made that the\n",
    "                  server has received the record in this case, and the retries\n",
    "                  configuration will not take effect (as the client won't\n",
    "                  generally know of any failures). The offset given back for each\n",
    "                  record will always be set to -1.\n",
    "                * ``1``: The broker leader will write the record to its local log but\n",
    "                  will respond without awaiting full acknowledgement from all\n",
    "                  followers. In this case should the leader fail immediately\n",
    "                  after acknowledging the record but before the followers have\n",
    "                  replicated it then the record will be lost.\n",
    "                * ``all``: The broker leader will wait for the full set of in-sync\n",
    "                  replicas to acknowledge the record. This guarantees that the\n",
    "                  record will not be lost as long as at least one in-sync replica\n",
    "                  remains alive. This is the strongest available guarantee.\n",
    "\n",
    "                If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
    "                :data:`True` defaults to ``acks=all``\n",
    "            compression_type (str): The compression type for all data generated by\n",
    "                the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
    "                or :data:`None`.\n",
    "                Compression is of full batches of data, so the efficacy of batching\n",
    "                will also impact the compression ratio (more batching means better\n",
    "                compression). Default: :data:`None`.\n",
    "            max_batch_size (int): Maximum size of buffered data per partition.\n",
    "                After this amount :meth:`send` coroutine will block until batch is\n",
    "                drained.\n",
    "                Default: 16384\n",
    "            linger_ms (int): The producer groups together any records that arrive\n",
    "                in between request transmissions into a single batched request.\n",
    "                Normally this occurs only under load when records arrive faster\n",
    "                than they can be sent out. However in some circumstances the client\n",
    "                may want to reduce the number of requests even under moderate load.\n",
    "                This setting accomplishes this by adding a small amount of\n",
    "                artificial delay; that is, if first request is processed faster,\n",
    "                than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
    "                Default: 0 (i.e. no delay).\n",
    "            partitioner (Callable): Callable used to determine which partition\n",
    "                each message is assigned to. Called (after key serialization):\n",
    "                ``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
    "                The default partitioner implementation hashes each non-None key\n",
    "                using the same murmur2 algorithm as the Java client so that\n",
    "                messages with the same key are assigned to the same partition.\n",
    "                When a key is :data:`None`, the message is delivered to a random partition\n",
    "                (filtered to partitions with available leaders only, if possible).\n",
    "            max_request_size (int): The maximum size of a request. This is also\n",
    "                effectively a cap on the maximum record size. Note that the server\n",
    "                has its own cap on record size which may be different from this.\n",
    "                This setting will limit the number of record batches the producer\n",
    "                will send in a single request to avoid sending huge requests.\n",
    "                Default: 1048576.\n",
    "            metadata_max_age_ms (int): The period of time in milliseconds after\n",
    "                which we force a refresh of metadata even if we haven't seen any\n",
    "                partition leadership changes to proactively discover any new\n",
    "                brokers or partitions. Default: 300000\n",
    "            request_timeout_ms (int): Produce request timeout in milliseconds.\n",
    "                As it's sent as part of\n",
    "                :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
    "                call), maximum waiting time can be up to ``2 *\n",
    "                request_timeout_ms``.\n",
    "                Default: 40000.\n",
    "            retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
    "                errors. Default: 100.\n",
    "            api_version (str): specify which kafka API version to use.\n",
    "                If set to ``auto``, will attempt to infer the broker version by\n",
    "                probing various APIs. Default: ``auto``\n",
    "            security_protocol (str): Protocol used to communicate with brokers.\n",
    "                Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "                Default: ``PLAINTEXT``.\n",
    "            ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
    "                for wrapping socket connections. Directly passed into asyncio's\n",
    "                :meth:`~asyncio.loop.create_connection`. For more\n",
    "                information see :ref:`ssl_auth`.\n",
    "                Default: :data:`None`\n",
    "            connections_max_idle_ms (int): Close idle connections after the number\n",
    "                of milliseconds specified by this config. Specifying :data:`None` will\n",
    "                disable idle checks. Default: 540000 (9 minutes).\n",
    "            enable_idempotence (bool): When set to :data:`True`, the producer will\n",
    "                ensure that exactly one copy of each message is written in the\n",
    "                stream. If :data:`False`, producer retries due to broker failures,\n",
    "                etc., may write duplicates of the retried message in the stream.\n",
    "                Note that enabling idempotence acks to set to ``all``. If it is not\n",
    "                explicitly set by the user it will be chosen. If incompatible\n",
    "                values are set, a :exc:`ValueError` will be thrown.\n",
    "                New in version 0.5.0.\n",
    "            sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "                is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "                are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "                ``OAUTHBEARER``.\n",
    "                Default: ``PLAIN``\n",
    "            sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "                Default: :data:`None`\n",
    "            sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "                Default: :data:`None`\n",
    "            sasl_oauth_token_provider (: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
    "                OAuthBearer token provider instance. (See\n",
    "                :mod:`kafka.oauth.abstract`).\n",
    "                Default: :data:`None`\n",
    "            group_id (str or None): name of the consumer group to join for dynamic\n",
    "                partition assignment (if enabled), and to use for fetching and\n",
    "                committing offsets. If None, auto-partition assignment (via\n",
    "                group coordinator) and offset commits are disabled.\n",
    "                Default: None\n",
    "            key_deserializer (Callable): Any callable that takes a\n",
    "                raw message key and returns a deserialized key.\n",
    "            value_deserializer (Callable, Optional): Any callable that takes a\n",
    "                raw message value and returns a deserialized value.\n",
    "            fetch_min_bytes (int): Minimum amount of data the server should\n",
    "                return for a fetch request, otherwise wait up to\n",
    "                `fetch_max_wait_ms` for more data to accumulate. Default: 1.\n",
    "            fetch_max_bytes (int): The maximum amount of data the server should\n",
    "                return for a fetch request. This is not an absolute maximum, if\n",
    "                the first message in the first non-empty partition of the fetch\n",
    "                is larger than this value, the message will still be returned\n",
    "                to ensure that the consumer can make progress. NOTE: consumer\n",
    "                performs fetches to multiple brokers in parallel so memory\n",
    "                usage will depend on the number of brokers containing\n",
    "                partitions for the topic.\n",
    "                Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).\n",
    "            fetch_max_wait_ms (int): The maximum amount of time in milliseconds\n",
    "                the server will block before answering the fetch request if\n",
    "                there isn't sufficient data to immediately satisfy the\n",
    "                requirement given by fetch_min_bytes. Default: 500.\n",
    "            max_partition_fetch_bytes (int): The maximum amount of data\n",
    "                per-partition the server will return. The maximum total memory\n",
    "                used for a request ``= #partitions * max_partition_fetch_bytes``.\n",
    "                This size must be at least as large as the maximum message size\n",
    "                the server allows or else it is possible for the producer to\n",
    "                send messages larger than the consumer can fetch. If that\n",
    "                happens, the consumer can get stuck trying to fetch a large\n",
    "                message on a certain partition. Default: 1048576.\n",
    "            max_poll_records (int): The maximum number of records returned in a\n",
    "                single call to :meth:`.getmany`. Defaults ``None``, no limit.\n",
    "            auto_offset_reset (str): A policy for resetting offsets on\n",
    "                :exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest\n",
    "                available message, ``latest`` will move to the most recent, and\n",
    "                ``none`` will raise an exception so you can handle this case.\n",
    "                Default: ``latest``.\n",
    "            enable_auto_commit (bool): If true the consumer's offset will be\n",
    "                periodically committed in the background. Default: True.\n",
    "            auto_commit_interval_ms (int): milliseconds between automatic\n",
    "                offset commits, if enable_auto_commit is True. Default: 5000.\n",
    "            check_crcs (bool): Automatically check the CRC32 of the records\n",
    "                consumed. This ensures no on-the-wire or on-disk corruption to\n",
    "                the messages occurred. This check adds some overhead, so it may\n",
    "                be disabled in cases seeking extreme performance. Default: True\n",
    "            partition_assignment_strategy (list): List of objects to use to\n",
    "                distribute partition ownership amongst consumer instances when\n",
    "                group management is used. This preference is implicit in the order\n",
    "                of the strategies in the list. When assignment strategy changes:\n",
    "                to support a change to the assignment strategy, new versions must\n",
    "                enable support both for the old assignment strategy and the new\n",
    "                one. The coordinator will choose the old assignment strategy until\n",
    "                all members have been updated. Then it will choose the new\n",
    "                strategy. Default: [:class:`.RoundRobinPartitionAssignor`]\n",
    "            max_poll_interval_ms (int): Maximum allowed time between calls to\n",
    "                consume messages (e.g., :meth:`.getmany`). If this interval\n",
    "                is exceeded the consumer is considered failed and the group will\n",
    "                rebalance in order to reassign the partitions to another consumer\n",
    "                group member. If API methods block waiting for messages, that time\n",
    "                does not count against this timeout. See `KIP-62`_ for more\n",
    "                information. Default 300000\n",
    "            rebalance_timeout_ms (int): The maximum time server will wait for this\n",
    "                consumer to rejoin the group in a case of rebalance. In Java client\n",
    "                this behaviour is bound to `max.poll.interval.ms` configuration,\n",
    "                but as ``aiokafka`` will rejoin the group in the background, we\n",
    "                decouple this setting to allow finer tuning by users that use\n",
    "                :class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults\n",
    "                to ``session_timeout_ms``\n",
    "            session_timeout_ms (int): Client group session and failure detection\n",
    "                timeout. The consumer sends periodic heartbeats\n",
    "                (`heartbeat.interval.ms`) to indicate its liveness to the broker.\n",
    "                If no hearts are received by the broker for a group member within\n",
    "                the session timeout, the broker will remove the consumer from the\n",
    "                group and trigger a rebalance. The allowed range is configured with\n",
    "                the **broker** configuration properties\n",
    "                `group.min.session.timeout.ms` and `group.max.session.timeout.ms`.\n",
    "                Default: 10000\n",
    "            heartbeat_interval_ms (int): The expected time in milliseconds\n",
    "                between heartbeats to the consumer coordinator when using\n",
    "                Kafka's group management feature. Heartbeats are used to ensure\n",
    "                that the consumer's session stays active and to facilitate\n",
    "                rebalancing when new consumers join or leave the group. The\n",
    "                value must be set lower than `session_timeout_ms`, but typically\n",
    "                should be set no higher than 1/3 of that value. It can be\n",
    "                adjusted even lower to control the expected time for normal\n",
    "                rebalances. Default: 3000\n",
    "            consumer_timeout_ms (int): maximum wait timeout for background fetching\n",
    "                routine. Mostly defines how fast the system will see rebalance and\n",
    "                request new data for new partitions. Default: 200\n",
    "            exclude_internal_topics (bool): Whether records from internal topics\n",
    "                (such as offsets) should be exposed to the consumer. If set to True\n",
    "                the only way to receive records from an internal topic is\n",
    "                subscribing to it. Requires 0.10+ Default: True\n",
    "            isolation_level (str): Controls how to read messages written\n",
    "                transactionally.\n",
    "\n",
    "                If set to ``read_committed``, :meth:`.getmany` will only return\n",
    "                transactional messages which have been committed.\n",
    "                If set to ``read_uncommitted`` (the default), :meth:`.getmany` will\n",
    "                return all messages, even transactional messages which have been\n",
    "                aborted.\n",
    "\n",
    "                Non-transactional messages will be returned unconditionally in\n",
    "                either mode.\n",
    "\n",
    "                Messages will always be returned in offset order. Hence, in\n",
    "                `read_committed` mode, :meth:`.getmany` will only return\n",
    "                messages up to the last stable offset (LSO), which is the one less\n",
    "                than the offset of the first open transaction. In particular any\n",
    "                messages appearing after messages belonging to ongoing transactions\n",
    "                will be withheld until the relevant transaction has been completed.\n",
    "                As a result, `read_committed` consumers will not be able to read up\n",
    "                to the high watermark when there are in flight transactions.\n",
    "                Further, when in `read_committed` the seek_to_end method will\n",
    "                return the LSO. See method docs below. Default: ``read_uncommitted``\n",
    "            sasl_oauth_token_provider (~aiokafka.abc.AbstractTokenProvider): OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).\n",
    "                Default: None\n",
    "        \"\"\"\n",
    "\n",
    "        # this is needed for documentation generation\n",
    "        self._title = title if title is not None else \"\"\n",
    "        self._description = description if description is not None else \"\"\n",
    "        self._version = version if version is not None else \"\"\n",
    "        if contact is not None:\n",
    "            self._contact_info = _get_contact_info(**contact)\n",
    "        else:\n",
    "            self._contact_info = _get_contact_info()\n",
    "\n",
    "        self._kafka_service_info = KafkaServiceInfo(\n",
    "            title=self._title,\n",
    "            version=self._version,\n",
    "            description=self._description,\n",
    "            contact=self._contact_info,\n",
    "        )\n",
    "        self._kafka_brokers = _get_kafka_brokers(kafka_brokers)\n",
    "\n",
    "        self._root_path = Path(\".\") if root_path is None else Path(root_path)\n",
    "\n",
    "        self._asyncapi_path = self._root_path / \"asyncapi\"\n",
    "        (self._asyncapi_path / \"docs\").mkdir(exist_ok=True, parents=True)\n",
    "        (self._asyncapi_path / \"spec\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # this is used as default parameters for creating AIOProducer and AIOConsumer objects\n",
    "        self._kafka_config = _get_kafka_config(**kwargs)\n",
    "\n",
    "        #\n",
    "        self._consumers_store: Dict[str, Tuple[ConsumeCallable, Dict[str, Any]]] = {}\n",
    "\n",
    "        self._producers_store: Dict[  # type: ignore\n",
    "            str, Tuple[ProduceCallable, AIOKafkaProducer, Dict[str, Any]]\n",
    "        ] = {}\n",
    "\n",
    "        self._producers_list: List[  # type: ignore\n",
    "            Union[AIOKafkaProducer, AIOKafkaProducerManager]\n",
    "        ] = []\n",
    "\n",
    "        # background tasks\n",
    "        self._scheduled_bg_tasks: List[Callable[..., Coroutine[Any, Any, Any]]] = []\n",
    "        self._bg_task_group_generator: Optional[anyio.abc.TaskGroup] = None\n",
    "        self._bg_tasks_group: Optional[anyio.abc.TaskGroup] = None\n",
    "\n",
    "        # todo: use this for errrors\n",
    "        self._on_error_topic: Optional[str] = None\n",
    "\n",
    "        self._is_started: bool = False\n",
    "        self._is_shutting_down: bool = False\n",
    "        self._kafka_consumer_tasks: List[asyncio.Task[Any]] = []\n",
    "        self._kafka_producer_tasks: List[asyncio.Task[Any]] = []\n",
    "        self._running_bg_tasks: List[asyncio.Task[Any]] = []\n",
    "        self.run = False\n",
    "\n",
    "        # testing functions\n",
    "        self.AppMocks = None\n",
    "        self.mocks = None\n",
    "        self.awaited_mocks = None\n",
    "\n",
    "    @property\n",
    "    def is_started(self) -> bool:\n",
    "        return self._is_started\n",
    "\n",
    "    def set_bootstrap_servers(self, bootstrap_servers: str) -> None:\n",
    "        self._kafka_config[\"bootstrap_servers\"] = bootstrap_servers\n",
    "\n",
    "    async def __aenter__(self) -> \"FastKafka\":\n",
    "        await self.startup()\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(\n",
    "        self,\n",
    "        exc_type: Optional[Type[BaseException]],\n",
    "        exc: Optional[BaseException],\n",
    "        tb: Optional[types.TracebackType],\n",
    "    ) -> None:\n",
    "        await self.shutdown()\n",
    "\n",
    "    async def startup(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def shutdown(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def consumes(\n",
    "        self,\n",
    "        topic: Optional[str] = None,\n",
    "        *,\n",
    "        prefix: str = \"on_\",\n",
    "        **kwargs: Dict[str, Any],\n",
    "    ) -> ConsumeCallable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def produces(  # type: ignore\n",
    "        self,\n",
    "        topic: Optional[str] = None,\n",
    "        *,\n",
    "        prefix: str = \"to_\",\n",
    "        producer: Optional[AIOKafkaProducer] = None,\n",
    "        **kwargs: Dict[str, Any],\n",
    "    ) -> ProduceCallable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_in_background(\n",
    "        self,\n",
    "    ) -> Callable[[], Any]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _populate_consumers(\n",
    "        self,\n",
    "        is_shutting_down_f: Callable[[], bool],\n",
    "    ) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_topics(self) -> Iterable[str]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _populate_producers(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _populate_bg_tasks(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_docs(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_mocks(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _shutdown_consumers(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _shutdown_producers(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _shutdown_bg_tasks(self) -> None:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "FastKafka.__module__ = \"fastkafka\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894af799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_title': '',\n",
       " '_description': '',\n",
       " '_version': '',\n",
       " '_contact_info': ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com'),\n",
       " '_kafka_service_info': KafkaServiceInfo(title='', version='', description='', contact=ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')),\n",
       " '_kafka_brokers': KafkaBrokers(brokers={'localhost': KafkaBroker(url='https://localhost', description='Local (dev) Kafka broker', port='9092', protocol='kafka', security=None)}),\n",
       " '_root_path': PosixPath('.'),\n",
       " '_asyncapi_path': PosixPath('asyncapi'),\n",
       " '_kafka_config': {'bootstrap_servers': 'localhost:9092',\n",
       "  'auto_offset_reset': 'earliest',\n",
       "  'max_poll_records': 100},\n",
       " '_consumers_store': {},\n",
       " '_producers_store': {},\n",
       " '_producers_list': [],\n",
       " '_scheduled_bg_tasks': [],\n",
       " '_bg_task_group_generator': None,\n",
       " '_bg_tasks_group': None,\n",
       " '_on_error_topic': None,\n",
       " '_is_started': False,\n",
       " '_is_shutting_down': False,\n",
       " '_kafka_consumer_tasks': [],\n",
       " '_kafka_producer_tasks': [],\n",
       " '_running_bg_tasks': [],\n",
       " 'run': False,\n",
       " 'AppMocks': None,\n",
       " 'mocks': None,\n",
       " 'awaited_mocks': None}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_app = FastKafka()\n",
    "kafka_app.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b99de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_title': '',\n",
       " '_description': '',\n",
       " '_version': '',\n",
       " '_contact_info': ContactInfo(name='Davor', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com'),\n",
       " '_kafka_service_info': KafkaServiceInfo(title='', version='', description='', contact=ContactInfo(name='Davor', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')),\n",
       " '_kafka_brokers': KafkaBrokers(brokers={'localhost': KafkaBroker(url='https://localhost', description='Local (dev) Kafka broker', port='9092', protocol='kafka', security=None)}),\n",
       " '_root_path': PosixPath('.'),\n",
       " '_asyncapi_path': PosixPath('asyncapi'),\n",
       " '_kafka_config': {'bootstrap_servers': 'localhost:9092',\n",
       "  'auto_offset_reset': 'earliest',\n",
       "  'max_poll_records': 100},\n",
       " '_consumers_store': {},\n",
       " '_producers_store': {},\n",
       " '_producers_list': [],\n",
       " '_scheduled_bg_tasks': [],\n",
       " '_bg_task_group_generator': None,\n",
       " '_bg_tasks_group': None,\n",
       " '_on_error_topic': None,\n",
       " '_is_started': False,\n",
       " '_is_shutting_down': False,\n",
       " '_kafka_consumer_tasks': [],\n",
       " '_kafka_producer_tasks': [],\n",
       " '_running_bg_tasks': [],\n",
       " 'run': False,\n",
       " 'AppMocks': None,\n",
       " 'mocks': None,\n",
       " 'awaited_mocks': None}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_app = FastKafka(contact={\"name\": \"Davor\"})\n",
    "kafka_app.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfbe17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_app(\n",
    "    *, root_path: str = \"/tmp/000_FastKafka\", bootstrap_servers: str = \"localhost:9092\"\n",
    "):\n",
    "    if Path(root_path).exists():\n",
    "        shutil.rmtree(root_path)\n",
    "\n",
    "    kafka_app = FastKafka(\n",
    "        kafka_brokers={\n",
    "            \"local\": {\n",
    "                \"url\": \"kafka\",\n",
    "                \"name\": \"development\",\n",
    "                \"description\": \"Local (dev) Kafka broker\",\n",
    "                \"port\": 9092,\n",
    "            }\n",
    "        },\n",
    "        root_path=root_path,\n",
    "        bootstrap_servers=bootstrap_servers,\n",
    "    )\n",
    "\n",
    "    return kafka_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66237424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastkafka.FastKafka>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = create_testing_app()\n",
    "assert Path(\"/tmp/000_FastKafka\").exists()\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "@delegates(AIOKafkaConsumer)  # type: ignore\n",
    "def consumes(\n",
    "    self: FastKafka,\n",
    "    topic: Optional[str] = None,\n",
    "    *,\n",
    "    prefix: str = \"on_\",\n",
    "    **kwargs: Dict[str, Any],\n",
    ") -> Callable[[ConsumeCallable], ConsumeCallable]:\n",
    "    \"\"\"Decorator registering the callback called when a message is received in a topic.\n",
    "\n",
    "    This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.\n",
    "\n",
    "    Args:\n",
    "        topic: Kafka topic that the consumer will subscribe to and execute the\n",
    "            decorated function when it receives a message from the topic,\n",
    "            default: None. If the topic is not specified, topic name will be\n",
    "            inferred from the decorated function name by stripping the defined prefix\n",
    "        prefix: Prefix stripped from the decorated function to define a topic name\n",
    "            if the topic argument is not passed, default: \"on_\". If the decorated\n",
    "            function name is not prefixed with the defined prefix and topic argument\n",
    "            is not passed, then this method will throw ValueError\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string (or list of\n",
    "            ``host[:port]`` strings) that the consumer should contact to bootstrap\n",
    "            initial cluster metadata.\n",
    "\n",
    "            This does not have to be the full node list.\n",
    "            It just needs to have at least one broker that will respond to a\n",
    "            Metadata API Request. Default port is 9092. If no servers are\n",
    "            specified, will default to ``localhost:9092``.\n",
    "        client_id (str): a name for this client. This string is passed in\n",
    "            each request to servers and can be used to identify specific\n",
    "            server-side log entries that correspond to this client. Also\n",
    "            submitted to :class:`~.consumer.group_coordinator.GroupCoordinator`\n",
    "            for logging with respect to consumer group administration. Default:\n",
    "            ``aiokafka-{version}``\n",
    "        group_id (str or None): name of the consumer group to join for dynamic\n",
    "            partition assignment (if enabled), and to use for fetching and\n",
    "            committing offsets. If None, auto-partition assignment (via\n",
    "            group coordinator) and offset commits are disabled.\n",
    "            Default: None\n",
    "        key_deserializer (Callable): Any callable that takes a\n",
    "            raw message key and returns a deserialized key.\n",
    "        value_deserializer (Callable, Optional): Any callable that takes a\n",
    "            raw message value and returns a deserialized value.\n",
    "        fetch_min_bytes (int): Minimum amount of data the server should\n",
    "            return for a fetch request, otherwise wait up to\n",
    "            `fetch_max_wait_ms` for more data to accumulate. Default: 1.\n",
    "        fetch_max_bytes (int): The maximum amount of data the server should\n",
    "            return for a fetch request. This is not an absolute maximum, if\n",
    "            the first message in the first non-empty partition of the fetch\n",
    "            is larger than this value, the message will still be returned\n",
    "            to ensure that the consumer can make progress. NOTE: consumer\n",
    "            performs fetches to multiple brokers in parallel so memory\n",
    "            usage will depend on the number of brokers containing\n",
    "            partitions for the topic.\n",
    "            Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).\n",
    "        fetch_max_wait_ms (int): The maximum amount of time in milliseconds\n",
    "            the server will block before answering the fetch request if\n",
    "            there isn't sufficient data to immediately satisfy the\n",
    "            requirement given by fetch_min_bytes. Default: 500.\n",
    "        max_partition_fetch_bytes (int): The maximum amount of data\n",
    "            per-partition the server will return. The maximum total memory\n",
    "            used for a request ``= #partitions * max_partition_fetch_bytes``.\n",
    "            This size must be at least as large as the maximum message size\n",
    "            the server allows or else it is possible for the producer to\n",
    "            send messages larger than the consumer can fetch. If that\n",
    "            happens, the consumer can get stuck trying to fetch a large\n",
    "            message on a certain partition. Default: 1048576.\n",
    "        max_poll_records (int): The maximum number of records returned in a\n",
    "            single call to :meth:`.getmany`. Defaults ``None``, no limit.\n",
    "        request_timeout_ms (int): Client request timeout in milliseconds.\n",
    "            Default: 40000.\n",
    "        retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
    "            errors. Default: 100.\n",
    "        auto_offset_reset (str): A policy for resetting offsets on\n",
    "            :exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest\n",
    "            available message, ``latest`` will move to the most recent, and\n",
    "            ``none`` will raise an exception so you can handle this case.\n",
    "            Default: ``latest``.\n",
    "        enable_auto_commit (bool): If true the consumer's offset will be\n",
    "            periodically committed in the background. Default: True.\n",
    "        auto_commit_interval_ms (int): milliseconds between automatic\n",
    "            offset commits, if enable_auto_commit is True. Default: 5000.\n",
    "        check_crcs (bool): Automatically check the CRC32 of the records\n",
    "            consumed. This ensures no on-the-wire or on-disk corruption to\n",
    "            the messages occurred. This check adds some overhead, so it may\n",
    "            be disabled in cases seeking extreme performance. Default: True\n",
    "        metadata_max_age_ms (int): The period of time in milliseconds after\n",
    "            which we force a refresh of metadata even if we haven't seen any\n",
    "            partition leadership changes to proactively discover any new\n",
    "            brokers or partitions. Default: 300000\n",
    "        partition_assignment_strategy (list): List of objects to use to\n",
    "            distribute partition ownership amongst consumer instances when\n",
    "            group management is used. This preference is implicit in the order\n",
    "            of the strategies in the list. When assignment strategy changes:\n",
    "            to support a change to the assignment strategy, new versions must\n",
    "            enable support both for the old assignment strategy and the new\n",
    "            one. The coordinator will choose the old assignment strategy until\n",
    "            all members have been updated. Then it will choose the new\n",
    "            strategy. Default: [:class:`.RoundRobinPartitionAssignor`]\n",
    "        max_poll_interval_ms (int): Maximum allowed time between calls to\n",
    "            consume messages (e.g., :meth:`.getmany`). If this interval\n",
    "            is exceeded the consumer is considered failed and the group will\n",
    "            rebalance in order to reassign the partitions to another consumer\n",
    "            group member. If API methods block waiting for messages, that time\n",
    "            does not count against this timeout. See `KIP-62`_ for more\n",
    "            information. Default 300000\n",
    "        rebalance_timeout_ms (int): The maximum time server will wait for this\n",
    "            consumer to rejoin the group in a case of rebalance. In Java client\n",
    "            this behaviour is bound to `max.poll.interval.ms` configuration,\n",
    "            but as ``aiokafka`` will rejoin the group in the background, we\n",
    "            decouple this setting to allow finer tuning by users that use\n",
    "            :class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults\n",
    "            to ``session_timeout_ms``\n",
    "        session_timeout_ms (int): Client group session and failure detection\n",
    "            timeout. The consumer sends periodic heartbeats\n",
    "            (`heartbeat.interval.ms`) to indicate its liveness to the broker.\n",
    "            If no hearts are received by the broker for a group member within\n",
    "            the session timeout, the broker will remove the consumer from the\n",
    "            group and trigger a rebalance. The allowed range is configured with\n",
    "            the **broker** configuration properties\n",
    "            `group.min.session.timeout.ms` and `group.max.session.timeout.ms`.\n",
    "            Default: 10000\n",
    "        heartbeat_interval_ms (int): The expected time in milliseconds\n",
    "            between heartbeats to the consumer coordinator when using\n",
    "            Kafka's group management feature. Heartbeats are used to ensure\n",
    "            that the consumer's session stays active and to facilitate\n",
    "            rebalancing when new consumers join or leave the group. The\n",
    "            value must be set lower than `session_timeout_ms`, but typically\n",
    "            should be set no higher than 1/3 of that value. It can be\n",
    "            adjusted even lower to control the expected time for normal\n",
    "            rebalances. Default: 3000\n",
    "        consumer_timeout_ms (int): maximum wait timeout for background fetching\n",
    "            routine. Mostly defines how fast the system will see rebalance and\n",
    "            request new data for new partitions. Default: 200\n",
    "        api_version (str): specify which kafka API version to use.\n",
    "            :class:`AIOKafkaConsumer` supports Kafka API versions >=0.9 only.\n",
    "            If set to ``auto``, will attempt to infer the broker version by\n",
    "            probing various APIs. Default: ``auto``\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "        ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
    "            for wrapping socket connections. Directly passed into asyncio's\n",
    "            :meth:`~asyncio.loop.create_connection`. For more information see\n",
    "            :ref:`ssl_auth`. Default: None.\n",
    "        exclude_internal_topics (bool): Whether records from internal topics\n",
    "            (such as offsets) should be exposed to the consumer. If set to True\n",
    "            the only way to receive records from an internal topic is\n",
    "            subscribing to it. Requires 0.10+ Default: True\n",
    "        connections_max_idle_ms (int): Close idle connections after the number\n",
    "            of milliseconds specified by this config. Specifying `None` will\n",
    "            disable idle checks. Default: 540000 (9 minutes).\n",
    "        isolation_level (str): Controls how to read messages written\n",
    "            transactionally.\n",
    "\n",
    "            If set to ``read_committed``, :meth:`.getmany` will only return\n",
    "            transactional messages which have been committed.\n",
    "            If set to ``read_uncommitted`` (the default), :meth:`.getmany` will\n",
    "            return all messages, even transactional messages which have been\n",
    "            aborted.\n",
    "\n",
    "            Non-transactional messages will be returned unconditionally in\n",
    "            either mode.\n",
    "\n",
    "            Messages will always be returned in offset order. Hence, in\n",
    "            `read_committed` mode, :meth:`.getmany` will only return\n",
    "            messages up to the last stable offset (LSO), which is the one less\n",
    "            than the offset of the first open transaction. In particular any\n",
    "            messages appearing after messages belonging to ongoing transactions\n",
    "            will be withheld until the relevant transaction has been completed.\n",
    "            As a result, `read_committed` consumers will not be able to read up\n",
    "            to the high watermark when there are in flight transactions.\n",
    "            Further, when in `read_committed` the seek_to_end method will\n",
    "            return the LSO. See method docs below. Default: ``read_uncommitted``\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values are:\n",
    "            ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: None\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: None\n",
    "        sasl_oauth_token_provider (~aiokafka.abc.AbstractTokenProvider): OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).\n",
    "            Default: None\n",
    "\n",
    "    Returns:\n",
    "        A function returning the same function\n",
    "\n",
    "    Throws:\n",
    "        ValueError\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _decorator(\n",
    "        on_topic: ConsumeCallable,\n",
    "        topic: Optional[str] = topic,\n",
    "        kwargs: Dict[str, Any] = kwargs,\n",
    "    ) -> ConsumeCallable:\n",
    "        topic_resolved: str = (\n",
    "            _get_topic_name(topic_callable=on_topic, prefix=prefix)\n",
    "            if topic is None\n",
    "            else topic\n",
    "        )\n",
    "\n",
    "        self._consumers_store[topic_resolved] = (on_topic, kwargs)\n",
    "\n",
    "        return on_topic\n",
    "\n",
    "    return _decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature(FastKafka.consumes).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(FastKafka.consumes, AIOKafkaConsumer).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb7fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = create_testing_app()\n",
    "\n",
    "\n",
    "# Basic check\n",
    "@app.consumes()\n",
    "def on_my_topic_1(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._consumers_store[\"my_topic_1\"] == (on_my_topic_1, {}), app._consumers_store\n",
    "\n",
    "\n",
    "# Check topic setting\n",
    "@app.consumes(topic=\"test_topic_2\")\n",
    "def some_func_name(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._consumers_store[\"test_topic_2\"] == (\n",
    "    some_func_name,\n",
    "    {},\n",
    "), app._consumers_store\n",
    "\n",
    "\n",
    "# Check prefix change\n",
    "@app.consumes(prefix=\"for_\")\n",
    "def for_test_topic_3(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._consumers_store[\"test_topic_3\"] == (\n",
    "    for_test_topic_3,\n",
    "    {},\n",
    "), app._consumers_store\n",
    "\n",
    "# Check passing of kwargs\n",
    "kwargs = {\"arg1\": \"val1\", \"arg2\": 2}\n",
    "\n",
    "\n",
    "@app.consumes(topic=\"test_topic\", **kwargs)\n",
    "def for_test_kwargs(msg: BaseModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._consumers_store[\"test_topic\"] == (\n",
    "    for_test_kwargs,\n",
    "    kwargs,\n",
    "), app._consumers_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e269659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "@delegates(AIOKafkaProducer)  # type: ignore\n",
    "def produces(\n",
    "    self: FastKafka,\n",
    "    topic: Optional[str] = None,\n",
    "    *,\n",
    "    prefix: str = \"to_\",\n",
    "    **kwargs: Dict[str, Any],\n",
    ") -> Callable[[ProduceCallable], ProduceCallable]:\n",
    "    \"\"\"Decorator registering the callback called when delivery report for a produced message is received\n",
    "\n",
    "    This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.\n",
    "\n",
    "    Args:\n",
    "        topic: Kafka topic that the producer will send returned values from\n",
    "            the decorated function to, default: None- If the topic is not\n",
    "            specified, topic name will be inferred from the decorated function\n",
    "            name by stripping the defined prefix.\n",
    "        prefix: Prefix stripped from the decorated function to define a topic\n",
    "            name if the topic argument is not passed, default: \"to_\". If the\n",
    "            decorated function name is not prefixed with the defined prefix\n",
    "            and topic argument is not passed, then this method will throw ValueError\n",
    "        producer: optional AIOKafkaProducer object used to produce messages\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "            ``host[:port]`` strings that the producer should contact to\n",
    "            bootstrap initial cluster metadata. This does not have to be the\n",
    "            full node list.  It just needs to have at least one broker that will\n",
    "            respond to a Metadata API Request. Default port is 9092. If no\n",
    "            servers are specified, will default to ``localhost:9092``.\n",
    "        client_id (str): a name for this client. This string is passed in\n",
    "            each request to servers and can be used to identify specific\n",
    "            server-side log entries that correspond to this client.\n",
    "            Default: ``aiokafka-producer-#`` (appended with a unique number\n",
    "            per instance)\n",
    "        key_serializer (Callable): used to convert user-supplied keys to bytes\n",
    "            If not :data:`None`, called as ``f(key),`` should return\n",
    "            :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        value_serializer (Callable): used to convert user-supplied message\n",
    "            values to :class:`bytes`. If not :data:`None`, called as\n",
    "            ``f(value)``, should return :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        acks (Any): one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
    "            the producer requires the leader to have received before considering a\n",
    "            request complete. This controls the durability of records that are\n",
    "            sent. The following settings are common:\n",
    "\n",
    "            * ``0``: Producer will not wait for any acknowledgment from the server\n",
    "              at all. The message will immediately be added to the socket\n",
    "              buffer and considered sent. No guarantee can be made that the\n",
    "              server has received the record in this case, and the retries\n",
    "              configuration will not take effect (as the client won't\n",
    "              generally know of any failures). The offset given back for each\n",
    "              record will always be set to -1.\n",
    "            * ``1``: The broker leader will write the record to its local log but\n",
    "              will respond without awaiting full acknowledgement from all\n",
    "              followers. In this case should the leader fail immediately\n",
    "              after acknowledging the record but before the followers have\n",
    "              replicated it then the record will be lost.\n",
    "            * ``all``: The broker leader will wait for the full set of in-sync\n",
    "              replicas to acknowledge the record. This guarantees that the\n",
    "              record will not be lost as long as at least one in-sync replica\n",
    "              remains alive. This is the strongest available guarantee.\n",
    "\n",
    "            If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
    "            :data:`True` defaults to ``acks=all``\n",
    "        compression_type (str): The compression type for all data generated by\n",
    "            the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
    "            or :data:`None`.\n",
    "            Compression is of full batches of data, so the efficacy of batching\n",
    "            will also impact the compression ratio (more batching means better\n",
    "            compression). Default: :data:`None`.\n",
    "        max_batch_size (int): Maximum size of buffered data per partition.\n",
    "            After this amount :meth:`send` coroutine will block until batch is\n",
    "            drained.\n",
    "            Default: 16384\n",
    "        linger_ms (int): The producer groups together any records that arrive\n",
    "            in between request transmissions into a single batched request.\n",
    "            Normally this occurs only under load when records arrive faster\n",
    "            than they can be sent out. However in some circumstances the client\n",
    "            may want to reduce the number of requests even under moderate load.\n",
    "            This setting accomplishes this by adding a small amount of\n",
    "            artificial delay; that is, if first request is processed faster,\n",
    "            than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
    "            Default: 0 (i.e. no delay).\n",
    "        partitioner (Callable): Callable used to determine which partition\n",
    "            each message is assigned to. Called (after key serialization):\n",
    "            ``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
    "            The default partitioner implementation hashes each non-None key\n",
    "            using the same murmur2 algorithm as the Java client so that\n",
    "            messages with the same key are assigned to the same partition.\n",
    "            When a key is :data:`None`, the message is delivered to a random partition\n",
    "            (filtered to partitions with available leaders only, if possible).\n",
    "        max_request_size (int): The maximum size of a request. This is also\n",
    "            effectively a cap on the maximum record size. Note that the server\n",
    "            has its own cap on record size which may be different from this.\n",
    "            This setting will limit the number of record batches the producer\n",
    "            will send in a single request to avoid sending huge requests.\n",
    "            Default: 1048576.\n",
    "        metadata_max_age_ms (int): The period of time in milliseconds after\n",
    "            which we force a refresh of metadata even if we haven't seen any\n",
    "            partition leadership changes to proactively discover any new\n",
    "            brokers or partitions. Default: 300000\n",
    "        request_timeout_ms (int): Produce request timeout in milliseconds.\n",
    "            As it's sent as part of\n",
    "            :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
    "            call), maximum waiting time can be up to ``2 *\n",
    "            request_timeout_ms``.\n",
    "            Default: 40000.\n",
    "        retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
    "            errors. Default: 100.\n",
    "        api_version (str): specify which kafka API version to use.\n",
    "            If set to ``auto``, will attempt to infer the broker version by\n",
    "            probing various APIs. Default: ``auto``\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "            Default: ``PLAINTEXT``.\n",
    "        ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
    "            for wrapping socket connections. Directly passed into asyncio's\n",
    "            :meth:`~asyncio.loop.create_connection`. For more\n",
    "            information see :ref:`ssl_auth`.\n",
    "            Default: :data:`None`\n",
    "        connections_max_idle_ms (int): Close idle connections after the number\n",
    "            of milliseconds specified by this config. Specifying :data:`None` will\n",
    "            disable idle checks. Default: 540000 (9 minutes).\n",
    "        enable_idempotence (bool): When set to :data:`True`, the producer will\n",
    "            ensure that exactly one copy of each message is written in the\n",
    "            stream. If :data:`False`, producer retries due to broker failures,\n",
    "            etc., may write duplicates of the retried message in the stream.\n",
    "            Note that enabling idempotence acks to set to ``all``. If it is not\n",
    "            explicitly set by the user it will be chosen. If incompatible\n",
    "            values are set, a :exc:`ValueError` will be thrown.\n",
    "            New in version 0.5.0.\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "            are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_oauth_token_provider (: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
    "            OAuthBearer token provider instance. (See\n",
    "            :mod:`kafka.oauth.abstract`).\n",
    "            Default: :data:`None`\n",
    "\n",
    "    Returns:\n",
    "        A function returning the same function\n",
    "\n",
    "    Raises:\n",
    "        ValueError: when needed\n",
    "    \"\"\"\n",
    "\n",
    "    def _decorator(\n",
    "        on_topic: ProduceCallable,\n",
    "        topic: Optional[str] = topic,\n",
    "        kwargs: Dict[str, Any] = kwargs,\n",
    "    ) -> ProduceCallable:\n",
    "        topic_resolved: str = (\n",
    "            _get_topic_name(topic_callable=on_topic, prefix=prefix)\n",
    "            if topic is None\n",
    "            else topic\n",
    "        )\n",
    "\n",
    "        self._producers_store[topic_resolved] = (on_topic, None, kwargs)\n",
    "        return producer_decorator(self._producers_store, on_topic, topic_resolved)\n",
    "\n",
    "    return _decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28376379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature(FastKafka.produces).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(FastKafka.produces, AIOKafkaProducer).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7cede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producer_decorator: producer_store={'my_topic_1': (<function to_my_topic_1>, None, {})}\n",
      "producer_decorator: producer_store={'my_topic_1': (<function to_my_topic_1>, None, {}), 'test_topic_2': (<function some_func_name>, None, {})}\n",
      "producer_decorator: producer_store={'my_topic_1': (<function to_my_topic_1>, None, {}), 'test_topic_2': (<function some_func_name>, None, {}), 'test_topic_3': (<function for_test_topic_3>, None, {})}\n",
      "producer_decorator: producer_store={'my_topic_1': (<function to_my_topic_1>, None, {}), 'test_topic_2': (<function some_func_name>, None, {}), 'test_topic_3': (<function for_test_topic_3>, None, {})}\n",
      "producer_decorator: producer_store={'my_topic_1': (<function to_my_topic_1>, None, {}), 'test_topic_2': (<function some_func_name>, None, {}), 'test_topic_3': (<function for_test_topic_3>, None, {}), 'test_topic': (<function for_test_kwargs>, None, {'arg1': 'val1', 'arg2': 2})}\n"
     ]
    }
   ],
   "source": [
    "app = create_testing_app()\n",
    "\n",
    "\n",
    "# Basic check\n",
    "async def to_my_topic_1(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Must be done without sugar to keep the original function reference\n",
    "check_func = to_my_topic_1\n",
    "to_my_topic_1 = app.produces()(to_my_topic_1)\n",
    "\n",
    "assert app._producers_store[\"my_topic_1\"] == (\n",
    "    check_func,\n",
    "    None,\n",
    "    {},\n",
    "), f\"{app._producers_store}, {to_my_topic_1}\"\n",
    "\n",
    "\n",
    "# Check topic setting\n",
    "def some_func_name(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "check_func = some_func_name\n",
    "some_func_name = app.produces(topic=\"test_topic_2\")(some_func_name)\n",
    "\n",
    "assert app._producers_store[\"test_topic_2\"] == (\n",
    "    check_func,\n",
    "    None,\n",
    "    {},\n",
    "), app._producers_store\n",
    "\n",
    "\n",
    "# Check prefix change\n",
    "@app.produces(prefix=\"for_\")\n",
    "def for_test_topic_3(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "check_func = for_test_topic_3\n",
    "some_func_name = app.produces(prefix=\"for_\")(for_test_topic_3)\n",
    "\n",
    "assert app._producers_store[\"test_topic_3\"] == (\n",
    "    check_func,\n",
    "    None,\n",
    "    {},\n",
    "), app._producers_store\n",
    "\n",
    "# Check passing of kwargs\n",
    "kwargs = {\"arg1\": \"val1\", \"arg2\": 2}\n",
    "\n",
    "\n",
    "async def for_test_kwargs(msg: BaseModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "check_func = for_test_kwargs\n",
    "for_test_kwargs = app.produces(topic=\"test_topic\", **kwargs)(for_test_kwargs)\n",
    "\n",
    "assert app._producers_store[\"test_topic\"] == (\n",
    "    check_func,\n",
    "    None,\n",
    "    kwargs,\n",
    "), app._producers_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d507987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def get_topics(self: FastKafka) -> Iterable[str]:\n",
    "    produce_topics = set(self._producers_store.keys())\n",
    "    consume_topics = set(self._consumers_store.keys())\n",
    "    return consume_topics.union(produce_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac053363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producer_decorator: producer_store={'topic_1': (<function to_topic_1>, None, {})}\n"
     ]
    }
   ],
   "source": [
    "app = create_testing_app()\n",
    "\n",
    "\n",
    "@app.produces()\n",
    "def to_topic_1() -> BaseModel:\n",
    "    pass\n",
    "\n",
    "\n",
    "@app.consumes()\n",
    "def on_topic_2(msg: BaseModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app.get_topics() == set([\"topic_1\", \"topic_2\"]), f\"{app.get_topics()=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4744bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def run_in_background(\n",
    "    self: FastKafka,\n",
    ") -> Callable[\n",
    "    [Callable[..., Coroutine[Any, Any, Any]]], Callable[..., Coroutine[Any, Any, Any]]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Decorator to schedule a task to be run in the background.\n",
    "\n",
    "    This decorator is used to schedule a task to be run in the background when the app's `_on_startup` event is triggered.\n",
    "\n",
    "    Returns:\n",
    "        Callable[None, None]: A decorator function that takes a background task as an input and stores it to be run in the backround.\n",
    "    \"\"\"\n",
    "\n",
    "    def _decorator(\n",
    "        bg_task: Callable[..., Coroutine[Any, Any, Any]]\n",
    "    ) -> Callable[..., Coroutine[Any, Any, Any]]:\n",
    "        \"\"\"\n",
    "        Store the background task.\n",
    "\n",
    "        Args:\n",
    "            bg_task (Callable[[], None]): The background task to be run asynchronously.\n",
    "\n",
    "        Returns:\n",
    "            Callable[[], None]: Original background task.\n",
    "        \"\"\"\n",
    "        logger.info(\n",
    "            f\"run_in_background() : Adding function '{bg_task.__name__}' as background task\"\n",
    "        )\n",
    "        self._scheduled_bg_tasks.append(bg_task)\n",
    "\n",
    "        return bg_task\n",
    "\n",
    "    return _decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16917ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: run_in_background() : Adding function 'async_background_job' as background task\n"
     ]
    }
   ],
   "source": [
    "# Check if the background job is getting registered\n",
    "\n",
    "app = create_testing_app()\n",
    "\n",
    "\n",
    "@app.run_in_background()\n",
    "async def async_background_job():\n",
    "    \"\"\"Async background job\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._scheduled_bg_tasks[0] == async_background_job, app._scheduled_bg_tasks[0]\n",
    "assert app._scheduled_bg_tasks.__len__() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInfo(BaseModel):\n",
    "    mobile: str = Field(..., example=\"+385987654321\")\n",
    "    name: str = Field(..., example=\"James Bond\")\n",
    "\n",
    "\n",
    "class MyMsgUrl(BaseModel):\n",
    "    info: MyInfo = Field(..., example=dict(mobile=\"+385987654321\", name=\"James Bond\"))\n",
    "    url: HttpUrl = Field(..., example=\"https://sis.gov.uk/agents/007\")\n",
    "\n",
    "\n",
    "class MyMsgEmail(BaseModel):\n",
    "    msg_url: MyMsgUrl = Field(\n",
    "        ...,\n",
    "        example=dict(\n",
    "            info=dict(mobile=\"+385987654321\", name=\"James Bond\"),\n",
    "            url=\"https://sis.gov.uk/agents/007\",\n",
    "        ),\n",
    "    )\n",
    "    email: EmailStr = Field(..., example=\"agent-007@sis.gov.uk\")\n",
    "\n",
    "\n",
    "def setup_testing_app(bootstrap_servers=\"localhost:9092\"):\n",
    "    app = create_testing_app(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "    @app.consumes(\"my_topic_1\")\n",
    "    def on_my_topic_one(msg: MyMsgUrl) -> None:\n",
    "        logger.debug(f\"on_my_topic_one(msg={msg},)\")\n",
    "\n",
    "    @app.consumes()\n",
    "    async def on_my_topic_2(msg: MyMsgEmail) -> None:\n",
    "        logger.debug(f\"on_my_topic_2(msg={msg},)\")\n",
    "\n",
    "    with pytest.raises(ValueError) as e:\n",
    "\n",
    "        @app.consumes()\n",
    "        def my_topic_3(msg: MyMsgEmail) -> None:\n",
    "            raise NotImplemented\n",
    "\n",
    "    @app.produces()\n",
    "    def to_my_topic_3(url: str) -> MyMsgUrl:\n",
    "        logger.debug(f\"on_my_topic_3(msg={url}\")\n",
    "        return MyMsgUrl(info=MyInfo(\"+3851987654321\", \"Sean Connery\"), url=url)\n",
    "\n",
    "    @app.produces()\n",
    "    async def to_my_topic_4(msg: MyMsgEmail) -> MyMsgEmail:\n",
    "        logger.debug(f\"on_my_topic_4(msg={msg}\")\n",
    "        return msg\n",
    "\n",
    "    @app.produces()\n",
    "    def to_my_topic_5(url: str) -> MyMsgUrl:\n",
    "        logger.debug(f\"on_my_topic_5(msg={url}\")\n",
    "        return MyMsgUrl(info=MyInfo(\"+3859123456789\", \"John Wayne\"), url=url)\n",
    "\n",
    "    @app.run_in_background()\n",
    "    async def long_bg_job():\n",
    "        logger.debug(f\"long_bg_job()\")\n",
    "        await asyncio.sleep(100)\n",
    "\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a945425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producer_decorator: producer_store={'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, None, {})}\n",
      "producer_decorator: producer_store={'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, None, {}), 'my_topic_4': (<function setup_testing_app.<locals>.to_my_topic_4>, None, {})}\n",
      "producer_decorator: producer_store={'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, None, {}), 'my_topic_4': (<function setup_testing_app.<locals>.to_my_topic_4>, None, {}), 'my_topic_5': (<function setup_testing_app.<locals>.to_my_topic_5>, None, {})}\n",
      "app._kafka_service_info=title='' version='' description='' contact=ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')\n",
      "app._kafka_brokers=brokers={'local': KafkaBroker(url='kafka', description='Local (dev) Kafka broker', port='9092', protocol='kafka', security=None)}\n"
     ]
    }
   ],
   "source": [
    "app = setup_testing_app()\n",
    "\n",
    "assert set(app._consumers_store.keys()) == set([\"my_topic_1\", \"my_topic_2\"])\n",
    "assert set(app._producers_store.keys()) == set(\n",
    "    [\"my_topic_3\", \"my_topic_4\", \"my_topic_5\"]\n",
    ")\n",
    "\n",
    "print(f\"app._kafka_service_info={app._kafka_service_info}\")\n",
    "print(f\"app._kafka_brokers={app._kafka_brokers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ea338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def _populate_consumers(\n",
    "    self: FastKafka,\n",
    "    is_shutting_down_f: Callable[[], bool],\n",
    ") -> None:\n",
    "    default_config: Dict[str, Any] = filter_using_signature(\n",
    "        AIOKafkaConsumer, **self._kafka_config\n",
    "    )\n",
    "    self._kafka_consumer_tasks = [\n",
    "        asyncio.create_task(\n",
    "            aiokafka_consumer_loop(\n",
    "                topic=topic,\n",
    "                callback=consumer,\n",
    "                msg_type=signature(consumer).parameters[\"msg\"].annotation,\n",
    "                is_shutting_down_f=is_shutting_down_f,\n",
    "                **{**default_config, **override_config},\n",
    "            )\n",
    "        )\n",
    "        for topic, (consumer, override_config) in self._consumers_store.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _shutdown_consumers(\n",
    "    self: FastKafka,\n",
    ") -> None:\n",
    "    if self._kafka_consumer_tasks:\n",
    "        await asyncio.wait(self._kafka_consumer_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.helpers: Java is already installed.\n",
      "[INFO] fastkafka._components.helpers: But not exported to PATH, exporting...\n",
      "[INFO] fastkafka._components.helpers: Kafka is installed.\n",
      "[INFO] fastkafka._components.helpers: But not exported to PATH, exporting...\n",
      "[INFO] fastkafka._testing.local_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.local_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.local_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "producer_decorator: producer_store={'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, None, {})}\n",
      "producer_decorator: producer_store={'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, None, {}), 'my_topic_4': (<function setup_testing_app.<locals>.to_my_topic_4>, None, {})}\n",
      "producer_decorator: producer_store={'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, None, {}), 'my_topic_4': (<function setup_testing_app.<locals>.to_my_topic_4>, None, {}), 'my_topic_5': (<function setup_testing_app.<locals>.to_my_topic_5>, None, {})}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[WARNING] aiokafka.cluster: No broker metadata found in MetadataResponse\n",
      "[WARNING] aiokafka.cluster: No broker metadata found in MetadataResponse\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_1'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_1'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_2'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_2'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[WARNING] aiokafka.cluster: Topic my_topic_1 is not available during auto-create initialization\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_1': 0}. \n",
      "[WARNING] aiokafka.cluster: Topic my_topic_2 is not available during auto-create initialization\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_2': 0}. \n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 258135...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 258135 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 257774...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 257774 terminated.\n"
     ]
    }
   ],
   "source": [
    "async with LocalKafkaBroker() as bootstrap_server:\n",
    "    app = setup_testing_app(bootstrap_servers=bootstrap_server)\n",
    "    app._populate_consumers(is_shutting_down_f=true_after(1))\n",
    "    assert len(app._kafka_consumer_tasks) == 2\n",
    "\n",
    "    await app._shutdown_consumers()\n",
    "\n",
    "    assert all([t.done() for t in app._kafka_consumer_tasks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ee2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# TODO: Add passing of vars\n",
    "async def _create_producer(  # type: ignore\n",
    "    *,\n",
    "    callback: ProduceCallable,\n",
    "    default_config: Dict[str, Any],\n",
    "    override_config: Dict[str, Any],\n",
    "    producers_list: List[Union[AIOKafkaProducer, AIOKafkaProducerManager]],\n",
    ") -> Union[AIOKafkaProducer, AIOKafkaProducerManager]:\n",
    "    \"\"\"Creates a producer\n",
    "\n",
    "    Args:\n",
    "        callback: A callback function that is called when the producer is ready.\n",
    "        producer: An existing producer to use.\n",
    "        default_config: A dictionary of default configuration values.\n",
    "        override_config: A dictionary of configuration values to override.\n",
    "        producers_list: A list of producers to add the new producer to.\n",
    "\n",
    "    Returns:\n",
    "        A producer.\n",
    "    \"\"\"\n",
    "\n",
    "    config = {\n",
    "        **filter_using_signature(AIOKafkaProducer, **default_config),\n",
    "        **override_config,\n",
    "    }\n",
    "    producer = AIOKafkaProducer(**config)\n",
    "    logger.info(\n",
    "        f\"_create_producer() : created producer using the config: '{sanitize_kafka_config(**config)}'\"\n",
    "    )\n",
    "\n",
    "    if not iscoroutinefunction(callback):\n",
    "        producer = AIOKafkaProducerManager(producer)\n",
    "\n",
    "    await producer.start()\n",
    "\n",
    "    producers_list.append(producer)\n",
    "\n",
    "    return producer\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _populate_producers(self: FastKafka) -> None:\n",
    "    \"\"\"Populates the producers for the FastKafka instance.\n",
    "\n",
    "    Args:\n",
    "        self: The FastKafka instance.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "\n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    default_config: Dict[str, Any] = self._kafka_config\n",
    "    self._producers_list = []\n",
    "    self._producers_store.update(\n",
    "        {\n",
    "            topic: (\n",
    "                callback,\n",
    "                await _create_producer(\n",
    "                    callback=callback,\n",
    "                    default_config=default_config,\n",
    "                    override_config=override_config,\n",
    "                    producers_list=self._producers_list,\n",
    "                ),\n",
    "                override_config,\n",
    "            )\n",
    "            for topic, (\n",
    "                callback,\n",
    "                _,\n",
    "                override_config,\n",
    "            ) in self._producers_store.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _shutdown_producers(self: FastKafka) -> None:\n",
    "    [await producer.stop() for producer in self._producers_list[::-1]]\n",
    "    # Remove references to stale producers\n",
    "    self._producers_list = []\n",
    "    self._producers_store.update(\n",
    "        {\n",
    "            topic: (\n",
    "                callback,\n",
    "                None,\n",
    "                override_config,\n",
    "            )\n",
    "            for topic, (\n",
    "                callback,\n",
    "                _,\n",
    "                override_config,\n",
    "            ) in self._producers_store.items()\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.helpers: Java is already installed.\n",
      "[INFO] fastkafka._components.helpers: Kafka is installed.\n",
      "[INFO] fastkafka._testing.local_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.local_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.local_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "producer_decorator: producer_store={'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, None, {})}\n",
      "producer_decorator: producer_store={'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, None, {}), 'my_topic_4': (<function setup_testing_app.<locals>.to_my_topic_4>, None, {})}\n",
      "producer_decorator: producer_store={'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, None, {}), 'my_topic_4': (<function setup_testing_app.<locals>.to_my_topic_4>, None, {}), 'my_topic_5': (<function setup_testing_app.<locals>.to_my_topic_5>, None, {})}\n",
      "{'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, None, {}), 'my_topic_4': (<function setup_testing_app.<locals>.to_my_topic_4>, None, {}), 'my_topic_5': (<function setup_testing_app.<locals>.to_my_topic_5>, None, {})}\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[WARNING] aiokafka.cluster: No broker metadata found in MetadataResponse\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "{'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, <fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager object>, {}), 'my_topic_4': (<function setup_testing_app.<locals>.to_my_topic_4>, <aiokafka.producer.producer.AIOKafkaProducer object>, {}), 'my_topic_5': (<function setup_testing_app.<locals>.to_my_topic_5>, <fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager object>, {})}\n",
      "[<fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager object>, <aiokafka.producer.producer.AIOKafkaProducer object>, <fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager object>]\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[<fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager object>, <aiokafka.producer.producer.AIOKafkaProducer object>, <fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager object>]\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 258933...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 258933 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 258573...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 258573 terminated.\n"
     ]
    }
   ],
   "source": [
    "async with LocalKafkaBroker() as bootstrap_server:\n",
    "    app = setup_testing_app(bootstrap_servers=bootstrap_server)\n",
    "    print(app._producers_store)\n",
    "    await app._populate_producers()\n",
    "    print(app._producers_store)\n",
    "    assert len(app._producers_list) == 3\n",
    "    print(app._producers_list)\n",
    "    await app._shutdown_producers()\n",
    "\n",
    "    # One more time for reentrancy\n",
    "    await app._populate_producers()\n",
    "    assert len(app._producers_list) == 3\n",
    "    print(app._producers_list)\n",
    "    await app._shutdown_producers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _populate_bg_tasks(\n",
    "    self: FastKafka,\n",
    ") -> None:\n",
    "    def _start_bg_task(task: Callable[..., Coroutine[Any, Any, Any]]) -> asyncio.Task:\n",
    "        logger.info(\n",
    "            f\"_populate_bg_tasks() : Starting background task '{task.__name__}'\"\n",
    "        )\n",
    "        return asyncio.create_task(task(), name=task.__name__)\n",
    "\n",
    "    self._running_bg_tasks = [_start_bg_task(task) for task in self._scheduled_bg_tasks]\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _shutdown_bg_tasks(\n",
    "    self: FastKafka,\n",
    ") -> None:\n",
    "    for task in self._running_bg_tasks:\n",
    "        logger.info(\n",
    "            f\"_shutdown_bg_tasks() : Cancelling background task '{task.get_name()}'\"\n",
    "        )\n",
    "        task.cancel()\n",
    "\n",
    "    for task in self._running_bg_tasks:\n",
    "        logger.info(\n",
    "            f\"_shutdown_bg_tasks() : Waiting for background task '{task.get_name()}' to finish\"\n",
    "        )\n",
    "        try:\n",
    "            await task\n",
    "        except asyncio.CancelledError:\n",
    "            pass\n",
    "        logger.info(\n",
    "            f\"_shutdown_bg_tasks() : Execution finished for background task '{task.get_name()}'\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c687d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with LocalKafkaBroker() as bootstrap_server:\n",
    "    app = setup_testing_app(bootstrap_servers=bootstrap_server)\n",
    "\n",
    "    @app.run_in_background()\n",
    "    async def long_bg_job():\n",
    "        logger.debug(f\"new_long_bg_job()\")\n",
    "        await asyncio.sleep(100)\n",
    "\n",
    "    await app._populate_bg_tasks()\n",
    "    assert len(app._scheduled_bg_tasks) == 2\n",
    "    assert len(app._running_bg_tasks) == 2\n",
    "    await app._shutdown_bg_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def startup(self: FastKafka) -> None:\n",
    "    def is_shutting_down_f(self: FastKafka = self) -> bool:\n",
    "        return self._is_shutting_down\n",
    "\n",
    "    #     self.create_docs()\n",
    "    await self._populate_producers()\n",
    "    self._populate_consumers(is_shutting_down_f)\n",
    "    await self._populate_bg_tasks()\n",
    "\n",
    "    self._is_started = True\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def shutdown(self: FastKafka) -> None:\n",
    "    self._is_shutting_down = True\n",
    "\n",
    "    await self._shutdown_bg_tasks()\n",
    "    await self._shutdown_consumers()\n",
    "    await self._shutdown_producers()\n",
    "\n",
    "    self._is_shutting_down = False\n",
    "    self._is_started = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b199a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test app reentrancy\n",
    "\n",
    "async with LocalKafkaBroker() as bootstrap_server:\n",
    "    with mock_AIOKafkaProducer_send() as mock:\n",
    "        app = create_testing_app(bootstrap_servers=bootstrap_server)\n",
    "\n",
    "        @app.produces()\n",
    "        async def to_my_test_topic(mobile: str, url: str) -> MyMsgUrl:\n",
    "            msg = MyMsgUrl(info=dict(mobile=mobile, name=\"James Bond\"), url=url)\n",
    "            return msg\n",
    "\n",
    "        try:\n",
    "            await app.startup()\n",
    "            await to_my_test_topic(mobile=\"+385912345678\", url=\"https://www.vip.hr\")\n",
    "        finally:\n",
    "            await app.shutdown()\n",
    "\n",
    "        try:\n",
    "            await app.startup()\n",
    "            await to_my_test_topic(mobile=\"+385987654321\", url=\"https://www.ht.hr\")\n",
    "        finally:\n",
    "            await app.shutdown()\n",
    "\n",
    "        mock.assert_has_calls(\n",
    "            [\n",
    "                unittest.mock.call(\n",
    "                    \"my_test_topic\",\n",
    "                    b'{\"info\": {\"mobile\": \"+385912345678\", \"name\": \"James Bond\"}, \"url\": \"https://www.vip.hr\"}',\n",
    "                    key=None,\n",
    "                ),\n",
    "                unittest.mock.call(\n",
    "                    \"my_test_topic\",\n",
    "                    b'{\"info\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"}, \"url\": \"https://www.ht.hr\"}',\n",
    "                    key=None,\n",
    "                ),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9cfce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock up send method of AIOKafkaProducer\n",
    "async with LocalKafkaBroker() as bootstrap_server:\n",
    "    with mock_AIOKafkaProducer_send() as mock:\n",
    "        app = create_testing_app(bootstrap_servers=bootstrap_server)\n",
    "\n",
    "        @app.produces()\n",
    "        async def to_my_test_topic(mobile: str, url: str) -> MyMsgUrl:\n",
    "            msg = MyMsgUrl(info=dict(mobile=mobile, name=\"James Bond\"), url=url)\n",
    "            return msg\n",
    "\n",
    "        @app.produces()\n",
    "        def to_my_test_topic_2(mobile: str, url: str) -> MyMsgUrl:\n",
    "            msg = MyMsgUrl(info=dict(mobile=mobile, name=\"James Bond\"), url=url)\n",
    "            return msg\n",
    "\n",
    "        try:\n",
    "            await app.startup()\n",
    "            await to_my_test_topic(mobile=\"+385912345678\", url=\"https://www.vip.hr\")\n",
    "            to_my_test_topic_2(mobile=\"+385987654321\", url=\"https://www.ht.hr\")\n",
    "        finally:\n",
    "            await app.shutdown()\n",
    "\n",
    "        mock.assert_has_calls(\n",
    "            [\n",
    "                unittest.mock.call(\n",
    "                    \"my_test_topic\",\n",
    "                    b'{\"info\": {\"mobile\": \"+385912345678\", \"name\": \"James Bond\"}, \"url\": \"https://www.vip.hr\"}',\n",
    "                    key=None,\n",
    "                ),\n",
    "                unittest.mock.call(\n",
    "                    \"my_test_topic_2\",\n",
    "                    b'{\"info\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"}, \"url\": \"https://www.ht.hr\"}',\n",
    "                    key=None,\n",
    "                ),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b114fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with LocalKafkaBroker() as bootstrap_server:\n",
    "    app = create_testing_app(bootstrap_servers=bootstrap_server)\n",
    "    fast_task = unittest.mock.Mock()\n",
    "    long_task = unittest.mock.Mock()\n",
    "\n",
    "    @app.run_in_background()\n",
    "    async def bg_task():\n",
    "        fast_task()\n",
    "        await asyncio.sleep(100)\n",
    "        long_task()\n",
    "\n",
    "    fast_task_second = unittest.mock.Mock()\n",
    "    long_task_second = unittest.mock.Mock()\n",
    "\n",
    "    @app.run_in_background()\n",
    "    async def bg_task_second():\n",
    "        fast_task_second()\n",
    "        await asyncio.sleep(100)\n",
    "        long_task_second()\n",
    "\n",
    "    try:\n",
    "        await app.startup()\n",
    "        await asyncio.sleep(5)\n",
    "    finally:\n",
    "        await app.shutdown()\n",
    "\n",
    "    fast_task.assert_called()\n",
    "    long_task.assert_not_called()\n",
    "\n",
    "    fast_task_second.assert_called()\n",
    "    long_task_second.assert_not_called()\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe047872",
   "metadata": {},
   "source": [
    "## Documentation generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880411a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def create_docs(self: FastKafka) -> None:\n",
    "    export_async_spec(\n",
    "        consumers={\n",
    "            topic: callback for topic, (callback, _) in self._consumers_store.items()\n",
    "        },\n",
    "        producers={\n",
    "            topic: callback for topic, (callback, _, _) in self._producers_store.items()\n",
    "        },\n",
    "        kafka_brokers=self._kafka_brokers,\n",
    "        kafka_service_info=self._kafka_service_info,\n",
    "        asyncapi_path=self._asyncapi_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec33cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = \"\"\"asyncapi: 2.5.0\n",
    "channels:\n",
    "  my_topic_1:\n",
    "    subscribe:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgUrl'\n",
    "  my_topic_2:\n",
    "    subscribe:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgEmail'\n",
    "  my_topic_3:\n",
    "    publish:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgUrl'\n",
    "  my_topic_4:\n",
    "    publish:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgEmail'\n",
    "  my_topic_5:\n",
    "    publish:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgUrl'\n",
    "components:\n",
    "  messages:\n",
    "    MyMsgEmail:\n",
    "      payload:\n",
    "        example:\n",
    "          email: agent-007@sis.gov.uk\n",
    "          msg_url:\n",
    "            info:\n",
    "              mobile: '+385987654321'\n",
    "              name: James Bond\n",
    "            url: https://sis.gov.uk/agents/007\n",
    "        properties:\n",
    "          email:\n",
    "            example: agent-007@sis.gov.uk\n",
    "            format: email\n",
    "            title: Email\n",
    "            type: string\n",
    "          msg_url:\n",
    "            allOf:\n",
    "            - $ref: '#/components/messages/MyMsgUrl'\n",
    "            example:\n",
    "              info:\n",
    "                mobile: '+385987654321'\n",
    "                name: James Bond\n",
    "              url: https://sis.gov.uk/agents/007\n",
    "            title: Msg Url\n",
    "        required:\n",
    "        - msg_url\n",
    "        - email\n",
    "        title: MyMsgEmail\n",
    "        type: object\n",
    "    MyMsgUrl:\n",
    "      payload:\n",
    "        example:\n",
    "          info:\n",
    "            mobile: '+385987654321'\n",
    "            name: James Bond\n",
    "          url: https://sis.gov.uk/agents/007\n",
    "        properties:\n",
    "          info:\n",
    "            allOf:\n",
    "            - $ref: '#/components/schemas/MyInfo'\n",
    "            example:\n",
    "              mobile: '+385987654321'\n",
    "              name: James Bond\n",
    "            title: Info\n",
    "          url:\n",
    "            example: https://sis.gov.uk/agents/007\n",
    "            format: uri\n",
    "            maxLength: 2083\n",
    "            minLength: 1\n",
    "            title: Url\n",
    "            type: string\n",
    "        required:\n",
    "        - info\n",
    "        - url\n",
    "        title: MyMsgUrl\n",
    "        type: object\n",
    "  schemas:\n",
    "    MyInfo:\n",
    "      payload:\n",
    "        properties:\n",
    "          mobile:\n",
    "            example: '+385987654321'\n",
    "            title: Mobile\n",
    "            type: string\n",
    "          name:\n",
    "            example: James Bond\n",
    "            title: Name\n",
    "            type: string\n",
    "        required:\n",
    "        - mobile\n",
    "        - name\n",
    "        title: MyInfo\n",
    "        type: object\n",
    "  securitySchemes: {}\n",
    "info:\n",
    "  contact:\n",
    "    email: noreply@gmail.com\n",
    "    name: Author\n",
    "    url: https://www.google.com\n",
    "  description: ''\n",
    "  title: ''\n",
    "  version: ''\n",
    "servers:\n",
    "  local:\n",
    "    description: Local (dev) Kafka broker\n",
    "    protocol: kafka\n",
    "    url: kafka\n",
    "    variables:\n",
    "      port:\n",
    "        default: '9092'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d2 = None, None\n",
    "\n",
    "docs_path = Path(\"/tmp/000_FastKafka/asyncapi/spec/asyncapi.yml\")\n",
    "if docs_path.exists():\n",
    "    os.remove(docs_path)\n",
    "\n",
    "\n",
    "async def test_me():\n",
    "    global d1\n",
    "    global d2\n",
    "    app = setup_testing_app(bootstrap_servers=bootstrap_server)\n",
    "    app.create_docs()\n",
    "    with open(docs_path) as specs:\n",
    "        d1 = yaml.safe_load(specs)\n",
    "        d2 = yaml.safe_load(expected)\n",
    "        assert d1 == d2, f\"{d1} != {d2}\"\n",
    "\n",
    "\n",
    "asyncio.run(test_me())\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafbc67",
   "metadata": {},
   "source": [
    "## App mocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class AwaitedMock:\n",
    "    @staticmethod\n",
    "    def _await_for(f: Callable[..., Any]) -> Callable[..., Any]:\n",
    "        @delegates(f)  # type: ignore\n",
    "        async def inner(*args, f=f, timeout: int = 60, **kwargs) -> Any:\n",
    "            if inspect.iscoroutinefunction(f):\n",
    "                return await asyncio.wait_for(f(*args, **kwargs), timeout=timeout)\n",
    "            else:\n",
    "                t0 = datetime.now()\n",
    "                e: Optional[Exception] = None\n",
    "                while True:\n",
    "                    try:\n",
    "                        return f(*args, **kwargs)\n",
    "                    except Exception as _e:\n",
    "                        await asyncio.sleep(1)\n",
    "                        e = _e\n",
    "\n",
    "                    if datetime.now() - t0 > timedelta(seconds=timeout):\n",
    "                        break\n",
    "\n",
    "                raise e\n",
    "\n",
    "        return inner  # type: ignore\n",
    "\n",
    "    def __init__(self, o: Any):\n",
    "        self._o = o\n",
    "\n",
    "        for name in o.__dir__():\n",
    "            if not name.startswith(\"_\"):\n",
    "                f = getattr(o, name)\n",
    "                if inspect.ismethod(f):\n",
    "                    setattr(self, name, self._await_for(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def create_mocks(self: FastKafka) -> None:\n",
    "    \"\"\"Creates self.mocks as a named tuple mapping a new function obtained by calling the original functions and a mock\"\"\"\n",
    "    app_methods = [f for f, _ in self._consumers_store.values()] + [\n",
    "        f for f, _, _ in self._producers_store.values()\n",
    "    ]\n",
    "    self.AppMocks = namedtuple(  # type: ignore\n",
    "        f\"{self.__class__.__name__}Mocks\", [f.__name__ for f in app_methods]\n",
    "    )\n",
    "\n",
    "    self.mocks = self.AppMocks(  # type: ignore\n",
    "        **{\n",
    "            f.__name__: AsyncMock() if inspect.iscoroutinefunction(f) else MagicMock()\n",
    "            for f in app_methods\n",
    "        }\n",
    "    )\n",
    "\n",
    "    self.awaited_mocks = self.AppMocks(  # type: ignore\n",
    "        **{name: AwaitedMock(mock) for name, mock in self.mocks._asdict().items()}\n",
    "    )\n",
    "\n",
    "    def add_mock(\n",
    "        f: Callable[..., Any], mock: Union[AsyncMock, MagicMock]\n",
    "    ) -> Callable[..., Any]:\n",
    "        \"\"\"Add call to mock when calling function f\"\"\"\n",
    "\n",
    "        @functools.wraps(f)\n",
    "        async def async_inner(\n",
    "            *args: Any, f: Callable[..., Any] = f, mock: AsyncMock = mock, **kwargs: Any\n",
    "        ) -> Any:\n",
    "            await mock(*args, **kwargs)\n",
    "            return await f(*args, **kwargs)\n",
    "\n",
    "        @functools.wraps(f)\n",
    "        def sync_inner(\n",
    "            *args: Any, f: Callable[..., Any] = f, mock: MagicMock = mock, **kwargs: Any\n",
    "        ) -> Any:\n",
    "            mock(*args, **kwargs)\n",
    "            return f(*args, **kwargs)\n",
    "\n",
    "        if inspect.iscoroutinefunction(f):\n",
    "            return async_inner\n",
    "        else:\n",
    "            return sync_inner\n",
    "\n",
    "    self._consumers_store.update(\n",
    "        {\n",
    "            name: (\n",
    "                add_mock(f, getattr(self.mocks, f.__name__)),\n",
    "                kwargs,\n",
    "            )\n",
    "            for name, (f, kwargs) in self._consumers_store.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "    self._producers_store.update(\n",
    "        {\n",
    "            name: (\n",
    "                add_mock(f, getattr(self.mocks, f.__name__)),\n",
    "                producer,\n",
    "                kwargs,\n",
    "            )\n",
    "            for name, (f, producer, kwargs) in self._producers_store.items()\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMsg(BaseModel):\n",
    "    msg: str = Field(...)\n",
    "\n",
    "\n",
    "app = FastKafka(bootstrap_servers=\"localhost:9092\")\n",
    "\n",
    "app = FastKafka(bootstrap_servers=\"localhost:9092\")\n",
    "\n",
    "\n",
    "@app.consumes()\n",
    "async def on_preprocessed_signals(msg: TestMsg):\n",
    "    await to_predictions(TestMsg(msg=\"prediction\"))\n",
    "\n",
    "\n",
    "@app.produces()\n",
    "async def to_predictions(prediction: TestMsg) -> TestMsg:\n",
    "    print(f\"Sending prediction: {prediction}\")\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca781c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.create_mocks()\n",
    "app.mocks.on_preprocessed_signals.assert_not_awaited()\n",
    "app.mocks.to_predictions.assert_not_awaited()\n",
    "app.create_mocks()\n",
    "app.mocks.on_preprocessed_signals.assert_not_awaited()\n",
    "app.mocks.to_predictions.assert_not_awaited()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b07708",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(AssertionError) as e:\n",
    "    await app.awaited_mocks.on_preprocessed_signals.assert_called_with(123, timeout=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.create_mocks()\n",
    "app.mocks.on_preprocessed_signals.assert_not_awaited()\n",
    "await app.awaited_mocks.on_preprocessed_signals.assert_not_awaited(timeout=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716526fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
