{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff734a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _application.app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5096aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import asyncio\n",
    "import functools\n",
    "import inspect\n",
    "import json\n",
    "import types\n",
    "from asyncio import iscoroutinefunction  # do not use the version from inspect\n",
    "from collections import namedtuple\n",
    "from contextlib import AbstractAsyncContextManager\n",
    "from datetime import datetime, timedelta\n",
    "from functools import wraps\n",
    "from inspect import signature\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from unittest.mock import AsyncMock, MagicMock\n",
    "\n",
    "import anyio\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "from pydantic import BaseModel\n",
    "from pydantic.main import ModelMetaclass\n",
    "\n",
    "import fastkafka._components.logger\n",
    "\n",
    "fastkafka._components.logger.should_supress_timestamps = True\n",
    "\n",
    "import fastkafka\n",
    "from fastkafka._components.aiokafka_consumer_loop import (\n",
    "    aiokafka_consumer_loop,\n",
    "    sanitize_kafka_config,\n",
    ")\n",
    "from fastkafka._components.aiokafka_producer_manager import AIOKafkaProducerManager\n",
    "from fastkafka._components.asyncapi import (\n",
    "    ConsumeCallable,\n",
    "    ContactInfo,\n",
    "    KafkaBroker,\n",
    "    KafkaBrokers,\n",
    "    KafkaServiceInfo,\n",
    "    export_async_spec,\n",
    ")\n",
    "from fastkafka._components.benchmarking import _benchmark\n",
    "from fastkafka._components.logger import get_logger\n",
    "from fastkafka._components.meta import delegates, export, filter_using_signature, patch\n",
    "from fastkafka._components.producer_decorator import ProduceCallable, producer_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddeaf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncer\n",
    "\n",
    "from fastkafka._components.encoder.avro import avro_decoder, avro_encoder\n",
    "from fastkafka._components.encoder.json import json_decoder, json_encoder\n",
    "from fastkafka._components.logger import supress_timestamps\n",
    "from fastkafka.testing import Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf16b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "supress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedeee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import unittest.mock\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "import pytest\n",
    "import yaml\n",
    "from pydantic import EmailStr, Field, HttpUrl\n",
    "\n",
    "from fastkafka.testing import ApacheKafkaBroker, mock_AIOKafkaProducer_send, true_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9177ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "# allows async calls in notebooks\n",
    "\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211534a4",
   "metadata": {},
   "source": [
    "### Constructor utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@delegates(AIOKafkaConsumer, but=[\"bootstrap_servers\"])\n",
    "@delegates(AIOKafkaProducer, but=[\"bootstrap_servers\"], keep=True)\n",
    "def _get_kafka_config(\n",
    "    **kwargs: Any,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Get kafka config\"\"\"\n",
    "    allowed_keys = set(signature(_get_kafka_config).parameters.keys())\n",
    "    if not set(kwargs.keys()) <= allowed_keys:\n",
    "        unallowed_keys = \", \".join(\n",
    "            sorted([f\"'{x}'\" for x in set(kwargs.keys()).difference(allowed_keys)])\n",
    "        )\n",
    "        raise ValueError(f\"Unallowed key arguments passed: {unallowed_keys}\")\n",
    "    retval = kwargs.copy()\n",
    "\n",
    "    # todo: check this values\n",
    "    config_defaults = {\n",
    "        \"bootstrap_servers\": \"localhost:9092\",\n",
    "        \"auto_offset_reset\": \"earliest\",\n",
    "        \"max_poll_records\": 100,\n",
    "        #         \"max_buffer_size\": 10_000,\n",
    "    }\n",
    "    for key, value in config_defaults.items():\n",
    "        if key not in retval:\n",
    "            retval[key] = value\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _get_kafka_config() == {\n",
    "    \"bootstrap_servers\": \"localhost:9092\",\n",
    "    \"auto_offset_reset\": \"earliest\",\n",
    "    \"max_poll_records\": 100,\n",
    "}\n",
    "\n",
    "assert _get_kafka_config(max_poll_records=1_000) == {\n",
    "    \"bootstrap_servers\": \"localhost:9092\",\n",
    "    \"auto_offset_reset\": \"earliest\",\n",
    "    \"max_poll_records\": 1_000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b17b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(ValueError) as e:\n",
    "    _get_kafka_config(random_key=1_000, whatever=\"whocares\")\n",
    "assert e.value.args == (\"Unallowed key arguments passed: 'random_key', 'whatever'\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3477cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_kafka_brokers(kafka_brokers: Optional[Dict[str, Any]] = None) -> KafkaBrokers:\n",
    "    \"\"\"Get Kafka brokers\n",
    "\n",
    "    Args:\n",
    "        kafka_brokers: Kafka brokers\n",
    "\n",
    "    \"\"\"\n",
    "    if kafka_brokers is None:\n",
    "        retval: KafkaBrokers = KafkaBrokers(\n",
    "            brokers={\n",
    "                \"localhost\": KafkaBroker(\n",
    "                    url=\"https://localhost\",\n",
    "                    description=\"Local (dev) Kafka broker\",\n",
    "                    port=\"9092\",\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        retval = KafkaBrokers(\n",
    "            brokers={\n",
    "                k: KafkaBroker.parse_raw(\n",
    "                    v.json() if hasattr(v, \"json\") else json.dumps(v)\n",
    "                )\n",
    "                for k, v in kafka_brokers.items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    _get_kafka_brokers(None).json()\n",
    "    == '{\"brokers\": {\"localhost\": {\"url\": \"https://localhost\", \"description\": \"Local (dev) Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n",
    ")\n",
    "\n",
    "assert (\n",
    "    _get_kafka_brokers(dict(localhost=dict(url=\"localhost\"))).json()\n",
    "    == '{\"brokers\": {\"localhost\": {\"url\": \"localhost\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n",
    ")\n",
    "\n",
    "assert (\n",
    "    _get_kafka_brokers(\n",
    "        dict(localhost=dict(url=\"localhost\"), staging=dict(url=\"staging.airt.ai\"))\n",
    "    ).json()\n",
    "    == '{\"brokers\": {\"localhost\": {\"url\": \"localhost\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}, \"staging\": {\"url\": \"staging.airt.ai\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_topic_name(\n",
    "    topic_callable: Union[ConsumeCallable, ProduceCallable], prefix: str = \"on_\"\n",
    ") -> str:\n",
    "    \"\"\"Get topic name\n",
    "    Args:\n",
    "        topic_callable: a function\n",
    "        prefix: prefix of the name of the function followed by the topic name\n",
    "\n",
    "    Returns:\n",
    "        The name of the topic\n",
    "    \"\"\"\n",
    "    topic = topic_callable.__name__\n",
    "    if not topic.startswith(prefix) or len(topic) <= len(prefix):\n",
    "        raise ValueError(f\"Function name '{topic}' must start with {prefix}\")\n",
    "    topic = topic[len(prefix) :]\n",
    "\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_topic_name_1():\n",
    "    pass\n",
    "\n",
    "\n",
    "assert _get_topic_name(on_topic_name_1) == \"topic_name_1\"\n",
    "\n",
    "assert _get_topic_name(on_topic_name_1, prefix=\"on_topic_\") == \"name_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_contact_info(\n",
    "    name: str = \"Author\",\n",
    "    url: str = \"https://www.google.com\",\n",
    "    email: str = \"noreply@gmail.com\",\n",
    ") -> ContactInfo:\n",
    "    return ContactInfo(name=name, url=url, email=email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e311f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _get_contact_info() == ContactInfo(\n",
    "    name=\"Author\",\n",
    "    url=HttpUrl(url=\"https://www.google.com\", scheme=\"http\"),\n",
    "    email=\"noreply@gmail.com\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eeeb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "I = TypeVar(\"I\", bound=BaseModel)\n",
    "O = TypeVar(\"O\", BaseModel, Awaitable[BaseModel])\n",
    "\n",
    "F = TypeVar(\"F\", bound=Callable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c37353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@export(\"fastkafka\")\n",
    "class FastKafka:\n",
    "    @delegates(_get_kafka_config)\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        title: Optional[str] = None,\n",
    "        description: Optional[str] = None,\n",
    "        version: Optional[str] = None,\n",
    "        contact: Optional[Dict[str, str]] = None,\n",
    "        kafka_brokers: Dict[str, Any],\n",
    "        root_path: Optional[Union[Path, str]] = None,\n",
    "        lifespan: Optional[Callable[[\"FastKafka\"], AsyncContextManager[None]]] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        \"\"\"Creates FastKafka application\n",
    "\n",
    "        Args:\n",
    "            title: optional title for the documentation. If None,\n",
    "                the title will be set to empty string\n",
    "            description: optional description for the documentation. If\n",
    "                None, the description will be set to empty string\n",
    "            version: optional version for the documentation. If None,\n",
    "                the version will be set to empty string\n",
    "            contact: optional contact for the documentation. If None, the\n",
    "                contact will be set to placeholder values:\n",
    "                name='Author' url=HttpUrl('https://www.google.com', ) email='noreply@gmail.com'\n",
    "            kafka_brokers: dictionary describing kafka brokers used for\n",
    "                generating documentation\n",
    "            root_path: path to where documentation will be created\n",
    "            lifespan: asynccontextmanager that is used for setting lifespan hooks.\n",
    "                __aenter__ is called before app start and __aexit__ after app stop.\n",
    "                The lifespan is called whe application is started as async context\n",
    "                manager, e.g.:`async with kafka_app...`\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # this is needed for documentation generation\n",
    "        self._title = title if title is not None else \"\"\n",
    "        self._description = description if description is not None else \"\"\n",
    "        self._version = version if version is not None else \"\"\n",
    "        if contact is not None:\n",
    "            self._contact_info = _get_contact_info(**contact)\n",
    "        else:\n",
    "            self._contact_info = _get_contact_info()\n",
    "\n",
    "        self._kafka_service_info = KafkaServiceInfo(\n",
    "            title=self._title,\n",
    "            version=self._version,\n",
    "            description=self._description,\n",
    "            contact=self._contact_info,\n",
    "        )\n",
    "        self._kafka_brokers = _get_kafka_brokers(kafka_brokers)\n",
    "\n",
    "        self._root_path = Path(\".\") if root_path is None else Path(root_path)\n",
    "\n",
    "        self._asyncapi_path = self._root_path / \"asyncapi\"\n",
    "        (self._asyncapi_path / \"docs\").mkdir(exist_ok=True, parents=True)\n",
    "        (self._asyncapi_path / \"spec\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # this is used as default parameters for creating AIOProducer and AIOConsumer objects\n",
    "        self._kafka_config = _get_kafka_config(**kwargs)\n",
    "\n",
    "        #\n",
    "        self._consumers_store: Dict[\n",
    "            str,\n",
    "            Tuple[\n",
    "                ConsumeCallable, Callable[[bytes, ModelMetaclass], Any], Dict[str, Any]\n",
    "            ],\n",
    "        ] = {}\n",
    "\n",
    "        self._producers_store: Dict[  # type: ignore\n",
    "            str, Tuple[ProduceCallable, AIOKafkaProducer, Dict[str, Any]]\n",
    "        ] = {}\n",
    "\n",
    "        self._producers_list: List[  # type: ignore\n",
    "            Union[AIOKafkaProducer, AIOKafkaProducerManager]\n",
    "        ] = []\n",
    "\n",
    "        self.benchmark_results: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "        # background tasks\n",
    "        self._scheduled_bg_tasks: List[Callable[..., Coroutine[Any, Any, Any]]] = []\n",
    "        self._bg_task_group_generator: Optional[anyio.abc.TaskGroup] = None\n",
    "        self._bg_tasks_group: Optional[anyio.abc.TaskGroup] = None\n",
    "\n",
    "        # todo: use this for errrors\n",
    "        self._on_error_topic: Optional[str] = None\n",
    "\n",
    "        self.lifespan = lifespan\n",
    "        self.lifespan_ctx: Optional[AsyncContextManager[None]] = None\n",
    "\n",
    "        self._is_started: bool = False\n",
    "        self._is_shutting_down: bool = False\n",
    "        self._kafka_consumer_tasks: List[asyncio.Task[Any]] = []\n",
    "        self._kafka_producer_tasks: List[asyncio.Task[Any]] = []\n",
    "        self._running_bg_tasks: List[asyncio.Task[Any]] = []\n",
    "        self.run = False\n",
    "\n",
    "        # testing functions\n",
    "        self.AppMocks = None\n",
    "        self.mocks = None\n",
    "        self.awaited_mocks = None\n",
    "\n",
    "    @property\n",
    "    def is_started(self) -> bool:\n",
    "        return self._is_started\n",
    "\n",
    "    def _set_bootstrap_servers(self, bootstrap_servers: str) -> None:\n",
    "        self._kafka_config[\"bootstrap_servers\"] = bootstrap_servers\n",
    "\n",
    "    def set_kafka_broker(self, kafka_broker_name: str) -> None:\n",
    "        if kafka_broker_name not in self._kafka_brokers.brokers:\n",
    "            raise ValueError(\n",
    "                f\"Given kafka_broker_name '{kafka_broker_name}' is not found in kafka_brokers, available options are {self._kafka_brokers.brokers.keys()}\"\n",
    "            )\n",
    "\n",
    "        broker_to_use = self._kafka_brokers.brokers[kafka_broker_name]\n",
    "        bootstrap_servers = f\"{broker_to_use.url}:{broker_to_use.port}\"\n",
    "        logger.info(\n",
    "            f\"set_kafka_broker() : Setting bootstrap_servers value to '{bootstrap_servers}'\"\n",
    "        )\n",
    "        self._set_bootstrap_servers(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "    async def __aenter__(self) -> \"FastKafka\":\n",
    "        if self.lifespan is not None:\n",
    "            self.lifespan_ctx = self.lifespan(self)\n",
    "            await self.lifespan_ctx.__aenter__()\n",
    "        await self._start()\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(\n",
    "        self,\n",
    "        exc_type: Optional[Type[BaseException]],\n",
    "        exc: Optional[BaseException],\n",
    "        tb: Optional[types.TracebackType],\n",
    "    ) -> None:\n",
    "        await self._stop()\n",
    "        if self.lifespan_ctx is not None:\n",
    "            await self.lifespan_ctx.__aexit__(exc_type, exc, tb)\n",
    "\n",
    "    async def _start(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _stop(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def consumes(\n",
    "        self,\n",
    "        topic: Optional[str] = None,\n",
    "        decoder: str = \"json\",\n",
    "        *,\n",
    "        prefix: str = \"on_\",\n",
    "        **kwargs: Dict[str, Any],\n",
    "    ) -> ConsumeCallable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def produces(  # type: ignore\n",
    "        self,\n",
    "        topic: Optional[str] = None,\n",
    "        encoder: str = \"json\",\n",
    "        *,\n",
    "        prefix: str = \"to_\",\n",
    "        producer: Optional[AIOKafkaProducer] = None,\n",
    "        **kwargs: Dict[str, Any],\n",
    "    ) -> ProduceCallable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def benchmark(\n",
    "        self,\n",
    "        interval: Union[int, timedelta] = 1,\n",
    "        *,\n",
    "        sliding_window_size: Optional[int] = None,\n",
    "    ) -> Callable[[F], F]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_in_background(\n",
    "        self,\n",
    "    ) -> Callable[[], Any]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _populate_consumers(\n",
    "        self,\n",
    "        is_shutting_down_f: Callable[[], bool],\n",
    "    ) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_topics(self) -> Iterable[str]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _populate_producers(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _populate_bg_tasks(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_docs(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_mocks(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _shutdown_consumers(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _shutdown_producers(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _shutdown_bg_tasks(self) -> None:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert FastKafka.__module__ == \"fastkafka\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894af799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_title': '',\n",
       " '_description': '',\n",
       " '_version': '',\n",
       " '_contact_info': ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com'),\n",
       " '_kafka_service_info': KafkaServiceInfo(title='', version='', description='', contact=ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')),\n",
       " '_kafka_brokers': KafkaBrokers(brokers={'localhost': KafkaBroker(url='localhost', description='Kafka broker', port='9092', protocol='kafka', security=None)}),\n",
       " '_root_path': PosixPath('.'),\n",
       " '_asyncapi_path': PosixPath('asyncapi'),\n",
       " '_kafka_config': {'bootstrap_servers': 'localhost:9092',\n",
       "  'auto_offset_reset': 'earliest',\n",
       "  'max_poll_records': 100},\n",
       " '_consumers_store': {},\n",
       " '_producers_store': {},\n",
       " '_producers_list': [],\n",
       " 'benchmark_results': {},\n",
       " '_scheduled_bg_tasks': [],\n",
       " '_bg_task_group_generator': None,\n",
       " '_bg_tasks_group': None,\n",
       " '_on_error_topic': None,\n",
       " 'lifespan': None,\n",
       " 'lifespan_ctx': None,\n",
       " '_is_started': False,\n",
       " '_is_shutting_down': False,\n",
       " '_kafka_consumer_tasks': [],\n",
       " '_kafka_producer_tasks': [],\n",
       " '_running_bg_tasks': [],\n",
       " 'run': False,\n",
       " 'AppMocks': None,\n",
       " 'mocks': None,\n",
       " 'awaited_mocks': None}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_app = FastKafka(kafka_brokers=dict(localhost=dict(url=\"localhost\", port=9092)))\n",
    "kafka_app.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b99de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_title': '',\n",
       " '_description': '',\n",
       " '_version': '',\n",
       " '_contact_info': ContactInfo(name='Davor', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com'),\n",
       " '_kafka_service_info': KafkaServiceInfo(title='', version='', description='', contact=ContactInfo(name='Davor', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')),\n",
       " '_kafka_brokers': KafkaBrokers(brokers={'localhost': KafkaBroker(url='localhost', description='Kafka broker', port='9092', protocol='kafka', security=None)}),\n",
       " '_root_path': PosixPath('.'),\n",
       " '_asyncapi_path': PosixPath('asyncapi'),\n",
       " '_kafka_config': {'bootstrap_servers': 'localhost:9092',\n",
       "  'auto_offset_reset': 'earliest',\n",
       "  'max_poll_records': 100},\n",
       " '_consumers_store': {},\n",
       " '_producers_store': {},\n",
       " '_producers_list': [],\n",
       " 'benchmark_results': {},\n",
       " '_scheduled_bg_tasks': [],\n",
       " '_bg_task_group_generator': None,\n",
       " '_bg_tasks_group': None,\n",
       " '_on_error_topic': None,\n",
       " 'lifespan': None,\n",
       " 'lifespan_ctx': None,\n",
       " '_is_started': False,\n",
       " '_is_shutting_down': False,\n",
       " '_kafka_consumer_tasks': [],\n",
       " '_kafka_producer_tasks': [],\n",
       " '_running_bg_tasks': [],\n",
       " 'run': False,\n",
       " 'AppMocks': None,\n",
       " 'mocks': None,\n",
       " 'awaited_mocks': None}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_app = FastKafka(\n",
    "    contact={\"name\": \"Davor\"},\n",
    "    kafka_brokers=dict(localhost=dict(url=\"localhost\", port=9092)),\n",
    ")\n",
    "kafka_app.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfbe17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_app(\n",
    "    *, root_path: str = \"/tmp/000_FastKafka\", bootstrap_servers: Optional[str] = None\n",
    "):\n",
    "    if Path(root_path).exists():\n",
    "        shutil.rmtree(root_path)\n",
    "\n",
    "    host, port = None, None\n",
    "    if bootstrap_servers is not None:\n",
    "        host, port = bootstrap_servers.split(\":\")\n",
    "\n",
    "    kafka_app = FastKafka(\n",
    "        kafka_brokers={\n",
    "            \"localhost\": {\n",
    "                \"url\": host if host is not None else \"localhost\",\n",
    "                \"name\": \"development\",\n",
    "                \"description\": \"Local (dev) Kafka broker\",\n",
    "                \"port\": port if port is not None else 9092,\n",
    "            }\n",
    "        },\n",
    "        root_path=root_path,\n",
    "    )\n",
    "    kafka_app.set_kafka_broker(kafka_broker_name=\"localhost\")\n",
    "\n",
    "    return kafka_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66237424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fastkafka.FastKafka>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = create_testing_app()\n",
    "assert Path(\"/tmp/000_FastKafka\").exists()\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebbf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_decoder_fn(decoder: str) -> Callable[[bytes, ModelMetaclass], Any]:\n",
    "    \"\"\"\n",
    "    Imports and returns decoder function based on input\n",
    "    \"\"\"\n",
    "    if decoder == \"json\":\n",
    "        from fastkafka._components.encoder.json import json_decoder\n",
    "\n",
    "        return json_decoder\n",
    "    elif decoder == \"avro\":\n",
    "        try:\n",
    "            from fastkafka._components.encoder.avro import avro_decoder\n",
    "        except ModuleNotFoundError:\n",
    "            raise ModuleNotFoundError(\n",
    "                \"Unable to import avro packages. Please install FastKafka using the command 'fastkafka[avro]'\"\n",
    "            )\n",
    "        return avro_decoder\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown decoder - {decoder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6715c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = _get_decoder_fn(\"json\")\n",
    "assert actual == json_decoder\n",
    "\n",
    "actual = _get_decoder_fn(\"avro\")\n",
    "assert actual == avro_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "@delegates(AIOKafkaConsumer)\n",
    "def consumes(\n",
    "    self: FastKafka,\n",
    "    topic: Optional[str] = None,\n",
    "    decoder: str = \"json\",\n",
    "    *,\n",
    "    prefix: str = \"on_\",\n",
    "    **kwargs: Dict[str, Any],\n",
    ") -> Callable[[ConsumeCallable], ConsumeCallable]:\n",
    "    \"\"\"Decorator registering the callback called when a message is received in a topic.\n",
    "\n",
    "    This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.\n",
    "\n",
    "    Args:\n",
    "        topic: Kafka topic that the consumer will subscribe to and execute the\n",
    "            decorated function when it receives a message from the topic,\n",
    "            default: None. If the topic is not specified, topic name will be\n",
    "            inferred from the decorated function name by stripping the defined prefix\n",
    "        decoder: Decoder to use to decode messages consumed from the topic,\n",
    "                default: json - By default, it uses json decoder to decode\n",
    "                bytes to json string and then it creates instance of pydantic\n",
    "                BaseModel\n",
    "        prefix: Prefix stripped from the decorated function to define a topic name\n",
    "            if the topic argument is not passed, default: \"on_\". If the decorated\n",
    "            function name is not prefixed with the defined prefix and topic argument\n",
    "            is not passed, then this method will throw ValueError\n",
    "\n",
    "    Returns:\n",
    "        A function returning the same function\n",
    "\n",
    "    Throws:\n",
    "        ValueError\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _decorator(\n",
    "        on_topic: ConsumeCallable,\n",
    "        topic: Optional[str] = topic,\n",
    "        decoder: str = decoder,\n",
    "        kwargs: Dict[str, Any] = kwargs,\n",
    "    ) -> ConsumeCallable:\n",
    "        topic_resolved: str = (\n",
    "            _get_topic_name(topic_callable=on_topic, prefix=prefix)\n",
    "            if topic is None\n",
    "            else topic\n",
    "        )\n",
    "\n",
    "        decoder_fn = _get_decoder_fn(decoder)\n",
    "        self._consumers_store[topic_resolved] = (on_topic, decoder_fn, kwargs)\n",
    "\n",
    "        return on_topic\n",
    "\n",
    "    return _decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb7fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'\n"
     ]
    }
   ],
   "source": [
    "app = create_testing_app()\n",
    "\n",
    "\n",
    "# Basic check\n",
    "@app.consumes()\n",
    "def on_my_topic_1(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._consumers_store[\"my_topic_1\"] == (\n",
    "    on_my_topic_1,\n",
    "    json_decoder,\n",
    "    {},\n",
    "), app._consumers_store\n",
    "\n",
    "\n",
    "# Check topic setting\n",
    "@app.consumes(topic=\"test_topic_2\")\n",
    "def some_func_name(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._consumers_store[\"test_topic_2\"] == (\n",
    "    some_func_name,\n",
    "    json_decoder,\n",
    "    {},\n",
    "), app._consumers_store\n",
    "\n",
    "\n",
    "# Check prefix change\n",
    "@app.consumes(prefix=\"for_\")\n",
    "def for_test_topic_3(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._consumers_store[\"test_topic_3\"] == (\n",
    "    for_test_topic_3,\n",
    "    json_decoder,\n",
    "    {},\n",
    "), app._consumers_store\n",
    "\n",
    "# Check passing of kwargs\n",
    "kwargs = {\"arg1\": \"val1\", \"arg2\": 2}\n",
    "\n",
    "\n",
    "@app.consumes(topic=\"test_topic\", **kwargs)\n",
    "def for_test_kwargs(msg: BaseModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._consumers_store[\"test_topic\"] == (\n",
    "    for_test_kwargs,\n",
    "    json_decoder,\n",
    "    kwargs,\n",
    "), app._consumers_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01256d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_encoder_fn(encoder: str) -> Callable[[BaseModel], bytes]:\n",
    "    \"\"\"\n",
    "    Imports and returns encoder function based on input\n",
    "    \"\"\"\n",
    "    if encoder == \"json\":\n",
    "        from fastkafka._components.encoder.json import json_encoder\n",
    "\n",
    "        return json_encoder\n",
    "    elif encoder == \"avro\":\n",
    "        try:\n",
    "            from fastkafka._components.encoder.avro import avro_encoder\n",
    "        except ModuleNotFoundError:\n",
    "            raise ModuleNotFoundError(\n",
    "                \"Unable to import avro packages. Please install FastKafka using the command 'fastkafka[avro]'\"\n",
    "            )\n",
    "        return avro_encoder\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown encoder - {encoder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804434e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = _get_encoder_fn(\"json\")\n",
    "assert actual == json_encoder\n",
    "\n",
    "actual = _get_encoder_fn(\"avro\")\n",
    "assert actual == avro_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e269659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "@delegates(AIOKafkaProducer)\n",
    "def produces(\n",
    "    self: FastKafka,\n",
    "    topic: Optional[str] = None,\n",
    "    encoder: str = \"json\",\n",
    "    *,\n",
    "    prefix: str = \"to_\",\n",
    "    **kwargs: Dict[str, Any],\n",
    ") -> Callable[[ProduceCallable], ProduceCallable]:\n",
    "    \"\"\"Decorator registering the callback called when delivery report for a produced message is received\n",
    "\n",
    "    This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.\n",
    "\n",
    "    Args:\n",
    "        topic: Kafka topic that the producer will send returned values from\n",
    "            the decorated function to, default: None- If the topic is not\n",
    "            specified, topic name will be inferred from the decorated function\n",
    "            name by stripping the defined prefix.\n",
    "        encoder: Encoder to use to encode messages before sending it to topic,\n",
    "                default: json - By default, it uses json encoder to convert\n",
    "                pydantic basemodel to json string and then encodes the string to bytes\n",
    "                using 'utf-8' encoding\n",
    "        prefix: Prefix stripped from the decorated function to define a topic\n",
    "            name if the topic argument is not passed, default: \"to_\". If the\n",
    "            decorated function name is not prefixed with the defined prefix\n",
    "            and topic argument is not passed, then this method will throw ValueError\n",
    "\n",
    "    Returns:\n",
    "        A function returning the same function\n",
    "\n",
    "    Raises:\n",
    "        ValueError: when needed\n",
    "    \"\"\"\n",
    "\n",
    "    def _decorator(\n",
    "        on_topic: ProduceCallable,\n",
    "        topic: Optional[str] = topic,\n",
    "        kwargs: Dict[str, Any] = kwargs,\n",
    "    ) -> ProduceCallable:\n",
    "        topic_resolved: str = (\n",
    "            _get_topic_name(topic_callable=on_topic, prefix=prefix)\n",
    "            if topic is None\n",
    "            else topic\n",
    "        )\n",
    "\n",
    "        self._producers_store[topic_resolved] = (on_topic, None, kwargs)\n",
    "        encoder_fn = _get_encoder_fn(encoder)\n",
    "        return producer_decorator(\n",
    "            self._producers_store, on_topic, topic_resolved, encoder_fn=encoder_fn\n",
    "        )\n",
    "\n",
    "    return _decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7cede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'\n"
     ]
    }
   ],
   "source": [
    "app = create_testing_app()\n",
    "\n",
    "\n",
    "# Basic check\n",
    "async def to_my_topic_1(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Must be done without sugar to keep the original function reference\n",
    "check_func = to_my_topic_1\n",
    "to_my_topic_1 = app.produces()(to_my_topic_1)\n",
    "\n",
    "assert app._producers_store[\"my_topic_1\"] == (\n",
    "    check_func,\n",
    "    None,\n",
    "    {},\n",
    "), f\"{app._producers_store}, {to_my_topic_1}\"\n",
    "\n",
    "\n",
    "# Check topic setting\n",
    "def some_func_name(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "check_func = some_func_name\n",
    "some_func_name = app.produces(topic=\"test_topic_2\")(some_func_name)\n",
    "\n",
    "assert app._producers_store[\"test_topic_2\"] == (\n",
    "    check_func,\n",
    "    None,\n",
    "    {},\n",
    "), app._producers_store\n",
    "\n",
    "\n",
    "# Check prefix change\n",
    "@app.produces(prefix=\"for_\")\n",
    "def for_test_topic_3(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "check_func = for_test_topic_3\n",
    "some_func_name = app.produces(prefix=\"for_\")(for_test_topic_3)\n",
    "\n",
    "assert app._producers_store[\"test_topic_3\"] == (\n",
    "    check_func,\n",
    "    None,\n",
    "    {},\n",
    "), app._producers_store\n",
    "\n",
    "# Check passing of kwargs\n",
    "kwargs = {\"arg1\": \"val1\", \"arg2\": 2}\n",
    "\n",
    "\n",
    "async def for_test_kwargs(msg: BaseModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "check_func = for_test_kwargs\n",
    "for_test_kwargs = app.produces(topic=\"test_topic\", **kwargs)(for_test_kwargs)\n",
    "\n",
    "assert app._producers_store[\"test_topic\"] == (\n",
    "    check_func,\n",
    "    None,\n",
    "    kwargs,\n",
    "), app._producers_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d507987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def get_topics(self: FastKafka) -> Iterable[str]:\n",
    "    produce_topics = set(self._producers_store.keys())\n",
    "    consume_topics = set(self._consumers_store.keys())\n",
    "    return consume_topics.union(produce_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac053363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'\n"
     ]
    }
   ],
   "source": [
    "app = create_testing_app()\n",
    "\n",
    "\n",
    "@app.produces()\n",
    "def to_topic_1() -> BaseModel:\n",
    "    pass\n",
    "\n",
    "\n",
    "@app.consumes()\n",
    "def on_topic_2(msg: BaseModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app.get_topics() == set([\"topic_1\", \"topic_2\"]), f\"{app.get_topics()=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4744bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def run_in_background(\n",
    "    self: FastKafka,\n",
    ") -> Callable[\n",
    "    [Callable[..., Coroutine[Any, Any, Any]]], Callable[..., Coroutine[Any, Any, Any]]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Decorator to schedule a task to be run in the background.\n",
    "\n",
    "    This decorator is used to schedule a task to be run in the background when the app's `_on_startup` event is triggered.\n",
    "\n",
    "    Returns:\n",
    "        Callable[None, None]: A decorator function that takes a background task as an input and stores it to be run in the backround.\n",
    "    \"\"\"\n",
    "\n",
    "    def _decorator(\n",
    "        bg_task: Callable[..., Coroutine[Any, Any, Any]]\n",
    "    ) -> Callable[..., Coroutine[Any, Any, Any]]:\n",
    "        \"\"\"\n",
    "        Store the background task.\n",
    "\n",
    "        Args:\n",
    "            bg_task (Callable[[], None]): The background task to be run asynchronously.\n",
    "\n",
    "        Returns:\n",
    "            Callable[[], None]: Original background task.\n",
    "        \"\"\"\n",
    "        logger.info(\n",
    "            f\"run_in_background() : Adding function '{bg_task.__name__}' as background task\"\n",
    "        )\n",
    "        self._scheduled_bg_tasks.append(bg_task)\n",
    "\n",
    "        return bg_task\n",
    "\n",
    "    return _decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16917ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'\n",
      "[INFO] __main__: run_in_background() : Adding function 'async_background_job' as background task\n"
     ]
    }
   ],
   "source": [
    "# Check if the background job is getting registered\n",
    "\n",
    "app = create_testing_app()\n",
    "\n",
    "\n",
    "@app.run_in_background()\n",
    "async def async_background_job():\n",
    "    \"\"\"Async background job\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._scheduled_bg_tasks[0] == async_background_job, app._scheduled_bg_tasks[0]\n",
    "assert app._scheduled_bg_tasks.__len__() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInfo(BaseModel):\n",
    "    mobile: str = Field(..., example=\"+385987654321\")\n",
    "    name: str = Field(..., example=\"James Bond\")\n",
    "\n",
    "\n",
    "class MyMsgUrl(BaseModel):\n",
    "    info: MyInfo = Field(..., example=dict(mobile=\"+385987654321\", name=\"James Bond\"))\n",
    "    url: HttpUrl = Field(..., example=\"https://sis.gov.uk/agents/007\")\n",
    "\n",
    "\n",
    "class MyMsgEmail(BaseModel):\n",
    "    msg_url: MyMsgUrl = Field(\n",
    "        ...,\n",
    "        example=dict(\n",
    "            info=dict(mobile=\"+385987654321\", name=\"James Bond\"),\n",
    "            url=\"https://sis.gov.uk/agents/007\",\n",
    "        ),\n",
    "    )\n",
    "    email: EmailStr = Field(..., example=\"agent-007@sis.gov.uk\")\n",
    "\n",
    "\n",
    "def setup_testing_app(bootstrap_servers=None):\n",
    "    app = create_testing_app(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "    @app.consumes(\"my_topic_1\")\n",
    "    def on_my_topic_one(msg: MyMsgUrl) -> None:\n",
    "        logger.debug(f\"on_my_topic_one(msg={msg},)\")\n",
    "\n",
    "    @app.consumes()\n",
    "    async def on_my_topic_2(msg: MyMsgEmail) -> None:\n",
    "        logger.debug(f\"on_my_topic_2(msg={msg},)\")\n",
    "\n",
    "    with pytest.raises(ValueError) as e:\n",
    "\n",
    "        @app.consumes()\n",
    "        def my_topic_3(msg: MyMsgEmail) -> None:\n",
    "            raise NotImplemented\n",
    "\n",
    "    @app.produces()\n",
    "    def to_my_topic_3(url: str) -> MyMsgUrl:\n",
    "        logger.debug(f\"on_my_topic_3(msg={url}\")\n",
    "        return MyMsgUrl(info=MyInfo(\"+3851987654321\", \"Sean Connery\"), url=url)\n",
    "\n",
    "    @app.produces()\n",
    "    async def to_my_topic_4(msg: MyMsgEmail) -> MyMsgEmail:\n",
    "        logger.debug(f\"on_my_topic_4(msg={msg}\")\n",
    "        return msg\n",
    "\n",
    "    @app.produces()\n",
    "    def to_my_topic_5(url: str) -> MyMsgUrl:\n",
    "        logger.debug(f\"on_my_topic_5(msg={url}\")\n",
    "        return MyMsgUrl(info=MyInfo(\"+3859123456789\", \"John Wayne\"), url=url)\n",
    "\n",
    "    @app.run_in_background()\n",
    "    async def long_bg_job():\n",
    "        logger.debug(f\"long_bg_job()\")\n",
    "        await asyncio.sleep(100)\n",
    "\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a945425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'\n",
      "[INFO] __main__: run_in_background() : Adding function 'long_bg_job' as background task\n",
      "app._kafka_service_info=title='' version='' description='' contact=ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')\n",
      "app._kafka_brokers=brokers={'localhost': KafkaBroker(url='localhost', description='Local (dev) Kafka broker', port='9092', protocol='kafka', security=None)}\n"
     ]
    }
   ],
   "source": [
    "app = setup_testing_app()\n",
    "\n",
    "assert set(app._consumers_store.keys()) == set([\"my_topic_1\", \"my_topic_2\"])\n",
    "assert set(app._producers_store.keys()) == set(\n",
    "    [\"my_topic_3\", \"my_topic_4\", \"my_topic_5\"]\n",
    ")\n",
    "\n",
    "print(f\"app._kafka_service_info={app._kafka_service_info}\")\n",
    "print(f\"app._kafka_brokers={app._kafka_brokers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ea338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _populate_consumers(\n",
    "    self: FastKafka,\n",
    "    is_shutting_down_f: Callable[[], bool],\n",
    ") -> None:\n",
    "    default_config: Dict[str, Any] = filter_using_signature(\n",
    "        AIOKafkaConsumer, **self._kafka_config\n",
    "    )\n",
    "    self._kafka_consumer_tasks = [\n",
    "        asyncio.create_task(\n",
    "            aiokafka_consumer_loop(\n",
    "                topic=topic,\n",
    "                decoder_fn=decoder_fn,\n",
    "                callback=consumer,\n",
    "                msg_type=signature(consumer).parameters[\"msg\"].annotation,\n",
    "                is_shutting_down_f=is_shutting_down_f,\n",
    "                **{**default_config, **override_config},\n",
    "            )\n",
    "        )\n",
    "        for topic, (\n",
    "            consumer,\n",
    "            decoder_fn,\n",
    "            override_config,\n",
    "        ) in self._consumers_store.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _shutdown_consumers(\n",
    "    self: FastKafka,\n",
    ") -> None:\n",
    "    if self._kafka_consumer_tasks:\n",
    "        await asyncio.wait(self._kafka_consumer_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'\n",
      "[INFO] __main__: run_in_background() : Adding function 'long_bg_job' as background task\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[WARNING] aiokafka.cluster: No broker metadata found in MetadataResponse\n",
      "[WARNING] aiokafka.cluster: No broker metadata found in MetadataResponse\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_1'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_1'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_2'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_2'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[WARNING] aiokafka.cluster: Topic my_topic_2 is not available during auto-create initialization\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_2': 0}. \n",
      "[WARNING] aiokafka.cluster: Topic my_topic_1 is not available during auto-create initialization\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_1': 0}. \n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 617586...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 617586 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 617213...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 617213 terminated.\n"
     ]
    }
   ],
   "source": [
    "async with ApacheKafkaBroker() as bootstrap_server:\n",
    "    app = setup_testing_app(bootstrap_servers=bootstrap_server)\n",
    "    app._populate_consumers(is_shutting_down_f=true_after(1))\n",
    "    assert len(app._kafka_consumer_tasks) == 2\n",
    "\n",
    "    await app._shutdown_consumers()\n",
    "\n",
    "    assert all([t.done() for t in app._kafka_consumer_tasks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ee2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# TODO: Add passing of vars\n",
    "async def _create_producer(  # type: ignore\n",
    "    *,\n",
    "    callback: ProduceCallable,\n",
    "    default_config: Dict[str, Any],\n",
    "    override_config: Dict[str, Any],\n",
    "    producers_list: List[Union[AIOKafkaProducer, AIOKafkaProducerManager]],\n",
    ") -> Union[AIOKafkaProducer, AIOKafkaProducerManager]:\n",
    "    \"\"\"Creates a producer\n",
    "\n",
    "    Args:\n",
    "        callback: A callback function that is called when the producer is ready.\n",
    "        producer: An existing producer to use.\n",
    "        default_config: A dictionary of default configuration values.\n",
    "        override_config: A dictionary of configuration values to override.\n",
    "        producers_list: A list of producers to add the new producer to.\n",
    "\n",
    "    Returns:\n",
    "        A producer.\n",
    "    \"\"\"\n",
    "\n",
    "    config = {\n",
    "        **filter_using_signature(AIOKafkaProducer, **default_config),\n",
    "        **override_config,\n",
    "    }\n",
    "    producer = AIOKafkaProducer(**config)\n",
    "    logger.info(\n",
    "        f\"_create_producer() : created producer using the config: '{sanitize_kafka_config(**config)}'\"\n",
    "    )\n",
    "\n",
    "    if not iscoroutinefunction(callback):\n",
    "        producer = AIOKafkaProducerManager(producer)\n",
    "\n",
    "    await producer.start()\n",
    "\n",
    "    producers_list.append(producer)\n",
    "\n",
    "    return producer\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _populate_producers(self: FastKafka) -> None:\n",
    "    \"\"\"Populates the producers for the FastKafka instance.\n",
    "\n",
    "    Args:\n",
    "        self: The FastKafka instance.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "\n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    default_config: Dict[str, Any] = self._kafka_config\n",
    "    self._producers_list = []\n",
    "    self._producers_store.update(\n",
    "        {\n",
    "            topic: (\n",
    "                callback,\n",
    "                await _create_producer(\n",
    "                    callback=callback,\n",
    "                    default_config=default_config,\n",
    "                    override_config=override_config,\n",
    "                    producers_list=self._producers_list,\n",
    "                ),\n",
    "                override_config,\n",
    "            )\n",
    "            for topic, (\n",
    "                callback,\n",
    "                _,\n",
    "                override_config,\n",
    "            ) in self._producers_store.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _shutdown_producers(self: FastKafka) -> None:\n",
    "    [await producer.stop() for producer in self._producers_list[::-1]]\n",
    "    # Remove references to stale producers\n",
    "    self._producers_list = []\n",
    "    self._producers_store.update(\n",
    "        {\n",
    "            topic: (\n",
    "                callback,\n",
    "                None,\n",
    "                override_config,\n",
    "            )\n",
    "            for topic, (\n",
    "                callback,\n",
    "                _,\n",
    "                override_config,\n",
    "            ) in self._producers_store.items()\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'\n",
      "[INFO] __main__: run_in_background() : Adding function 'long_bg_job' as background task\n",
      "{'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, None, {}), 'my_topic_4': (<function setup_testing_app.<locals>.to_my_topic_4>, None, {}), 'my_topic_5': (<function setup_testing_app.<locals>.to_my_topic_5>, None, {})}\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[WARNING] aiokafka.cluster: No broker metadata found in MetadataResponse\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "{'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, <fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager object>, {}), 'my_topic_4': (<function setup_testing_app.<locals>.to_my_topic_4>, <aiokafka.producer.producer.AIOKafkaProducer object>, {}), 'my_topic_5': (<function setup_testing_app.<locals>.to_my_topic_5>, <fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager object>, {})}\n",
      "[<fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager object>, <aiokafka.producer.producer.AIOKafkaProducer object>, <fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager object>]\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[<fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager object>, <aiokafka.producer.producer.AIOKafkaProducer object>, <fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager object>]\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 618436...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 618436 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 618064...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 618064 terminated.\n"
     ]
    }
   ],
   "source": [
    "async with ApacheKafkaBroker() as bootstrap_server:\n",
    "    app = setup_testing_app(bootstrap_servers=bootstrap_server)\n",
    "    print(app._producers_store)\n",
    "    await app._populate_producers()\n",
    "    print(app._producers_store)\n",
    "    assert len(app._producers_list) == 3\n",
    "    print(app._producers_list)\n",
    "    await app._shutdown_producers()\n",
    "\n",
    "    # One more time for reentrancy\n",
    "    await app._populate_producers()\n",
    "    assert len(app._producers_list) == 3\n",
    "    print(app._producers_list)\n",
    "    await app._shutdown_producers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _populate_bg_tasks(\n",
    "    self: FastKafka,\n",
    ") -> None:\n",
    "    def _start_bg_task(task: Callable[..., Coroutine[Any, Any, Any]]) -> asyncio.Task:\n",
    "        logger.info(\n",
    "            f\"_populate_bg_tasks() : Starting background task '{task.__name__}'\"\n",
    "        )\n",
    "        return asyncio.create_task(task(), name=task.__name__)\n",
    "\n",
    "    self._running_bg_tasks = [_start_bg_task(task) for task in self._scheduled_bg_tasks]\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _shutdown_bg_tasks(\n",
    "    self: FastKafka,\n",
    ") -> None:\n",
    "    for task in self._running_bg_tasks:\n",
    "        logger.info(\n",
    "            f\"_shutdown_bg_tasks() : Cancelling background task '{task.get_name()}'\"\n",
    "        )\n",
    "        task.cancel()\n",
    "\n",
    "    for task in self._running_bg_tasks:\n",
    "        logger.info(\n",
    "            f\"_shutdown_bg_tasks() : Waiting for background task '{task.get_name()}' to finish\"\n",
    "        )\n",
    "        try:\n",
    "            await task\n",
    "        except asyncio.CancelledError:\n",
    "            pass\n",
    "        logger.info(\n",
    "            f\"_shutdown_bg_tasks() : Execution finished for background task '{task.get_name()}'\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c687d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'\n",
      "[INFO] __main__: run_in_background() : Adding function 'long_bg_job' as background task\n",
      "[INFO] __main__: run_in_background() : Adding function 'long_bg_job' as background task\n",
      "[INFO] __main__: _populate_bg_tasks() : Starting background task 'long_bg_job'\n",
      "[INFO] __main__: _populate_bg_tasks() : Starting background task 'long_bg_job'\n",
      "[INFO] __main__: _shutdown_bg_tasks() : Cancelling background task 'long_bg_job'\n",
      "[INFO] __main__: _shutdown_bg_tasks() : Cancelling background task 'long_bg_job'\n",
      "[INFO] __main__: _shutdown_bg_tasks() : Waiting for background task 'long_bg_job' to finish\n",
      "[INFO] __main__: _shutdown_bg_tasks() : Execution finished for background task 'long_bg_job'\n",
      "[INFO] __main__: _shutdown_bg_tasks() : Waiting for background task 'long_bg_job' to finish\n",
      "[INFO] __main__: _shutdown_bg_tasks() : Execution finished for background task 'long_bg_job'\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 619288...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 619288 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 618915...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 618915 terminated.\n"
     ]
    }
   ],
   "source": [
    "async with ApacheKafkaBroker() as bootstrap_server:\n",
    "    app = setup_testing_app(bootstrap_servers=bootstrap_server)\n",
    "\n",
    "    @app.run_in_background()\n",
    "    async def long_bg_job():\n",
    "        logger.debug(f\"new_long_bg_job()\")\n",
    "        await asyncio.sleep(100)\n",
    "\n",
    "    await app._populate_bg_tasks()\n",
    "    assert len(app._scheduled_bg_tasks) == 2\n",
    "    assert len(app._running_bg_tasks) == 2\n",
    "    await app._shutdown_bg_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _start(self: FastKafka) -> None:\n",
    "    def is_shutting_down_f(self: FastKafka = self) -> bool:\n",
    "        return self._is_shutting_down\n",
    "\n",
    "    #     self.create_docs()\n",
    "    await self._populate_producers()\n",
    "    self._populate_consumers(is_shutting_down_f)\n",
    "    await self._populate_bg_tasks()\n",
    "\n",
    "    self._is_started = True\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _stop(self: FastKafka) -> None:\n",
    "    self._is_shutting_down = True\n",
    "\n",
    "    await self._shutdown_bg_tasks()\n",
    "    await self._shutdown_consumers()\n",
    "    await self._shutdown_producers()\n",
    "\n",
    "    self._is_shutting_down = False\n",
    "    self._is_started = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b199a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[WARNING] aiokafka.cluster: No broker metadata found in MetadataResponse\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 620135...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 620135 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 619762...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 619762 terminated.\n"
     ]
    }
   ],
   "source": [
    "# Test app reentrancy\n",
    "\n",
    "async with ApacheKafkaBroker() as bootstrap_server:\n",
    "    with mock_AIOKafkaProducer_send() as mock:\n",
    "        app = create_testing_app(bootstrap_servers=bootstrap_server)\n",
    "\n",
    "        @app.produces()\n",
    "        async def to_my_test_topic(mobile: str, url: str) -> MyMsgUrl:\n",
    "            msg = MyMsgUrl(info=dict(mobile=mobile, name=\"James Bond\"), url=url)\n",
    "            return msg\n",
    "\n",
    "        try:\n",
    "            await app._start()\n",
    "            await to_my_test_topic(mobile=\"+385912345678\", url=\"https://www.vip.hr\")\n",
    "        finally:\n",
    "            await app._stop()\n",
    "\n",
    "        try:\n",
    "            await app._start()\n",
    "            await to_my_test_topic(mobile=\"+385987654321\", url=\"https://www.ht.hr\")\n",
    "        finally:\n",
    "            await app._stop()\n",
    "\n",
    "        mock.assert_has_calls(\n",
    "            [\n",
    "                unittest.mock.call(\n",
    "                    \"my_test_topic\",\n",
    "                    b'{\"info\": {\"mobile\": \"+385912345678\", \"name\": \"James Bond\"}, \"url\": \"https://www.vip.hr\"}',\n",
    "                    key=None,\n",
    "                ),\n",
    "                unittest.mock.call(\n",
    "                    \"my_test_topic\",\n",
    "                    b'{\"info\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"}, \"url\": \"https://www.ht.hr\"}',\n",
    "                    key=None,\n",
    "                ),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9cfce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[WARNING] aiokafka.cluster: No broker metadata found in MetadataResponse\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 620983...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 620983 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 620610...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 620610 terminated.\n"
     ]
    }
   ],
   "source": [
    "# mock up send method of AIOKafkaProducer\n",
    "async with ApacheKafkaBroker() as bootstrap_server:\n",
    "    with mock_AIOKafkaProducer_send() as mock:\n",
    "        app = create_testing_app(bootstrap_servers=bootstrap_server)\n",
    "\n",
    "        @app.produces()\n",
    "        async def to_my_test_topic(mobile: str, url: str) -> MyMsgUrl:\n",
    "            msg = MyMsgUrl(info=dict(mobile=mobile, name=\"James Bond\"), url=url)\n",
    "            return msg\n",
    "\n",
    "        @app.produces()\n",
    "        def to_my_test_topic_2(mobile: str, url: str) -> MyMsgUrl:\n",
    "            msg = MyMsgUrl(info=dict(mobile=mobile, name=\"James Bond\"), url=url)\n",
    "            return msg\n",
    "\n",
    "        try:\n",
    "            await app._start()\n",
    "            await to_my_test_topic(mobile=\"+385912345678\", url=\"https://www.vip.hr\")\n",
    "            to_my_test_topic_2(mobile=\"+385987654321\", url=\"https://www.ht.hr\")\n",
    "        finally:\n",
    "            await app._stop()\n",
    "\n",
    "        mock.assert_has_calls(\n",
    "            [\n",
    "                unittest.mock.call(\n",
    "                    \"my_test_topic\",\n",
    "                    b'{\"info\": {\"mobile\": \"+385912345678\", \"name\": \"James Bond\"}, \"url\": \"https://www.vip.hr\"}',\n",
    "                    key=None,\n",
    "                ),\n",
    "                unittest.mock.call(\n",
    "                    \"my_test_topic_2\",\n",
    "                    b'{\"info\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"}, \"url\": \"https://www.ht.hr\"}',\n",
    "                    key=None,\n",
    "                ),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b114fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'\n",
      "[INFO] __main__: run_in_background() : Adding function 'bg_task' as background task\n",
      "[INFO] __main__: run_in_background() : Adding function 'bg_task_second' as background task\n",
      "[INFO] __main__: _populate_bg_tasks() : Starting background task 'bg_task'\n",
      "[INFO] __main__: _populate_bg_tasks() : Starting background task 'bg_task_second'\n",
      "[INFO] __main__: _shutdown_bg_tasks() : Cancelling background task 'bg_task'\n",
      "[INFO] __main__: _shutdown_bg_tasks() : Cancelling background task 'bg_task_second'\n",
      "[INFO] __main__: _shutdown_bg_tasks() : Waiting for background task 'bg_task' to finish\n",
      "[INFO] __main__: _shutdown_bg_tasks() : Execution finished for background task 'bg_task'\n",
      "[INFO] __main__: _shutdown_bg_tasks() : Waiting for background task 'bg_task_second' to finish\n",
      "[INFO] __main__: _shutdown_bg_tasks() : Execution finished for background task 'bg_task_second'\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 621832...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 621832 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 621460...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 621460 terminated.\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "async with ApacheKafkaBroker() as bootstrap_server:\n",
    "    app = create_testing_app(bootstrap_servers=bootstrap_server)\n",
    "    fast_task = unittest.mock.Mock()\n",
    "    long_task = unittest.mock.Mock()\n",
    "\n",
    "    @app.run_in_background()\n",
    "    async def bg_task():\n",
    "        fast_task()\n",
    "        await asyncio.sleep(100)\n",
    "        long_task()\n",
    "\n",
    "    fast_task_second = unittest.mock.Mock()\n",
    "    long_task_second = unittest.mock.Mock()\n",
    "\n",
    "    @app.run_in_background()\n",
    "    async def bg_task_second():\n",
    "        fast_task_second()\n",
    "        await asyncio.sleep(100)\n",
    "        long_task_second()\n",
    "\n",
    "    try:\n",
    "        await app._start()\n",
    "        await asyncio.sleep(5)\n",
    "    finally:\n",
    "        await app._stop()\n",
    "\n",
    "    fast_task.assert_called()\n",
    "    long_task.assert_not_called()\n",
    "\n",
    "    fast_task_second.assert_called()\n",
    "    long_task_second.assert_not_called()\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4bb0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): entering...\n",
      "[WARNING] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): (<_UnixSelectorEventLoop running=True closed=False debug=False>) is already running!\n",
      "[WARNING] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): calling nest_asyncio.apply()\n",
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: <class 'fastkafka.testing.ApacheKafkaBroker'>.start(): returning 127.0.0.1:9092\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): exited.\n",
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.stop(): entering...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 622680...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 622680 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 622307...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 622307 terminated.\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.stop(): exited.\n"
     ]
    }
   ],
   "source": [
    "# test lifespan hook\n",
    "\n",
    "global_dict = {}\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastKafka):\n",
    "    try:\n",
    "        global_dict[\"set_var\"] = 123\n",
    "        global_dict[\"app\"] = app\n",
    "        yield\n",
    "    finally:\n",
    "        global_dict[\"set_var\"] = 321\n",
    "\n",
    "\n",
    "with ApacheKafkaBroker(apply_nest_asyncio=True) as bootstrap_servers:\n",
    "    host, port = bootstrap_servers.split(\":\")\n",
    "\n",
    "    kafka_app = FastKafka(\n",
    "        kafka_brokers={\n",
    "            \"localhost\": {\n",
    "                \"url\": host if host is not None else \"localhost\",\n",
    "                \"name\": \"development\",\n",
    "                \"description\": \"Local (dev) Kafka broker\",\n",
    "                \"port\": port if port is not None else 9092,\n",
    "            }\n",
    "        },\n",
    "        root_path=\"/tmp/000_FastKafka\",\n",
    "        lifespan=lifespan,\n",
    "    )\n",
    "\n",
    "    kafka_app.set_kafka_broker(kafka_broker_name=\"localhost\")\n",
    "\n",
    "    # Dict unchanged\n",
    "    assert global_dict == {}\n",
    "\n",
    "    async with kafka_app:\n",
    "        # Lifespan aenter triggered\n",
    "        assert global_dict[\"set_var\"] == 123\n",
    "        # Kafka app reference passed\n",
    "        assert global_dict[\"app\"] == kafka_app\n",
    "\n",
    "    # Lifespan aexit triggered\n",
    "    assert global_dict[\"set_var\"] == 321"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe047872",
   "metadata": {},
   "source": [
    "## Documentation generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880411a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def create_docs(self: FastKafka) -> None:\n",
    "    export_async_spec(\n",
    "        consumers={\n",
    "            topic: callback for topic, (callback, _, _) in self._consumers_store.items()\n",
    "        },\n",
    "        producers={\n",
    "            topic: callback for topic, (callback, _, _) in self._producers_store.items()\n",
    "        },\n",
    "        kafka_brokers=self._kafka_brokers,\n",
    "        kafka_service_info=self._kafka_service_info,\n",
    "        asyncapi_path=self._asyncapi_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec33cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = \"\"\"asyncapi: 2.5.0\n",
    "channels:\n",
    "  my_topic_1:\n",
    "    subscribe:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgUrl'\n",
    "  my_topic_2:\n",
    "    subscribe:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgEmail'\n",
    "  my_topic_3:\n",
    "    publish:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgUrl'\n",
    "  my_topic_4:\n",
    "    publish:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgEmail'\n",
    "  my_topic_5:\n",
    "    publish:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgUrl'\n",
    "components:\n",
    "  messages:\n",
    "    MyMsgEmail:\n",
    "      payload:\n",
    "        example:\n",
    "          email: agent-007@sis.gov.uk\n",
    "          msg_url:\n",
    "            info:\n",
    "              mobile: '+385987654321'\n",
    "              name: James Bond\n",
    "            url: https://sis.gov.uk/agents/007\n",
    "        properties:\n",
    "          email:\n",
    "            example: agent-007@sis.gov.uk\n",
    "            format: email\n",
    "            title: Email\n",
    "            type: string\n",
    "          msg_url:\n",
    "            allOf:\n",
    "            - $ref: '#/components/messages/MyMsgUrl'\n",
    "            example:\n",
    "              info:\n",
    "                mobile: '+385987654321'\n",
    "                name: James Bond\n",
    "              url: https://sis.gov.uk/agents/007\n",
    "            title: Msg Url\n",
    "        required:\n",
    "        - msg_url\n",
    "        - email\n",
    "        title: MyMsgEmail\n",
    "        type: object\n",
    "    MyMsgUrl:\n",
    "      payload:\n",
    "        example:\n",
    "          info:\n",
    "            mobile: '+385987654321'\n",
    "            name: James Bond\n",
    "          url: https://sis.gov.uk/agents/007\n",
    "        properties:\n",
    "          info:\n",
    "            allOf:\n",
    "            - $ref: '#/components/schemas/MyInfo'\n",
    "            example:\n",
    "              mobile: '+385987654321'\n",
    "              name: James Bond\n",
    "            title: Info\n",
    "          url:\n",
    "            example: https://sis.gov.uk/agents/007\n",
    "            format: uri\n",
    "            maxLength: 2083\n",
    "            minLength: 1\n",
    "            title: Url\n",
    "            type: string\n",
    "        required:\n",
    "        - info\n",
    "        - url\n",
    "        title: MyMsgUrl\n",
    "        type: object\n",
    "  schemas:\n",
    "    MyInfo:\n",
    "      payload:\n",
    "        properties:\n",
    "          mobile:\n",
    "            example: '+385987654321'\n",
    "            title: Mobile\n",
    "            type: string\n",
    "          name:\n",
    "            example: James Bond\n",
    "            title: Name\n",
    "            type: string\n",
    "        required:\n",
    "        - mobile\n",
    "        - name\n",
    "        title: MyInfo\n",
    "        type: object\n",
    "  securitySchemes: {}\n",
    "info:\n",
    "  contact:\n",
    "    email: noreply@gmail.com\n",
    "    name: Author\n",
    "    url: https://www.google.com\n",
    "  description: ''\n",
    "  title: ''\n",
    "  version: ''\n",
    "servers:\n",
    "  localhost:\n",
    "    description: Local (dev) Kafka broker\n",
    "    protocol: kafka\n",
    "    url: localhost\n",
    "    variables:\n",
    "      port:\n",
    "        default: '9092'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'\n",
      "[INFO] __main__: run_in_background() : Adding function 'long_bg_job' as background task\n",
      "[INFO] fastkafka._components.asyncapi: Old async specifications at '/tmp/000_FastKafka/asyncapi/spec/asyncapi.yml' does not exist.\n",
      "[INFO] fastkafka._components.asyncapi: New async specifications generated at: '/tmp/000_FastKafka/asyncapi/spec/asyncapi.yml'\n",
      "[INFO] fastkafka._components.asyncapi: Async docs generated at '/tmp/000_FastKafka/asyncapi/docs'\n",
      "[INFO] fastkafka._components.asyncapi: Output of '$ npx -y -p @asyncapi/generator ag /tmp/000_FastKafka/asyncapi/spec/asyncapi.yml @asyncapi/html-template -o /tmp/000_FastKafka/asyncapi/docs --force-write'\u001b[32m\n",
      "\n",
      "Done! ✨\u001b[0m\n",
      "\u001b[33mCheck out your shiny new generated files at \u001b[0m\u001b[35m/tmp/000_FastKafka/asyncapi/docs\u001b[0m\u001b[33m.\u001b[0m\n",
      "\n",
      "\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "d1, d2 = None, None\n",
    "\n",
    "docs_path = Path(\"/tmp/000_FastKafka/asyncapi/spec/asyncapi.yml\")\n",
    "if docs_path.exists():\n",
    "    os.remove(docs_path)\n",
    "\n",
    "\n",
    "async def test_me():\n",
    "    global d1\n",
    "    global d2\n",
    "    app = setup_testing_app()\n",
    "    app.create_docs()\n",
    "    with open(docs_path) as specs:\n",
    "        d1 = yaml.safe_load(specs)\n",
    "        d2 = yaml.safe_load(expected)\n",
    "        assert d1 == d2, f\"{d1} != {d2}\"\n",
    "\n",
    "\n",
    "asyncio.run(test_me())\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafbc67",
   "metadata": {},
   "source": [
    "## App mocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class AwaitedMock:\n",
    "    @staticmethod\n",
    "    def _await_for(f: Callable[..., Any]) -> Callable[..., Any]:\n",
    "        @delegates(f)\n",
    "        async def inner(\n",
    "            *args: Any, f: Callable[..., Any] = f, timeout: int = 60, **kwargs: Any\n",
    "        ) -> Any:\n",
    "            if inspect.iscoroutinefunction(f):\n",
    "                return await asyncio.wait_for(f(*args, **kwargs), timeout=timeout)\n",
    "            else:\n",
    "                t0 = datetime.now()\n",
    "                e: Optional[Exception] = None\n",
    "                while True:\n",
    "                    try:\n",
    "                        return f(*args, **kwargs)\n",
    "                    except Exception as _e:\n",
    "                        await asyncio.sleep(1)\n",
    "                        e = _e\n",
    "\n",
    "                    if datetime.now() - t0 > timedelta(seconds=timeout):\n",
    "                        break\n",
    "\n",
    "                raise e\n",
    "\n",
    "        return inner\n",
    "\n",
    "    def __init__(self, o: Any):\n",
    "        self._o = o\n",
    "\n",
    "        for name in o.__dir__():\n",
    "            if not name.startswith(\"_\"):\n",
    "                f = getattr(o, name)\n",
    "                if inspect.ismethod(f):\n",
    "                    setattr(self, name, self._await_for(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def create_mocks(self: FastKafka) -> None:\n",
    "    \"\"\"Creates self.mocks as a named tuple mapping a new function obtained by calling the original functions and a mock\"\"\"\n",
    "    app_methods = [f for f, _, _ in self._consumers_store.values()] + [\n",
    "        f for f, _, _ in self._producers_store.values()\n",
    "    ]\n",
    "    self.AppMocks = namedtuple(  # type: ignore\n",
    "        f\"{self.__class__.__name__}Mocks\", [f.__name__ for f in app_methods]\n",
    "    )\n",
    "\n",
    "    self.mocks = self.AppMocks(  # type: ignore\n",
    "        **{\n",
    "            f.__name__: AsyncMock() if inspect.iscoroutinefunction(f) else MagicMock()\n",
    "            for f in app_methods\n",
    "        }\n",
    "    )\n",
    "\n",
    "    self.awaited_mocks = self.AppMocks(  # type: ignore\n",
    "        **{name: AwaitedMock(mock) for name, mock in self.mocks._asdict().items()}\n",
    "    )\n",
    "\n",
    "    def add_mock(\n",
    "        f: Callable[..., Any], mock: Union[AsyncMock, MagicMock]\n",
    "    ) -> Callable[..., Any]:\n",
    "        \"\"\"Add call to mock when calling function f\"\"\"\n",
    "\n",
    "        @functools.wraps(f)\n",
    "        async def async_inner(\n",
    "            *args: Any, f: Callable[..., Any] = f, mock: AsyncMock = mock, **kwargs: Any\n",
    "        ) -> Any:\n",
    "            await mock(*args, **kwargs)\n",
    "            return await f(*args, **kwargs)\n",
    "\n",
    "        @functools.wraps(f)\n",
    "        def sync_inner(\n",
    "            *args: Any, f: Callable[..., Any] = f, mock: MagicMock = mock, **kwargs: Any\n",
    "        ) -> Any:\n",
    "            mock(*args, **kwargs)\n",
    "            return f(*args, **kwargs)\n",
    "\n",
    "        if inspect.iscoroutinefunction(f):\n",
    "            return async_inner\n",
    "        else:\n",
    "            return sync_inner\n",
    "\n",
    "    self._consumers_store.update(\n",
    "        {\n",
    "            name: (\n",
    "                add_mock(f, getattr(self.mocks, f.__name__)),\n",
    "                decoder_fn,\n",
    "                kwargs,\n",
    "            )\n",
    "            for name, (f, decoder_fn, kwargs) in self._consumers_store.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "    self._producers_store.update(\n",
    "        {\n",
    "            name: (\n",
    "                add_mock(f, getattr(self.mocks, f.__name__)),\n",
    "                producer,\n",
    "                kwargs,\n",
    "            )\n",
    "            for name, (f, producer, kwargs) in self._producers_store.items()\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMsg(BaseModel):\n",
    "    msg: str = Field(...)\n",
    "\n",
    "\n",
    "app = FastKafka(kafka_brokers=dict(localhost=dict(url=\"localhost\", port=9092)))\n",
    "\n",
    "\n",
    "@app.consumes()\n",
    "async def on_preprocessed_signals(msg: TestMsg):\n",
    "    await to_predictions(TestMsg(msg=\"prediction\"))\n",
    "\n",
    "\n",
    "@app.produces()\n",
    "async def to_predictions(prediction: TestMsg) -> TestMsg:\n",
    "    print(f\"Sending prediction: {prediction}\")\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca781c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.create_mocks()\n",
    "app.mocks.on_preprocessed_signals.assert_not_awaited()\n",
    "app.mocks.to_predictions.assert_not_awaited()\n",
    "app.create_mocks()\n",
    "app.mocks.on_preprocessed_signals.assert_not_awaited()\n",
    "app.mocks.to_predictions.assert_not_awaited()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b07708",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(AssertionError) as e:\n",
    "    await app.awaited_mocks.on_preprocessed_signals.assert_called_with(123, timeout=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.create_mocks()\n",
    "app.mocks.on_preprocessed_signals.assert_not_awaited()\n",
    "await app.awaited_mocks.on_preprocessed_signals.assert_not_awaited(timeout=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16440cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdaa582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def benchmark(\n",
    "    self: FastKafka,\n",
    "    interval: Union[int, timedelta] = 1,\n",
    "    *,\n",
    "    sliding_window_size: Optional[int] = None,\n",
    ") -> Callable[[Callable[[I], Optional[O]]], Callable[[I], Optional[O]]]:\n",
    "    \"\"\"Decorator to benchmark produces/consumes functions\n",
    "\n",
    "    Args:\n",
    "        interval: Period to use to calculate throughput. If value is of type int,\n",
    "            then it will be used as seconds. If value is of type timedelta,\n",
    "            then it will be used as it is. default: 1 - one second\n",
    "        sliding_window_size: The size of the sliding window to use to calculate\n",
    "            average throughput. default: None - By default average throughput is\n",
    "            not calculated\n",
    "    \"\"\"\n",
    "\n",
    "    def _decorator(func: Callable[[I], Optional[O]]) -> Callable[[I], Optional[O]]:\n",
    "        func_name = f\"{func.__module__}.{func.__qualname__}\"\n",
    "\n",
    "        @wraps(func)\n",
    "        def wrapper(\n",
    "            *args: I,\n",
    "            **kwargs: I,\n",
    "        ) -> Optional[O]:\n",
    "            _benchmark(\n",
    "                interval=interval,\n",
    "                sliding_window_size=sliding_window_size,\n",
    "                func_name=func_name,\n",
    "                benchmark_results=self.benchmark_results,\n",
    "            )\n",
    "            return func(*args, **kwargs)\n",
    "\n",
    "        @wraps(func)\n",
    "        async def async_wrapper(\n",
    "            *args: I,\n",
    "            **kwargs: I,\n",
    "        ) -> Optional[O]:\n",
    "            _benchmark(\n",
    "                interval=interval,\n",
    "                sliding_window_size=sliding_window_size,\n",
    "                func_name=func_name,\n",
    "                benchmark_results=self.benchmark_results,\n",
    "            )\n",
    "            return await func(*args, **kwargs)  # type: ignore\n",
    "\n",
    "        if inspect.iscoroutinefunction(func):\n",
    "            return async_wrapper  # type: ignore\n",
    "        else:\n",
    "            return wrapper\n",
    "\n",
    "    return _decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4fcda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'preprocessed_signals'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'preprocessed_signals'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'preprocessed_signals': 1}. \n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'predictions'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'predictions'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'predictions': 1}. \n",
      "[INFO] fastkafka.benchmark: Throughput =   972, Avg throughput =   972 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput =   971, Avg throughput =   971 - For __main__.to_predictions(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput = 1,260, Avg throughput = 1,116 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput = 1,260, Avg throughput = 1,116 - For __main__.to_predictions(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput = 1,223, Avg throughput = 1,152 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput = 1,222, Avg throughput = 1,151 - For __main__.to_predictions(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput =   899, Avg throughput = 1,088 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput =   899, Avg throughput = 1,088 - For __main__.to_predictions(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput = 1,062, Avg throughput = 1,083 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput = 1,062, Avg throughput = 1,083 - For __main__.to_predictions(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput =   924, Avg throughput = 1,074 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput =   924, Avg throughput = 1,074 - For __main__.to_predictions(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput = 1,026, Avg throughput = 1,027 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput = 1,026, Avg throughput = 1,027 - For __main__.to_predictions(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput = 1,391, Avg throughput = 1,061 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)\n",
      "[INFO] fastkafka.benchmark: Throughput = 1,391, Avg throughput = 1,060 - For __main__.to_predictions(interval=1,sliding_window_size=5)\n",
      "Hello I am over after 100k msgs\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 623563...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 623563 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 623190...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 623190 terminated.\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "class TestMsg(BaseModel):\n",
    "    msg: str = Field(...)\n",
    "\n",
    "\n",
    "app = FastKafka(kafka_brokers=dict(localhost=dict(url=\"localhost\", port=9092)))\n",
    "# app.benchmark_results[\"test\"] = dict(count=0)\n",
    "\n",
    "\n",
    "@app.consumes()\n",
    "@app.benchmark(interval=1, sliding_window_size=5)\n",
    "async def on_preprocessed_signals(msg: TestMsg):\n",
    "    await to_predictions(TestMsg(msg=\"prediction\"))\n",
    "\n",
    "\n",
    "@app.produces()\n",
    "@app.benchmark(interval=1, sliding_window_size=5)\n",
    "async def to_predictions(prediction: TestMsg) -> TestMsg:\n",
    "    #     print(f\"Sending prediction: {prediction}\")\n",
    "    return prediction\n",
    "\n",
    "\n",
    "async with Tester(app).using_local_kafka() as tester:\n",
    "    for i in range(10_000):\n",
    "        await tester.to_preprocessed_signals(TestMsg(msg=f\"signal {i}\"))\n",
    "    print(\"Hello I am over after 100k msgs\")\n",
    "    #     await asyncio.sleep(5)\n",
    "    tester.mocks.on_predictions.assert_called()\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e7fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
