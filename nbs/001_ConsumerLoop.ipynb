{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _components.aiokafka_consumer_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import asyncio\n",
    "from asyncio import iscoroutinefunction  # do not use the version from inspect\n",
    "from datetime import datetime, timedelta\n",
    "from os import environ\n",
    "from typing import *\n",
    "\n",
    "import anyio\n",
    "from anyio.streams.memory import MemoryObjectReceiveStream\n",
    "import asyncer\n",
    "from aiokafka import AIOKafkaConsumer\n",
    "from aiokafka.structs import ConsumerRecord, TopicPartition\n",
    "from pydantic import BaseModel, Field, HttpUrl, NonNegativeInt\n",
    "\n",
    "from fast_kafka_api._components.logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a446cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import AsyncMock, MagicMock, Mock, call, patch\n",
    "\n",
    "from fast_kafka_api._components.logger import supress_timestamps\n",
    "from fast_kafka_api.testing import (\n",
    "    create_and_fill_testing_topic,\n",
    "    nb_safe_seed,\n",
    "    true_after,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = nb_safe_seed(\"_components.aiokafka_consumer_loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e9e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "# allows async calls in notebooks\n",
    "\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53542175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92feb585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "supress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a41c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "kafka_server_url = (\n",
    "    environ[\"KAFKA_HOSTNAME\"] if \"KAFKA_HOSTNAME\" in environ else \"localhost\"\n",
    ")\n",
    "\n",
    "kafka_server_port = environ[\"KAFKA_PORT\"] if \"KAFKA_PORT\" in environ else \"9092\"\n",
    "\n",
    "aiokafka_config = {\n",
    "    \"bootstrap_servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMessage(BaseModel):\n",
    "    url: HttpUrl = Field(..., example=\"http://www.acme.com\", description=\"Url example\")\n",
    "    port: NonNegativeInt = Field(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consumer_record(topic: str, partition: int, msg: BaseModel):\n",
    "    record = ConsumerRecord(\n",
    "        topic=topic,\n",
    "        partition=partition,\n",
    "        offset=0,\n",
    "        timestamp=0,\n",
    "        timestamp_type=0,\n",
    "        key=None,\n",
    "        value=msg.json().encode(\"utf-8\"),\n",
    "        checksum=0,\n",
    "        serialized_key_size=0,\n",
    "        serialized_value_size=0,\n",
    "        headers=[],\n",
    "    )\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf847b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _create_safe_callback(\n",
    "    callback: Callable[[BaseModel], Awaitable[None]]\n",
    ") -> Callable[[BaseModel], Awaitable[None]]:\n",
    "    \"\"\"\n",
    "    Wraps an async callback into a safe callback that catches any Exception and loggs them as warnings\n",
    "\n",
    "    Params:\n",
    "        callback: async callable that will be wrapped into a safe callback\n",
    "\n",
    "    Returns:\n",
    "        Wrapped callback into a safe callback that handles exceptions\n",
    "    \"\"\"\n",
    "    async def safe_callback(\n",
    "        msg: BaseModel,\n",
    "        callback: Callable[[BaseModel], Awaitable[None]] = callback,\n",
    "    ) -> None:\n",
    "        try:\n",
    "            #                         logger.debug(f\"process_msgs(): awaiting '{callback}({msg})'\")\n",
    "            await callback(msg)\n",
    "        except Exception as e:\n",
    "            logger.warning(\n",
    "                f\"safe_callback(): exception caugth {e.__repr__()} while awaiting '{callback}({msg})'\"\n",
    "            )\n",
    "    return safe_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b218a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if callback is called when wrapped\n",
    "\n",
    "example_msg = \"Example msg\"\n",
    "callback = AsyncMock()\n",
    "safe_callback = _create_safe_callback(callback)\n",
    "\n",
    "await safe_callback(f\"{example_msg}\")\n",
    "\n",
    "callback.assert_awaited_once_with(f\"{example_msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if exception is caught and logged when callback is called and throws an exception\n",
    "\n",
    "with patch.object(logger, \"warning\") as mock:\n",
    "    example_msg = \"Example msg\"\n",
    "    exception = Exception(\"\")\n",
    "\n",
    "    callback = AsyncMock()\n",
    "    callback.side_effect = exception\n",
    "    safe_callback = _create_safe_callback(callback)\n",
    "\n",
    "    await safe_callback(f\"{example_msg}\")\n",
    "    \n",
    "    callback.assert_awaited_once_with(f\"{example_msg}\")\n",
    "    mock.assert_called_once_with(\n",
    "        f\"safe_callback(): exception caugth {exception.__repr__()} while awaiting '{callback}({example_msg})'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd1a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _prepare_callback(\n",
    "    callback: Union[Callable[[BaseModel], None], Callable[[BaseModel], Awaitable[None]]]\n",
    ") -> Callable[[BaseModel], Awaitable[None]]:\n",
    "    \"\"\"\n",
    "    Prepares a callback to be used in the consumer loop. \n",
    "        1. If callback is sync, asyncify it\n",
    "        2. Wrap the callback into a safe callback for exception handling\n",
    "\n",
    "    Params:\n",
    "        callback: async callable that will be prepared for use in consumer\n",
    "\n",
    "    Returns:\n",
    "        Prepared callback\n",
    "    \"\"\"\n",
    "    callback: Callable[[BaseModel], Awaitable[None]] = (\n",
    "        callback if iscoroutinefunction(callback) else asyncer.asyncify(callback)\n",
    "    )\n",
    "    return _create_safe_callback(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e996f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if callback is called when wrapped\n",
    "\n",
    "for is_async in [False, True]:\n",
    "    example_msg = \"Example msg\"\n",
    "    callback = AsyncMock() if is_async else Mock()\n",
    "    prepared_callback = _prepare_callback(callback)\n",
    "\n",
    "    await prepared_callback(f\"{example_msg}\")\n",
    "\n",
    "    callback.assert_called_once_with(f\"{example_msg}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d08f515",
   "metadata": {},
   "source": [
    "# Check if exception is caught and logged when callback is called and throws an exception\n",
    "\n",
    "for is_async in [False, True]:\n",
    "    with patch.object(logger, \"warning\") as mock:\n",
    "        example_msg = \"Example msg\"\n",
    "        exception = Exception(\"\")\n",
    "\n",
    "        callback = AsyncMock() if is_async else Mock()\n",
    "        callback.side_effect = exception\n",
    "        prepared_callback = _prepare_callback(callback)\n",
    "\n",
    "        await prepared_callback(f\"{example_msg}\")\n",
    "\n",
    "        callback.assert_called_once_with(f\"{example_msg}\")\n",
    "        mock.assert_called_once_with(\n",
    "            f\"process_msgs(): exception caugth {exception.__repr__()} while awaiting '{callback}({example_msg})'\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0977bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "async def _decode_msgs_and_stream(msgs: Dict[TopicPartition, bytes], msg_types: Dict[str, BaseModel], send_stream: anyio.streams.memory.MemoryObjectSendStream[Any]) -> None:\n",
    "    \"\"\"\n",
    "    Prepares a callback to be used in the consumer loop. \n",
    "        1. If callback is sync, asyncify it\n",
    "        2. Wrap the callback into a safe callback for exception handling\n",
    "\n",
    "    Params:\n",
    "        callback: async callable that will be prepared for use in consumer\n",
    "\n",
    "    Returns:\n",
    "        Prepared callback\n",
    "    \"\"\"\n",
    "    for topic_partition, topic_msgs in msgs.items():\n",
    "        topic = topic_partition.topic\n",
    "        msg_type = msg_types[topic]\n",
    "        try:\n",
    "            decoded_msgs = [\n",
    "                msg_type.parse_raw(msg.value.decode(\"utf-8\")) for msg in topic_msgs\n",
    "            ]\n",
    "            for msg in decoded_msgs:\n",
    "                await send_stream.send((topic, msg))\n",
    "        except Exception as e:\n",
    "            logger.warning(\n",
    "                f\"process_msgs(): Unexpected exception '{e.__repr__()}' caught and ignored for topic='{topic_partition.topic}', partition='{topic_partition.partition}' and messages: {topic_msgs}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335aa93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: one msg, one topic\n",
    "\n",
    "with patch(\"anyio.streams.memory.MemoryObjectSendStream.send\") as mock:\n",
    "    send_stream, receive_stream = anyio.create_memory_object_stream()\n",
    "\n",
    "    topic = \"topic_0\"\n",
    "    partition = 0\n",
    "    topic_part_0_0 = TopicPartition(topic, partition)\n",
    "    msg = MyMessage(url=\"http://www.acme.com\", port=22)\n",
    "    record = create_consumer_record(topic=topic, partition=partition, msg=msg)\n",
    "\n",
    "    await _decode_msgs_and_stream(\n",
    "        msgs={topic_part_0_0: [record]},\n",
    "        msg_types={topic: MyMessage},\n",
    "        send_stream=send_stream,\n",
    "    )\n",
    "\n",
    "    mock.assert_called_with((topic, msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ecc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check different topics\n",
    "\n",
    "# Two msg, two topics, send called twice with each topic\n",
    "\n",
    "with patch(\"anyio.streams.memory.MemoryObjectSendStream.send\") as mock:\n",
    "    send_stream, receive_stream = anyio.create_memory_object_stream()\n",
    "\n",
    "    topic_partitions = [(\"topic_0\", 0), (\"topic_1\", 0)]\n",
    "\n",
    "    msg = MyMessage(url=\"http://www.acme.com\", port=22)\n",
    "    record = create_consumer_record(topic=topic, partition=partition, msg=msg)\n",
    "\n",
    "    await _decode_msgs_and_stream(\n",
    "        msgs={\n",
    "            TopicPartition(topic, partition): [\n",
    "                create_consumer_record(topic=topic, partition=partition, msg=msg)\n",
    "            ]\n",
    "            for topic, partition in topic_partitions\n",
    "        },\n",
    "        msg_types={topic: MyMessage for topic, _ in topic_partitions},\n",
    "        send_stream=send_stream,\n",
    "    )\n",
    "\n",
    "    mock.assert_has_calls(\n",
    "        [\n",
    "            call((\"topic_0\", msg)),\n",
    "            call((\"topic_1\", msg)),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3fa870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check multiple msgs in same topic\n",
    "\n",
    "# Two msg, one topic, send called twice for same topic\n",
    "\n",
    "with patch(\"anyio.streams.memory.MemoryObjectSendStream.send\") as mock:\n",
    "    send_stream, receive_stream = anyio.create_memory_object_stream()\n",
    "\n",
    "    topic_partitions = [(\"topic_0\", 0)]\n",
    "\n",
    "    msg = MyMessage(url=\"http://www.acme.com\", port=22)\n",
    "    record = create_consumer_record(topic=topic, partition=partition, msg=msg)\n",
    "\n",
    "    await _decode_msgs_and_stream(\n",
    "        msgs={\n",
    "            TopicPartition(topic, partition): [\n",
    "                create_consumer_record(topic=topic, partition=partition, msg=msg),\n",
    "                create_consumer_record(topic=topic, partition=partition, msg=msg),\n",
    "            ]\n",
    "            for topic, partition in topic_partitions\n",
    "        },\n",
    "        msg_types={topic: MyMessage for topic, _ in topic_partitions},\n",
    "        send_stream=send_stream,\n",
    "    )\n",
    "\n",
    "    mock.assert_has_calls(\n",
    "        [\n",
    "            call((\"topic_0\", msg)),\n",
    "            call((\"topic_0\", msg)),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403988a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check multiple partitions\n",
    "\n",
    "# Two msg, one topic, differenct partitions, send called twice for same topic\n",
    "\n",
    "with patch(\"anyio.streams.memory.MemoryObjectSendStream.send\") as mock:\n",
    "    send_stream, receive_stream = anyio.create_memory_object_stream()\n",
    "\n",
    "    topic_partitions = [(\"topic_0\", 0), (\"topic_0\", 1)]\n",
    "\n",
    "    msg = MyMessage(url=\"http://www.acme.com\", port=22)\n",
    "    record = create_consumer_record(topic=topic, partition=partition, msg=msg)\n",
    "\n",
    "    await _decode_msgs_and_stream(\n",
    "        msgs={\n",
    "            TopicPartition(topic, partition): [\n",
    "                create_consumer_record(topic=topic, partition=partition, msg=msg)\n",
    "                ]\n",
    "            for topic, partition in topic_partitions\n",
    "        },\n",
    "        msg_types={topic: MyMessage for topic, _ in topic_partitions},\n",
    "        send_stream=send_stream,\n",
    "    )\n",
    "\n",
    "    mock.assert_has_calls(\n",
    "        [\n",
    "            call((\"topic_0\", msg)),\n",
    "            call((\"topic_0\", msg)),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "async def _aiokafka_consumer_loop(  # type: ignore\n",
    "    consumer: AIOKafkaConsumer,\n",
    "    *,\n",
    "    callbacks: Dict[str, Callable[[BaseModel], Union[None, Awaitable[None]]]],\n",
    "    timeout_ms: int = 100,\n",
    "    max_buffer_size: int = 10_000,\n",
    "    msg_types: Dict[str, Type[BaseModel]],\n",
    "    is_shutting_down_f: Callable[[], bool],\n",
    ") -> None:\n",
    "    \"\"\"Write docs\n",
    "\n",
    "    Todo: add batch size if needed\n",
    "    \"\"\"\n",
    "\n",
    "    prepared_callbacks = {\n",
    "        topic: _prepare_callback(callback) for topic, callback in callbacks.items()\n",
    "    }\n",
    "\n",
    "    async def process_message_callback(\n",
    "        receive_stream: MemoryObjectReceiveStream[Any], callback=prepared_callbacks\n",
    "    ) -> None:\n",
    "        async with receive_stream:\n",
    "            async for topic, msg in receive_stream:\n",
    "                if topic in callbacks:\n",
    "                    await callbacks[topic](msg)\n",
    "                else:\n",
    "                    logger.error(f\"No callback defined for topic {topic}\")\n",
    "\n",
    "    send_stream, receive_stream = anyio.create_memory_object_stream(\n",
    "        max_buffer_size=max_buffer_size\n",
    "    )\n",
    "\n",
    "    async with anyio.create_task_group() as tg:\n",
    "        tg.start_soon(process_message_callback, receive_stream)\n",
    "        async with send_stream:\n",
    "            while not is_shutting_down_f():\n",
    "                msgs = await consumer.getmany(timeout_ms=timeout_ms)\n",
    "                try:\n",
    "                    await _decode_msgs_and_stream(msgs, msg_types, send_stream)\n",
    "                except Exception as e:\n",
    "                    logger.warning(\n",
    "                        f\"_aiokafka_consumer_loop(): Unexpected exception '{e}' caught and ignored for messages: {msgs}\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77397e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"topic_0\"\n",
    "partition = 0\n",
    "msg = MyMessage(url=\"http://www.acme.com\", port=22)\n",
    "record = create_consumer_record(topic=topic, partition=partition, msg=msg)\n",
    "\n",
    "mock_consumer = MagicMock()\n",
    "msgs = {TopicPartition(topic, 0): [record]}\n",
    "\n",
    "f = asyncio.Future()\n",
    "f.set_result(msgs)\n",
    "mock_consumer.configure_mock(**{\"getmany.return_value\": f})\n",
    "mock_callback = Mock()\n",
    "\n",
    "\n",
    "def is_shutting_down_f(mock_func):\n",
    "    def _is_shutting_down_f():\n",
    "        return mock_func.called\n",
    "\n",
    "    return _is_shutting_down_f\n",
    "\n",
    "\n",
    "for is_async in [True, False]:\n",
    "    await _aiokafka_consumer_loop(\n",
    "        consumer=mock_consumer,\n",
    "        max_buffer_size=100,\n",
    "        callbacks={\n",
    "            topic: asyncer.asyncify(mock_callback) if is_async else mock_callback\n",
    "        },\n",
    "        msg_types={topic: MyMessage},\n",
    "        is_shutting_down_f=is_shutting_down_f(mock_consumer.getmany),\n",
    "    )\n",
    "\n",
    "    assert mock_consumer.getmany.call_count == 1\n",
    "    mock_callback.assert_called_once_with(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanity check: exception in callback\n",
    "# # One msg, one topic, process_f called once with callback and decoded_msg\n",
    "\n",
    "# topic = \"topic_0\"\n",
    "# partition = 0\n",
    "# topic_part_0_0 = TopicPartition(topic, partition)\n",
    "# msg = MyMessage(url=\"http://www.acme.com\", port=22)\n",
    "# record = create_consumer_record(topic=topic, partition=partition, msg=msg)\n",
    "\n",
    "\n",
    "# async def process_f(arg):\n",
    "#     callback, msg = arg\n",
    "#     await callback(msg)\n",
    "\n",
    "\n",
    "# for is_async in [False, True]:\n",
    "#     print(f\"is_async={is_async}\")\n",
    "#     callback_0 = Mock()\n",
    "#     callback_0.side_effect = Mock(side_effect=Exception(\"Test\"))\n",
    "#     await process_msgs(\n",
    "#         msgs={topic_part_0_0: [record]},\n",
    "#         callbacks={topic: (asyncer.asyncify(callback_0) if is_async else callback_0)},\n",
    "#         msg_types={topic: MyMessage},\n",
    "#         process_f=process_f,\n",
    "#     )\n",
    "\n",
    "#     #     process_f.assert_called_with((callback_0, msg))\n",
    "#     callback_0.assert_called_with(msg)\n",
    "#     assert callback_0.call_count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46031397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def sanitize_kafka_config(**kwargs):\n",
    "    \"\"\"Sanitize Kafka config\"\"\"\n",
    "    return {k: \"*\"*len(v) if \"pass\" in k.lower() else v for k, v in kwargs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'bootstrap_servers': 'whatever.cloud:9092',\n",
    " 'auto_offset_reset': 'earliest',\n",
    " 'security_protocol': 'SASL_SSL',\n",
    " 'sasl_mechanism': 'PLAIN',\n",
    " 'sasl_plain_username': 'username',\n",
    " 'sasl_plain_password': 'password',\n",
    " 'ssl_context': \"something\"}\n",
    "\n",
    "assert sanitize_kafka_config(**kwargs)[\"sasl_plain_password\"] == '********'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ba3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "async def aiokafka_consumer_loop(  # type: ignore\n",
    "    topics: List[str],\n",
    "    *,\n",
    "    bootstrap_servers: str,\n",
    "    auto_offset_reset: str,\n",
    "    max_poll_records: int = 1_000,\n",
    "    timeout_ms: int = 100,\n",
    "    max_buffer_size: int = 10_000,\n",
    "    callbacks: Dict[str, Callable[[BaseModel], Union[None, Awaitable[None]]]],\n",
    "    msg_types: Dict[str, Type[BaseModel]],\n",
    "    is_shutting_down_f: Callable[[], bool],\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    \"\"\"todo: write docs\"\"\"\n",
    "    logger.info(f\"aiokafka_consumer_loop() starting...\")\n",
    "    try:\n",
    "        consumer_kwargs = dict(\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            auto_offset_reset=auto_offset_reset,\n",
    "            max_poll_records=max_poll_records,\n",
    "        )\n",
    "        consumer_kwargs = {**consumer_kwargs, **kwargs}\n",
    "        consumer = AIOKafkaConsumer(\n",
    "            **consumer_kwargs,\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"aiokafka_consumer_loop(): Consumer created using the following parameters: {sanitize_kafka_config(**consumer_kwargs)}\"\n",
    "        )\n",
    "\n",
    "        await consumer.start()\n",
    "        logger.info(\"aiokafka_consumer_loop(): Consumer started.\")\n",
    "        consumer.subscribe(topics)\n",
    "        logger.info(\"aiokafka_consumer_loop(): Consumer subscribed.\")\n",
    "\n",
    "        try:\n",
    "            await _aiokafka_consumer_loop(\n",
    "                consumer=consumer,\n",
    "                max_buffer_size=max_buffer_size,\n",
    "                timeout_ms=timeout_ms,\n",
    "                callbacks=callbacks,\n",
    "                msg_types=msg_types,\n",
    "                is_shutting_down_f=is_shutting_down_f,\n",
    "            )\n",
    "        finally:\n",
    "            await consumer.stop()\n",
    "            logger.info(f\"aiokafka_consumer_loop(): Consumer stopped.\")\n",
    "            logger.info(f\"aiokafka_consumer_loop() finished.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"aiokafka_consumer_loop(): unexpected exception raised: '{e.__repr__()}'\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fast_kafka_api.helpers: create_missing_topics(['my_topic_aiokafka_consumer_loop_5696213874']): new_topics = [NewTopic(topic=my_topic_aiokafka_consumer_loop_5696213874,num_partitions=3)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efeffa9aa9494b939f884ca019c4ce24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "producing to 'my_topic_aiokafka_consumer_loop_5696213874':   0%|          | 0/9178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: aiokafka_consumer_loop() starting...\n",
      "[INFO] __main__: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'tvrtko-fast-kafka-api-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 1000}\n",
      "[INFO] __main__: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_aiokafka_consumer_loop_5696213874'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_aiokafka_consumer_loop_5696213874'}\n",
      "[INFO] __main__: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_aiokafka_consumer_loop_5696213874': 3}. \n",
      "[INFO] __main__: msgs_received=1000\n",
      "[INFO] __main__: msgs_received=2000\n",
      "[INFO] __main__: msgs_received=3000\n",
      "[INFO] __main__: msgs_received=4000\n",
      "[INFO] __main__: msgs_received=5000\n",
      "[INFO] __main__: msgs_received=6000\n",
      "[INFO] __main__: msgs_received=7000\n",
      "[INFO] __main__: msgs_received=8000\n",
      "[INFO] __main__: msgs_received=9000\n",
      "[INFO] __main__: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] __main__: aiokafka_consumer_loop() finished.\n"
     ]
    }
   ],
   "source": [
    "msgs_sent = 9178\n",
    "msgs = [\n",
    "    MyMessage(url=\"http://www.ai.com\", port=port).json().encode(\"utf-8\")\n",
    "    for port in range(msgs_sent)\n",
    "]\n",
    "msgs_received = 0\n",
    "\n",
    "\n",
    "async def count_msg(msg: MyMessage):\n",
    "    global msgs_received\n",
    "    msgs_received = msgs_received + 1\n",
    "    if msgs_received % 1000 == 0:\n",
    "        logger.info(f\"{msgs_received=}\")\n",
    "\n",
    "\n",
    "async with create_and_fill_testing_topic(\n",
    "    topic_prefix=\"my_topic_aiokafka_consumer_loop_\",\n",
    "    msgs=msgs,\n",
    "    seed=seed(1),\n",
    "    **aiokafka_config,\n",
    ") as topic:\n",
    "    await aiokafka_consumer_loop(\n",
    "        topics=[topic],\n",
    "        auto_offset_reset=\"earliest\",\n",
    "        callbacks={topic: count_msg},\n",
    "        msg_types={topic: MyMessage},\n",
    "        is_shutting_down_f=true_after(2),\n",
    "        **aiokafka_config,\n",
    "    )\n",
    "\n",
    "assert msgs_sent == msgs_received, f\"{msgs_sent} != {msgs_received}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9d9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fast_kafka_api.helpers: create_missing_topics(['my_topic_aiokafka_consumer_loop_5168585847']): new_topics = [NewTopic(topic=my_topic_aiokafka_consumer_loop_5168585847,num_partitions=3)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63b30ad33b84c879424ac30d5a70c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "producing to 'my_topic_aiokafka_consumer_loop_5168585847':   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: aiokafka_consumer_loop() starting...\n",
      "[INFO] __main__: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'tvrtko-fast-kafka-api-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 1000}\n",
      "[INFO] __main__: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_aiokafka_consumer_loop_5168585847'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_aiokafka_consumer_loop_5168585847'}\n",
      "[INFO] __main__: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_aiokafka_consumer_loop_5168585847': 3}. \n",
      "[INFO] __main__: msgs_received=1000\n",
      "[INFO] __main__: msgs_received=2000\n",
      "[INFO] __main__: msgs_received=3000\n",
      "[INFO] __main__: msgs_received=4000\n",
      "[INFO] __main__: msgs_received=5000\n",
      "[INFO] __main__: msgs_received=6000\n",
      "[INFO] __main__: msgs_received=7000\n",
      "[INFO] __main__: msgs_received=8000\n",
      "[INFO] __main__: msgs_received=9000\n",
      "[INFO] __main__: msgs_received=10000\n",
      "[INFO] __main__: msgs_received=11000\n",
      "[INFO] __main__: msgs_received=12000\n",
      "[INFO] __main__: msgs_received=13000\n",
      "[INFO] __main__: msgs_received=14000\n",
      "[INFO] __main__: msgs_received=15000\n",
      "[INFO] __main__: msgs_received=16000\n",
      "[INFO] __main__: msgs_received=17000\n",
      "[INFO] __main__: msgs_received=18000\n",
      "[INFO] __main__: msgs_received=19000\n",
      "[INFO] __main__: msgs_received=20000\n",
      "[INFO] __main__: msgs_received=21000\n",
      "[INFO] __main__: msgs_received=22000\n",
      "[INFO] __main__: msgs_received=23000\n",
      "[INFO] __main__: msgs_received=24000\n",
      "[INFO] __main__: msgs_received=25000\n",
      "[INFO] __main__: msgs_received=26000\n",
      "[INFO] __main__: msgs_received=27000\n",
      "[INFO] __main__: msgs_received=28000\n",
      "[INFO] __main__: msgs_received=29000\n",
      "[INFO] __main__: msgs_received=30000\n",
      "[INFO] __main__: msgs_received=31000\n",
      "[INFO] __main__: msgs_received=32000\n",
      "[INFO] __main__: msgs_received=33000\n",
      "[INFO] __main__: msgs_received=34000\n",
      "[INFO] __main__: msgs_received=35000\n",
      "[INFO] __main__: msgs_received=36000\n",
      "[INFO] __main__: msgs_received=37000\n",
      "[INFO] __main__: msgs_received=38000\n",
      "[INFO] __main__: msgs_received=39000\n",
      "[INFO] __main__: msgs_received=40000\n",
      "[INFO] __main__: msgs_received=41000\n",
      "[INFO] __main__: msgs_received=42000\n",
      "[INFO] __main__: msgs_received=43000\n",
      "[INFO] __main__: msgs_received=44000\n",
      "[INFO] __main__: msgs_received=45000\n",
      "[INFO] __main__: msgs_received=46000\n",
      "[INFO] __main__: msgs_received=47000\n",
      "[INFO] __main__: msgs_received=48000\n",
      "[INFO] __main__: msgs_received=49000\n",
      "[INFO] __main__: msgs_received=50000\n",
      "[INFO] __main__: msgs_received=51000\n",
      "[INFO] __main__: msgs_received=52000\n",
      "[INFO] __main__: msgs_received=53000\n",
      "[INFO] __main__: msgs_received=54000\n",
      "[INFO] __main__: msgs_received=55000\n",
      "[INFO] __main__: msgs_received=56000\n",
      "[INFO] __main__: msgs_received=57000\n",
      "[INFO] __main__: msgs_received=58000\n",
      "[INFO] __main__: msgs_received=59000\n",
      "[INFO] __main__: msgs_received=60000\n",
      "[INFO] __main__: msgs_received=61000\n",
      "[INFO] __main__: msgs_received=62000\n",
      "[INFO] __main__: msgs_received=63000\n",
      "[INFO] __main__: msgs_received=64000\n",
      "[INFO] __main__: msgs_received=65000\n",
      "[INFO] __main__: msgs_received=66000\n",
      "[INFO] __main__: msgs_received=67000\n",
      "[INFO] __main__: msgs_received=68000\n",
      "[INFO] __main__: msgs_received=69000\n",
      "[INFO] __main__: msgs_received=70000\n",
      "[INFO] __main__: msgs_received=71000\n",
      "[INFO] __main__: msgs_received=72000\n",
      "[INFO] __main__: msgs_received=73000\n",
      "[INFO] __main__: msgs_received=74000\n",
      "[INFO] __main__: msgs_received=75000\n",
      "[INFO] __main__: msgs_received=76000\n",
      "[INFO] __main__: msgs_received=77000\n",
      "[INFO] __main__: msgs_received=78000\n",
      "[INFO] __main__: msgs_received=79000\n",
      "[INFO] __main__: msgs_received=80000\n",
      "[INFO] __main__: msgs_received=81000\n",
      "[INFO] __main__: msgs_received=82000\n",
      "[INFO] __main__: msgs_received=83000\n",
      "[INFO] __main__: msgs_received=84000\n",
      "[INFO] __main__: msgs_received=85000\n",
      "[INFO] __main__: msgs_received=86000\n",
      "[INFO] __main__: msgs_received=87000\n",
      "[INFO] __main__: msgs_received=88000\n",
      "[INFO] __main__: msgs_received=89000\n",
      "[INFO] __main__: msgs_received=90000\n",
      "[INFO] __main__: msgs_received=91000\n",
      "[INFO] __main__: msgs_received=92000\n",
      "[INFO] __main__: msgs_received=93000\n",
      "[INFO] __main__: msgs_received=94000\n",
      "[INFO] __main__: msgs_received=95000\n",
      "[INFO] __main__: msgs_received=96000\n",
      "[INFO] __main__: msgs_received=97000\n",
      "[INFO] __main__: msgs_received=98000\n",
      "[INFO] __main__: msgs_received=99000\n",
      "[INFO] __main__: msgs_received=100000\n",
      "[INFO] __main__: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] __main__: aiokafka_consumer_loop() finished.\n",
      "Messages processed: 100,000\n",
      "Time              : 6.29 s\n",
      "Throughput.       : 15,909 msg/s\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "msgs_sent = 100_000\n",
    "msgs = [\n",
    "    MyMessage(url=\"http://www.ai.com\", port=port).json().encode(\"utf-8\")\n",
    "    for port in range(msgs_sent)\n",
    "]\n",
    "msgs_received = 0\n",
    "\n",
    "\n",
    "async def count_msg(msg: MyMessage):\n",
    "    global msgs_received\n",
    "    msgs_received = msgs_received + 1\n",
    "    if msgs_received % 1000 == 0:\n",
    "        logger.info(f\"{msgs_received=}\")\n",
    "\n",
    "\n",
    "def _is_shutting_down_f():\n",
    "    return msgs_received == msgs_sent\n",
    "\n",
    "\n",
    "async with create_and_fill_testing_topic(\n",
    "    topic_prefix=\"my_topic_aiokafka_consumer_loop_\",\n",
    "    msgs=msgs,\n",
    "    seed=seed(3),\n",
    "    **aiokafka_config,\n",
    ") as topic:\n",
    "    start = datetime.now()\n",
    "    await aiokafka_consumer_loop(\n",
    "        topics=[topic],\n",
    "        auto_offset_reset=\"earliest\",\n",
    "        callbacks={topic: count_msg},\n",
    "        msg_types={topic: MyMessage},\n",
    "        is_shutting_down_f=_is_shutting_down_f,\n",
    "        **aiokafka_config\n",
    "    )\n",
    "    t = (datetime.now() - start) / timedelta(seconds=1)\n",
    "    thrp = msgs_received / t\n",
    "\n",
    "    print(f\"Messages processed: {msgs_received:,d}\")\n",
    "    print(f\"Time              : {t:.2f} s\")\n",
    "    print(f\"Throughput.       : {thrp:,.0f} msg/s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
