{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _testing.in_memory_broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import uuid\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "from contextlib import contextmanager\n",
    "import inspect\n",
    "import asyncio\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from typing import *\n",
    "import fastkafka._application.app\n",
    "import fastkafka._components.aiokafka_consumer_loop\n",
    "import fastkafka._components.aiokafka_producer_manager\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "from aiokafka.structs import ConsumerRecord, TopicPartition, RecordMetadata\n",
    "\n",
    "import fastkafka._application.app\n",
    "from fastkafka._components.meta import copy_func, patch, delegates, classcontextmanager, _get_default_kwargs_from_sig\n",
    "from fastkafka._components.logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import asynccontextmanager\n",
    "import unittest\n",
    "\n",
    "import pytest\n",
    "\n",
    "from fastkafka.testing import ApacheKafkaBroker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e8c6f7",
   "metadata": {},
   "source": [
    "# Local Kafka broker\n",
    "> In-memory mockup of Kafka broker protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96928f97",
   "metadata": {},
   "source": [
    "## Kafka partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d6bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@dataclass\n",
    "class KafkaRecord:\n",
    "    topic: str = \"\"\n",
    "    partition: int = 0\n",
    "    key: Optional[bytes] = None\n",
    "    value: bytes = b\"\"\n",
    "    offset: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fae851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class KafkaPartition:\n",
    "    def __init__(self, *, partition: int, topic: str):\n",
    "        self.partition = partition\n",
    "        self.topic = topic\n",
    "        self.messages: List[KafkaRecord] = list()\n",
    "\n",
    "    def write(self, value: bytes, key: Optional[bytes] = None) -> KafkaRecord:\n",
    "        record = KafkaRecord(\n",
    "            topic=self.topic, partition=self.partition, value=value, key=key, offset=len(self.messages)\n",
    "        )\n",
    "        self.messages.append(record)\n",
    "        return record\n",
    "\n",
    "    def read(self, offset: int) -> List[KafkaRecord]:\n",
    "        return self.messages[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_index = 0\n",
    "topic = \"test\"\n",
    "partition = KafkaPartition(partition=partition_index, topic=topic)\n",
    "\n",
    "msgs = [b\"some_msg\" for _ in range(25)]\n",
    "expected = [\n",
    "    KafkaRecord(topic=topic, partition=partition_index, value=msg, offset=offset)\n",
    "    for offset, msg in enumerate(msgs)\n",
    "]\n",
    "\n",
    "for msg in msgs:\n",
    "    partition.write(msg)\n",
    "\n",
    "for offset in [0, 10, 20]:\n",
    "    actual = partition.read(offset=offset)\n",
    "\n",
    "    assert actual == expected[offset:], print(f\"{actual} != {expected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be209005",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_index = 0\n",
    "topic = \"test\"\n",
    "key = b\"some_key\"\n",
    "partition = KafkaPartition(partition=partition_index, topic=topic)\n",
    "\n",
    "msgs = [b\"some_msg\" for _ in range(25)]\n",
    "expected = [\n",
    "    KafkaRecord(\n",
    "        topic=topic, partition=partition_index, value=msg, key=key, offset=offset\n",
    "    )\n",
    "    for offset, msg in enumerate(msgs)\n",
    "]\n",
    "\n",
    "for msg in msgs:\n",
    "    partition.write(msg, key=key)\n",
    "\n",
    "for offset in [0, 10, 20]:\n",
    "    actual = partition.read(offset=offset)\n",
    "\n",
    "    assert actual == expected[offset:], print(f\"{actual} != {expected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145cd897",
   "metadata": {},
   "source": [
    "## Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class KafkaTopic:\n",
    "    def __init__(self, topic: str, num_partitions: int = 1):\n",
    "        self.topic = topic\n",
    "        self.num_partitions = num_partitions\n",
    "        self.partitions: List[KafkaPartition] = [\n",
    "            KafkaPartition(topic=topic, partition=partition_index)\n",
    "            for partition_index in range(num_partitions)\n",
    "        ]\n",
    "\n",
    "    def read( # type: ignore\n",
    "        self, partition: int, offset: int\n",
    "    ) -> Dict[TopicPartition, List[KafkaRecord]]:\n",
    "        topic_partition = TopicPartition(topic=self.topic, partition=partition)\n",
    "        records = self.partitions[partition].read(offset)\n",
    "        return {topic_partition: records}\n",
    "\n",
    "    def write_with_partition(\n",
    "        self,\n",
    "        value: bytes,\n",
    "        partition: int,\n",
    "    ) -> KafkaRecord:\n",
    "        return self.partitions[partition].write(value)\n",
    "\n",
    "    def write_with_key(self, value: bytes, key: bytes) -> KafkaRecord:\n",
    "        partition = int(hash(key)) % self.num_partitions\n",
    "        return self.partitions[partition].write(value, key=key)\n",
    "\n",
    "    def write(\n",
    "        self,\n",
    "        value: bytes,\n",
    "        key: Optional[bytes] = None,\n",
    "        partition: Optional[int] = None,\n",
    "    ) -> KafkaRecord:\n",
    "        if partition is not None:\n",
    "            return self.write_with_partition(value, partition)\n",
    "\n",
    "        if key is not None:\n",
    "            return self.write_with_key(value, key)\n",
    "\n",
    "        partition = random.randint(0, self.num_partitions - 1) # nosec\n",
    "        return self.write_with_partition(value, partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fcfe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = b\"msg\"\n",
    "\n",
    "topic = KafkaTopic(\"test_topic\", 1)\n",
    "\n",
    "expected = KafkaRecord(topic=\"test_topic\", partition=0, value=msg, offset=0)\n",
    "actual = topic.write(msg)\n",
    "\n",
    "assert expected == actual\n",
    "\n",
    "expected = KafkaRecord(topic=\"test_topic\", partition=0, value=msg, key=b\"123\", offset=1)\n",
    "actual = topic.write(msg, key=b\"123\")\n",
    "\n",
    "assert expected == actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = \"test_topic\"\n",
    "msgs = [b\"msg\" for _ in range(1000)]\n",
    "partition_num = 10\n",
    "\n",
    "topic = KafkaTopic(topic_name, partition_num)\n",
    "\n",
    "# write to topic\n",
    "for msg in msgs:\n",
    "    topic.write(msg)\n",
    "\n",
    "# For each partition in topic check:\n",
    "for partition in range(partition_num):\n",
    "    topic_partition = TopicPartition(topic=topic_name, partition=partition)\n",
    "    data = topic.read(partition=partition, offset=0)\n",
    "    \n",
    "    # Read returns correct TopicPartition key\n",
    "    assert topic_partition in data.keys()\n",
    "    \n",
    "    # Data is written into partition\n",
    "    assert len(data[topic_partition]) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f492339",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = \"test_topic\"\n",
    "msgs = [b\"msg\" for _ in range(1000)]\n",
    "partition_num = 2\n",
    "\n",
    "topic = KafkaTopic(topic_name, partition_num)\n",
    "\n",
    "# write to topic with defined partition\n",
    "for msg in msgs:\n",
    "    topic.write(msg, partition=0)\n",
    "\n",
    "lengths = [\n",
    "    len(\n",
    "        topic.read(partition=i, offset=0)[TopicPartition(topic=topic_name, partition=i)]\n",
    "    )\n",
    "    for i in range(partition_num)\n",
    "]\n",
    "\n",
    "assert [1000, 0] == lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff42583",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = \"test_topic\"\n",
    "msgs = [b\"msg\" for _ in range(1000)]\n",
    "partition_num = 3\n",
    "\n",
    "topic = KafkaTopic(topic_name, partition_num)\n",
    "\n",
    "# write to topic with defined key\n",
    "for msg in msgs[:450]:\n",
    "    topic.write(msg, key=b\"some_key\")\n",
    "\n",
    "for msg in msgs[450:]:\n",
    "    topic.write(msg, key=b\"some_key443\")\n",
    "\n",
    "lengths = [\n",
    "    len(\n",
    "        topic.read(partition=i, offset=0)[TopicPartition(topic=topic_name, partition=i)]\n",
    "    )\n",
    "    for i in range(partition_num)\n",
    "]\n",
    "\n",
    "assert [0, 450, 550] == sorted(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25de69",
   "metadata": {},
   "source": [
    "## Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ConsumerMetadata:\n",
    "    topic: str\n",
    "    offset: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e96510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaBroker:\n",
    "    def __init__(topics: Iterable[str], num_partitions: int):\n",
    "        self.topics: Dict[str, KafkaTopic] = {\n",
    "            topic: KafkaTopic(topic, num_partitions) for topic in topics\n",
    "        }\n",
    "            \n",
    "    def connect() -> uuid.UUID:\n",
    "        return uuid.uuid4()\n",
    "    \n",
    "    def dissconnect():\n",
    "        pass\n",
    "    \n",
    "    def subscribe():\n",
    "        pass\n",
    "    \n",
    "    def read():\n",
    "        pass\n",
    "    \n",
    "    def write():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238be0f",
   "metadata": {},
   "source": [
    "## Kafka Broker Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7596402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def create_consumer_record(topic: str, msg: bytes) -> ConsumerRecord: # type: ignore\n",
    "    record = ConsumerRecord(\n",
    "        topic=topic,\n",
    "        partition=0,\n",
    "        offset=0,\n",
    "        timestamp=0,\n",
    "        timestamp_type=0,\n",
    "        key=None,\n",
    "        value=msg,\n",
    "        checksum=0,\n",
    "        serialized_key_size=0,\n",
    "        serialized_value_size=0,\n",
    "        headers=[],\n",
    "    )\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = create_consumer_record(\"my_topic\", b\"my_msg\")\n",
    "record.partition = 1\n",
    "assert record.partition == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_meta = ConsumerMetadata(\"my_topic\", 0)\n",
    "assert consumer_meta.topic == \"my_topic\"\n",
    "assert consumer_meta.offset == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5737f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# InMemoryBroker\n",
    "@classcontextmanager()\n",
    "class InMemoryBroker:\n",
    "    def __init__(self, topics: Set[str]):\n",
    "        self.data: Dict[str, List[ConsumerRecord]] = {topic: list() for topic in topics}  # type: ignore\n",
    "        self.consumers_metadata: Dict[uuid.UUID, List[ConsumerMetadata]] = {}\n",
    "        self.is_started: bool = False\n",
    "\n",
    "    def connect(self) -> uuid.UUID:\n",
    "        return uuid.uuid4()\n",
    "\n",
    "    def subscribe(self, actor_id: uuid.UUID, *, auto_offest_reset: str, topic: str) -> None:\n",
    "        consumer_metadata = self.consumers_metadata.get(actor_id, list())\n",
    "        # todo: what if whatever?\n",
    "        consumer_metadata.append(\n",
    "            ConsumerMetadata(\n",
    "                topic, len(self.data[topic]) if auto_offest_reset == \"latest\" else 0\n",
    "            )\n",
    "        )\n",
    "        self.consumers_metadata[actor_id] = consumer_metadata\n",
    "\n",
    "    def unsubscribe(self, actor_id: uuid.UUID) -> None:\n",
    "        try:\n",
    "            del self.consumers_metadata[actor_id]\n",
    "        except KeyError:\n",
    "            logger.warning(f\"No subscription with {actor_id=} found!\")\n",
    "\n",
    "    def produce(  # type: ignore\n",
    "        self, *, topic: str, msg: bytes, key: Optional[bytes] = None\n",
    "    ) -> RecordMetadata:\n",
    "        if topic in self.data:\n",
    "            record = create_consumer_record(topic, msg)\n",
    "            self.data[topic].append(record)\n",
    "        else:\n",
    "            # todo: log only once\n",
    "            logger.warning(\n",
    "                f\"Topic {topic} is not available during auto-create initialization\"\n",
    "            )\n",
    "        return RecordMetadata(\n",
    "            topic=topic,\n",
    "            partition=0,\n",
    "            topic_partition=TopicPartition(topic=topic, partition=0),\n",
    "            offset=0,\n",
    "            timestamp=1680602752070,\n",
    "            timestamp_type=0,\n",
    "            log_start_offset=0,\n",
    "        )\n",
    "\n",
    "    def consume(  # type: ignore\n",
    "        self, actor_id: uuid.UUID\n",
    "    ) -> Dict[TopicPartition, List[ConsumerRecord]]:\n",
    "        msgs: Dict[TopicPartition, List[ConsumerRecord]] = {}  # type: ignore\n",
    "\n",
    "        consumer_metadata = self.consumers_metadata[actor_id]\n",
    "\n",
    "        for metadata in consumer_metadata:\n",
    "            try:\n",
    "                msgs[TopicPartition(metadata.topic, 0)] = self.data[metadata.topic][\n",
    "                    metadata.offset :\n",
    "                ]\n",
    "                metadata.offset = len(self.data[metadata.topic])\n",
    "            except KeyError:\n",
    "                raise RuntimeError(\n",
    "                    f\"{metadata.topic=} not found, did you pass it to InMemoryBroker on init to be created?\"\n",
    "                )\n",
    "        return msgs\n",
    "\n",
    "    @contextmanager\n",
    "    def lifecycle(self) -> Iterator[\"InMemoryBroker\"]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    async def _start(self) -> str:\n",
    "        logger.info(\"InMemoryBroker._start() called\")\n",
    "        self.__enter__() # type: ignore\n",
    "        return \"localbroker:0\"\n",
    "\n",
    "    async def _stop(self) -> None:\n",
    "        logger.info(\"InMemoryBroker._stop() called\")\n",
    "        self.__exit__(None, None, None) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59295ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_broker = InMemoryBroker([\"my_topic\"])\n",
    "\n",
    "actor_id = in_memory_broker.connect()\n",
    "\n",
    "with pytest.raises(KeyError) as e:\n",
    "    in_memory_broker.consume(actor_id)\n",
    "    \n",
    "print(e)\n",
    "\n",
    "in_memory_broker.subscribe(actor_id=actor_id, auto_offest_reset=\"earliest\", topic=\"my_topic\")\n",
    "\n",
    "msg = in_memory_broker.consume(actor_id)\n",
    "assert msg == {TopicPartition(topic='my_topic', partition=0): []}, msg\n",
    "\n",
    "# with raise()\n",
    "in_memory_broker.produce(topic=\"not_my_topic\", msg=b\"not my message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88761db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "with ApacheKafkaBroker([\"my_topic\"], apply_nest_asyncio=True) as bootstrap_servers:\n",
    "    producer = AIOKafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    await producer.start()\n",
    "    for _ in range(1000):\n",
    "        record = await producer.send(topic=\"not_my_topic\", value=b\"not my message\")\n",
    "    await producer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dcfab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "with ApacheKafkaBroker([\"my_topic\"], apply_nest_asyncio=True) as bootstrap_servers:\n",
    "    consumer = AIOKafkaConsumer(\"my_topic\", bootstrap_servers=bootstrap_servers)\n",
    "    await consumer.start()\n",
    "    print(\"getmany()...\")\n",
    "    msg = await consumer.getmany(timeout_ms=0)\n",
    "    print(\"exiting...\")\n",
    "    await consumer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5b3a9",
   "metadata": {},
   "source": [
    "## Consumer patching\n",
    "\n",
    "We need to patch AIOKafkaConsumer methods so that we can redirect the consumer to our local kafka broker.\n",
    "\n",
    "Patched methods:\n",
    "\n",
    "- [x] \\_\\_init\\_\\_\n",
    "- [x] start\n",
    "- [x] subscribe\n",
    "- [x] stop\n",
    "- [x] getmany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ca42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# InMemoryConsumer\n",
    "class InMemoryConsumer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        broker: InMemoryBroker,\n",
    "    ) -> None:\n",
    "        self.broker = broker\n",
    "        self._id: Optional[uuid.UUID] = None\n",
    "        self._auto_offset_reset: str = \"latest\"\n",
    "\n",
    "    @delegates(AIOKafkaConsumer)\n",
    "    def __call__(\n",
    "        self, \n",
    "        **kwargs: Any\n",
    "    ) -> \"InMemoryConsumer\":   \n",
    "        defaults = _get_default_kwargs_from_sig(InMemoryConsumer.__call__, **kwargs)\n",
    "        consume_copy = InMemoryConsumer(self.broker)\n",
    "        consume_copy._auto_offset_reset = defaults[\"auto_offset_reset\"]\n",
    "        return consume_copy\n",
    "\n",
    "    @delegates(AIOKafkaConsumer.start)\n",
    "    async def start(self, **kwargs: Any) -> None:\n",
    "        pass\n",
    "\n",
    "    @delegates(AIOKafkaConsumer.stop)\n",
    "    async def stop(self, **kwargs: Any) -> None:\n",
    "        pass\n",
    "\n",
    "    @delegates(AIOKafkaConsumer.subscribe)\n",
    "    def subscribe(self, topics: List[str], **kwargs: Any) -> None:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @delegates(AIOKafkaConsumer.getmany)\n",
    "    async def getmany( # type: ignore\n",
    "        self, **kwargs: Any\n",
    "    ) -> Dict[TopicPartition, List[ConsumerRecord]]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677208e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = InMemoryBroker([\"topic\"])\n",
    "\n",
    "ConsumerClass = InMemoryConsumer(broker)\n",
    "\n",
    "for cls in [ConsumerClass, AIOKafkaConsumer]:\n",
    "\n",
    "    consumer = cls()\n",
    "    assert consumer._auto_offset_reset == \"latest\"\n",
    "\n",
    "    consumer = cls(auto_offset_reset=\"earliest\")\n",
    "    assert consumer._auto_offset_reset == \"earliest\", consumer._auto_offset_reset\n",
    "\n",
    "    consumer = cls(auto_offset_reset=\"whatever\")\n",
    "    assert consumer._auto_offset_reset == \"whatever\"\n",
    "    \n",
    "    await consumer.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e4df06",
   "metadata": {},
   "source": [
    "Patching start so that we don't try to start the real AIOKafkaConsumer instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "@delegates(AIOKafkaConsumer.start)\n",
    "async def start(self: InMemoryConsumer, **kwargs: Any) -> None:\n",
    "    logger.info(\"AIOKafkaConsumer patched start() called()\")\n",
    "    if self._id is not None:\n",
    "        raise RuntimeError(\n",
    "            \"Consumer start() already called! Run consumer stop() before running start() again\"\n",
    "        )\n",
    "    self._id = self.broker.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072dd1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = InMemoryBroker([\"my_topic\"])\n",
    "\n",
    "ConsumerClass = InMemoryConsumer(broker)\n",
    "\n",
    "for cls in [ConsumerClass]:\n",
    "\n",
    "    consumer = cls()\n",
    "    await consumer.start()\n",
    "    await consumer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c11cd3",
   "metadata": {},
   "source": [
    "Patching subscribe so that we can connect to our Local, in-memory, Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de72311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "@delegates(AIOKafkaConsumer.subscribe)\n",
    "def subscribe(self: InMemoryConsumer, topics: List[str], **kwargs: Any) -> None:\n",
    "    logger.info(\"AIOKafkaConsumer patched subscribe() called\")\n",
    "    if self._id is None:\n",
    "        raise RuntimeError(\"Consumer start() not called! Run consumer start() first\")\n",
    "    logger.info(f\"AIOKafkaConsumer.subscribe(), subscribing to: {topics}\")\n",
    "    for topic in topics:\n",
    "        self.broker.subscribe(\n",
    "            self._id, topic=topic, auto_offest_reset=self._auto_offset_reset\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = InMemoryBroker(topics=[\"my_topic\"])\n",
    "\n",
    "ConsumerClass = InMemoryConsumer(broker)\n",
    "consumer = ConsumerClass()\n",
    "\n",
    "await consumer.start()\n",
    "consumer.subscribe([\"my_topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80a4d5",
   "metadata": {},
   "source": [
    "Patching stop so that be dont break anything by calling the real AIOKafkaConsumer stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@patch\n",
    "@delegates(AIOKafkaConsumer.stop)\n",
    "async def stop(self: InMemoryConsumer, **kwargs: Any) -> None:\n",
    "    logger.info(\"AIOKafkaConsumer patched stop() called\")\n",
    "    if self._id is None:\n",
    "        raise RuntimeError(\n",
    "            \"Consumer start() not called! Run consumer start() first\"\n",
    "        )\n",
    "    self.broker.unsubscribe(self._id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc667214",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = InMemoryBroker(topics=[\"my_topic\"])\n",
    "    \n",
    "ConsumerClass = InMemoryConsumer(broker)\n",
    "consumer = ConsumerClass()\n",
    "\n",
    "await consumer.start()\n",
    "consumer.subscribe([\"my_topic\"])\n",
    "await consumer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c733b4",
   "metadata": {},
   "source": [
    "Patching getmany so that the messages are pulled from our Local, in-memory, Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@patch\n",
    "@delegates(AIOKafkaConsumer.getmany)\n",
    "async def getmany( # type: ignore\n",
    "    self: InMemoryConsumer, **kwargs: Any\n",
    ") -> Dict[TopicPartition, List[ConsumerRecord]]:\n",
    "    return self.broker.consume(self._id) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4940bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = InMemoryBroker(topics=[\"my_topic\"])\n",
    "\n",
    "ConsumerClass = InMemoryConsumer(broker)\n",
    "consumer = ConsumerClass(auto_offset_reset=\"latest\")\n",
    "\n",
    "await consumer.start()\n",
    "\n",
    "consumer.subscribe([\"my_topic\"])\n",
    "await consumer.getmany()\n",
    "\n",
    "await consumer.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723468f0",
   "metadata": {},
   "source": [
    "## Producer patching\n",
    "\n",
    "We need to patch AIOKafkaProducer methods so that we can redirect the producer to our local kafka broker\n",
    "\n",
    "- [x] \\_\\_init\\_\\_\n",
    "- [x] start\n",
    "- [x] stop\n",
    "- [x] send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class InMemoryProducer:\n",
    "    def __init__(self, broker: InMemoryBroker, **kwargs: Any) -> None:\n",
    "        self.broker = broker\n",
    "        self.id: Optional[uuid.UUID] = None\n",
    "\n",
    "    @delegates(AIOKafkaProducer)\n",
    "    def __call__(self, **kwargs: Any) -> \"InMemoryProducer\":\n",
    "        return InMemoryProducer(self.broker)\n",
    "\n",
    "    @delegates(AIOKafkaProducer.start)\n",
    "    async def start(self, **kwargs: Any) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @delegates(AIOKafkaProducer.stop)\n",
    "    async def stop(self, **kwargs: Any) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @delegates(AIOKafkaProducer.send)\n",
    "    async def send(  # type: ignore\n",
    "        self: AIOKafkaProducer,\n",
    "        topic: str,\n",
    "        msg: bytes,\n",
    "        key: Optional[bytes] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf322a",
   "metadata": {},
   "source": [
    "Patching AIOKafkaProducer start so that we mock the startup procedure of AIOKafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ac5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch # type: ignore\n",
    "@delegates(AIOKafkaProducer.start)\n",
    "async def start(self: InMemoryProducer, **kwargs: Any) -> None:\n",
    "    logger.info(\"AIOKafkaProducer patched start() called()\")\n",
    "    if self.id is not None:\n",
    "        raise RuntimeError(\n",
    "            \"Producer start() already called! Run producer stop() before running start() again\"\n",
    "        )\n",
    "    self.id = self.broker.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = InMemoryBroker(topics=[\"my_topic\"])\n",
    "\n",
    "ProducerClass = InMemoryProducer(broker)\n",
    "producer = ProducerClass()\n",
    "\n",
    "await producer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e9be9",
   "metadata": {},
   "source": [
    "Patching AIOKafkaProducerStop so that we don't uniintentionally try to stop a real instance of AIOKafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32412969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch # type: ignore\n",
    "@delegates(AIOKafkaProducer.stop)\n",
    "async def stop(self: InMemoryProducer, **kwargs: Any) -> None:\n",
    "    logger.info(\"AIOKafkaProducer patched stop() called\")\n",
    "    if self.id is None:\n",
    "        raise RuntimeError(\n",
    "            \"Producer start() not called! Run producer start() first\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = InMemoryBroker(topics=[\"my_topic\"])\n",
    "\n",
    "ProducerClass = InMemoryProducer(broker)\n",
    "producer = ProducerClass()\n",
    "\n",
    "await producer.start()\n",
    "await producer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c77a56",
   "metadata": {},
   "source": [
    "Patching AIOKafkaProducer send so that we redirect sent messages to Local, in-memory, Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "@delegates(AIOKafkaProducer.send)\n",
    "async def send( # type: ignore\n",
    "    self: InMemoryProducer,\n",
    "    topic: str,\n",
    "    msg: bytes,\n",
    "    key: Optional[bytes] = None,\n",
    "    **kwargs: Any,\n",
    "): #asyncio.Task[ConsumerRecord]\n",
    "    if self.id is None:\n",
    "        raise RuntimeError(\n",
    "            \"Producer start() not called! Run producer start() first\"\n",
    "        )\n",
    "    record = self.broker.produce(topic=topic, msg=msg, key=key)\n",
    "\n",
    "    async def _f(record: ConsumerRecord = record) -> ConsumerRecord: # type: ignore\n",
    "        return record\n",
    "\n",
    "    return asyncio.create_task(_f())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda1d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = InMemoryBroker(topics=[\"my_topic\"])\n",
    "\n",
    "ProducerClass = InMemoryProducer(broker)\n",
    "producer = ProducerClass()\n",
    "\n",
    "await producer.start()\n",
    "msg_fut = await producer.send(\"my_topic\", b\"some_msg\")\n",
    "await msg_fut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4d12b",
   "metadata": {},
   "source": [
    "## Add patching to InMemoryBroker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "@contextmanager\n",
    "def lifecycle(self: InMemoryBroker) -> Iterator[InMemoryBroker]:\n",
    "    logger.info(\n",
    "        \"InMemoryBroker._patch_consumers_and_producers(): Patching consumers and producers!\"\n",
    "    )\n",
    "    try:\n",
    "        logger.info(\"InMemoryBroker starting\")\n",
    "        \n",
    "        old_consumer_app = fastkafka._application.app.AIOKafkaConsumer\n",
    "        old_producer_app = fastkafka._application.app.AIOKafkaProducer\n",
    "        old_consumer_loop = fastkafka._components.aiokafka_consumer_loop.AIOKafkaConsumer\n",
    "        old_producer_manager = fastkafka._components.aiokafka_producer_manager.AIOKafkaProducer\n",
    "        \n",
    "        fastkafka._application.app.AIOKafkaConsumer = InMemoryConsumer(self)\n",
    "        fastkafka._application.app.AIOKafkaProducer = InMemoryProducer(self)\n",
    "        fastkafka._components.aiokafka_consumer_loop.AIOKafkaConsumer = InMemoryConsumer(self)\n",
    "        fastkafka._components.aiokafka_producer_manager.AIOKafkaProducer = InMemoryProducer(self)\n",
    "        \n",
    "        self.is_started = True\n",
    "        yield self\n",
    "    finally:\n",
    "        logger.info(\"InMemoryBroker stopping\")\n",
    "        \n",
    "        fastkafka._application.app.AIOKafkaConsumer = old_consumer_app\n",
    "        fastkafka._application.app.AIOKafkaProducer = old_producer_app\n",
    "        fastkafka._components.aiokafka_consumer_loop.AIOKafkaConsumer = old_consumer_loop\n",
    "        fastkafka._components.aiokafka_producer_manager.AIOKafkaProducer = old_producer_manager\n",
    "        \n",
    "        self.is_started = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert fastkafka._application.app.AIOKafkaConsumer == AIOKafkaConsumer\n",
    "assert fastkafka._application.app.AIOKafkaProducer == AIOKafkaProducer\n",
    "\n",
    "with InMemoryBroker([\"topic\"]) as broker:\n",
    "    assert isinstance(fastkafka._application.app.AIOKafkaConsumer, InMemoryConsumer)\n",
    "    assert isinstance(fastkafka._application.app.AIOKafkaProducer, InMemoryProducer)\n",
    "    assert fastkafka._application.app.AIOKafkaConsumer().broker == broker\n",
    "    assert fastkafka._application.app.AIOKafkaProducer().broker == broker\n",
    "    \n",
    "assert fastkafka._application.app.AIOKafkaConsumer == AIOKafkaConsumer\n",
    "assert fastkafka._application.app.AIOKafkaProducer == AIOKafkaProducer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335aea0",
   "metadata": {},
   "source": [
    "## Broker, consumer and producer integration tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4275bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@asynccontextmanager\n",
    "async def create_consumer_and_producer(\n",
    "    auto_offset_reset: str = \"latest\",\n",
    ") -> AsyncIterator[Tuple[AIOKafkaConsumer, AIOKafkaProducer]]:\n",
    "    consumer = fastkafka._application.app.AIOKafkaConsumer(auto_offset_reset=auto_offset_reset)\n",
    "    producer = fastkafka._application.app.AIOKafkaProducer()\n",
    "\n",
    "    await consumer.start()\n",
    "    await producer.start()\n",
    "\n",
    "    yield (consumer, producer)\n",
    "\n",
    "    await consumer.stop()\n",
    "    await producer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7688d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkEqual(L1, L2):\n",
    "    return len(L1) == len(L2) and sorted(L1) == sorted(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert checkEqual([1, 2], [3]) == False\n",
    "assert checkEqual([1, 2, 3], [3, 2, 1]) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc21a6e",
   "metadata": {},
   "source": [
    "Sanity check, let's see if the messages are sent to broker and received by the consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90249e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"test_topic\"\n",
    "sent_msgs = [f\"msg{i}\".encode(\"UTF-8\") for i in range(320)]\n",
    "\n",
    "with InMemoryBroker([topic]) as broker:\n",
    "    async with create_consumer_and_producer(auto_offset_reset=\"earliest\") as (\n",
    "        consumer,\n",
    "        producer,\n",
    "    ):\n",
    "        [await producer.send(topic, msg) for msg in sent_msgs]\n",
    "        consumer.subscribe([topic])\n",
    "        received = await consumer.getmany()\n",
    "        received_msgs = [msg.value for _, msgs in received.items() for msg in msgs]\n",
    "        data = [msg.value for msg in broker.data[topic]]\n",
    "    assert checkEqual(\n",
    "        received_msgs, sent_msgs\n",
    "    ), f\"{sent_msgs=}\\n{received_msgs=}\\n{data=}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40fa9ed",
   "metadata": {},
   "source": [
    "Check if only subscribed topic messages are received by the consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic1 = \"test_topic1\"\n",
    "topic2 = \"test_topic2\"\n",
    "sent_msgs_1 = [(f\"msg{i}\" + topic1).encode(\"UTF-8\") for i in range(32)]\n",
    "sent_msgs_2 = [(f\"msg{i}\" + topic2).encode(\"UTF-8\") for i in range(32)]\n",
    "\n",
    "with InMemoryBroker([topic1, topic2]) as broker:\n",
    "    async with create_consumer_and_producer(auto_offset_reset=\"earliest\") as (\n",
    "        consumer,\n",
    "        producer,\n",
    "    ):\n",
    "        [await producer.send(topic1, msg) for msg in sent_msgs_1]\n",
    "        [await producer.send(topic2, msg) for msg in sent_msgs_2]\n",
    "\n",
    "        consumer.subscribe([topic1])\n",
    "        received = await consumer.getmany()\n",
    "        received_msgs = [msg.value for _, msgs in received.items() for msg in msgs]\n",
    "\n",
    "    assert checkEqual(sent_msgs_1, received_msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1c5c5",
   "metadata": {},
   "source": [
    "Check if msgs are received only after subscribing when auto_offset_reset is set to \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6bba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"test_topic\"\n",
    "sent_msgs_before = [f\"msg{i}\".encode(\"UTF-8\") for i in range(32)]\n",
    "sent_msgs_after = [f\"msg{i}\".encode(\"UTF-8\") for i in range(32, 64)]\n",
    "\n",
    "with InMemoryBroker([topic]) as broker:\n",
    "    async with create_consumer_and_producer() as (consumer, producer):\n",
    "        [await producer.send(topic, msg) for msg in sent_msgs_before]\n",
    "\n",
    "        consumer.subscribe([topic])\n",
    "        [await producer.send(topic, msg) for msg in sent_msgs_after]\n",
    "        received = await consumer.getmany()\n",
    "        received_msgs = [msg.value for _, msgs in received.items() for msg in msgs]\n",
    "\n",
    "    assert checkEqual(sent_msgs_after, received_msgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
