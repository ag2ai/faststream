{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _testing.in_memory_broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import uuid\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "from contextlib import contextmanager\n",
    "import inspect\n",
    "import asyncio\n",
    "import copy\n",
    "import random\n",
    "import hashlib\n",
    "import string\n",
    "\n",
    "from typing import *\n",
    "import fastkafka._application.app\n",
    "import fastkafka._components.aiokafka_consumer_loop\n",
    "import fastkafka._components.aiokafka_producer_manager\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "from aiokafka.structs import ConsumerRecord, TopicPartition, RecordMetadata\n",
    "\n",
    "import fastkafka._application.app\n",
    "from fastkafka._components.meta import copy_func, patch, delegates, classcontextmanager, _get_default_kwargs_from_sig\n",
    "from fastkafka._components.logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import asynccontextmanager\n",
    "import unittest\n",
    "\n",
    "import pytest\n",
    "\n",
    "from fastkafka.testing import ApacheKafkaBroker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e8c6f7",
   "metadata": {},
   "source": [
    "# Local Kafka broker\n",
    "> In-memory mockup of Kafka broker protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96928f97",
   "metadata": {},
   "source": [
    "## Kafka partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d6bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class KafkaRecord:\n",
    "    topic: str = \"\"\n",
    "    partition: int = 0\n",
    "    key: Optional[bytes] = None\n",
    "    value: bytes = b\"\"\n",
    "    offset: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fae851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class KafkaPartition:\n",
    "    def __init__(self, *, partition: int, topic: str):\n",
    "        self.partition = partition\n",
    "        self.topic = topic\n",
    "        self.messages: List[KafkaRecord] = list()\n",
    "\n",
    "    def write(self, value: bytes, key: Optional[bytes] = None) -> RecordMetadata: # type: ignore\n",
    "        record = KafkaRecord(\n",
    "            topic=self.topic,\n",
    "            partition=self.partition,\n",
    "            value=value,\n",
    "            key=key,\n",
    "            offset=len(self.messages),\n",
    "        )\n",
    "        record_meta = RecordMetadata(\n",
    "            topic=self.topic,\n",
    "            partition=self.partition,\n",
    "            topic_partition=TopicPartition(topic=self.topic, partition=self.partition),\n",
    "            offset=len(self.messages),\n",
    "            timestamp=1680602752070,\n",
    "            timestamp_type=0,\n",
    "            log_start_offset=0,\n",
    "        )\n",
    "        self.messages.append(record)\n",
    "        return record_meta\n",
    "\n",
    "    def read(self, offset: int) -> Tuple[List[KafkaRecord], int]:\n",
    "        return self.messages[offset:], len(self.messages)\n",
    "\n",
    "    def latest_offset(self) -> int:\n",
    "        return len(self.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_index = 0\n",
    "topic = \"test\"\n",
    "partition = KafkaPartition(partition=partition_index, topic=topic)\n",
    "\n",
    "msgs = [b\"some_msg\" for _ in range(25)]\n",
    "\n",
    "expected = [\n",
    "    KafkaRecord(topic=topic, partition=partition_index, value=msg, offset=offset)\n",
    "    for offset, msg in enumerate(msgs)\n",
    "]\n",
    "\n",
    "for msg in msgs:\n",
    "    partition.write(msg)\n",
    "\n",
    "for offset in [0, 10, 20]:\n",
    "    actual = partition.read(offset=offset)\n",
    "\n",
    "    assert actual == (expected[offset:], len(msgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be209005",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_index = 0\n",
    "topic = \"test\"\n",
    "key = b\"some_key\"\n",
    "partition = KafkaPartition(partition=partition_index, topic=topic)\n",
    "\n",
    "msgs = [b\"some_msg\" for _ in range(25)]\n",
    "expected = [\n",
    "    KafkaRecord(\n",
    "        topic=topic, partition=partition_index, value=msg, key=key, offset=offset\n",
    "    )\n",
    "    for offset, msg in enumerate(msgs)\n",
    "]\n",
    "\n",
    "for msg in msgs:\n",
    "    partition.write(msg, key=key)\n",
    "\n",
    "for offset in [0, 10, 20]:\n",
    "    actual = partition.read(offset=offset)\n",
    "\n",
    "    assert actual == (expected[offset:], len(msgs)), print(f\"{actual} != {expected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145cd897",
   "metadata": {},
   "source": [
    "## Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class KafkaTopic:\n",
    "    def __init__(self, topic: str, num_partitions: int = 1):\n",
    "        self.topic = topic\n",
    "        self.num_partitions = num_partitions\n",
    "        self.partitions: List[KafkaPartition] = [\n",
    "            KafkaPartition(topic=topic, partition=partition_index)\n",
    "            for partition_index in range(num_partitions)\n",
    "        ]\n",
    "\n",
    "    def read(  # type: ignore\n",
    "        self, partition: int, offset: int\n",
    "    ) -> Tuple[TopicPartition, List[KafkaRecord], int]:\n",
    "        topic_partition = TopicPartition(topic=self.topic, partition=partition)\n",
    "        records, offset = self.partitions[partition].read(offset)\n",
    "        return topic_partition, records, offset\n",
    "\n",
    "    def write_with_partition(  # type: ignore\n",
    "        self,\n",
    "        value: bytes,\n",
    "        partition: int,\n",
    "    ) -> RecordMetadata:\n",
    "        return self.partitions[partition].write(value)\n",
    "\n",
    "    def write_with_key(self, value: bytes, key: bytes) -> RecordMetadata:  # type: ignore\n",
    "        partition = int(hashlib.sha256(key).hexdigest(), 16) % self.num_partitions\n",
    "        return self.partitions[partition].write(value, key=key)\n",
    "\n",
    "    def write(  # type: ignore\n",
    "        self,\n",
    "        value: bytes,\n",
    "        *,\n",
    "        key: Optional[bytes] = None,\n",
    "        partition: Optional[int] = None,\n",
    "    ) -> RecordMetadata:\n",
    "        if partition is not None:\n",
    "            return self.write_with_partition(value, partition)\n",
    "\n",
    "        if key is not None:\n",
    "            return self.write_with_key(value, key)\n",
    "\n",
    "        partition = random.randint(0, self.num_partitions - 1)  # nosec\n",
    "        return self.write_with_partition(value, partition)\n",
    "\n",
    "    def latest_offset(self, partition: int) -> int:\n",
    "        return self.partitions[partition].latest_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fcfe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = b\"msg\"\n",
    "\n",
    "topic = KafkaTopic(\"test_topic\", 1)\n",
    "\n",
    "expected = RecordMetadata(\n",
    "    topic=\"test_topic\",\n",
    "    partition=0,\n",
    "    topic_partition=TopicPartition(topic=\"test_topic\", partition=0),\n",
    "    offset=0,\n",
    "    timestamp=1680602752070,\n",
    "    timestamp_type=0,\n",
    "    log_start_offset=0,\n",
    ")\n",
    "actual = topic.write(msg)\n",
    "\n",
    "assert expected == actual\n",
    "\n",
    "expected = RecordMetadata(\n",
    "    topic=\"test_topic\",\n",
    "    partition=0,\n",
    "    topic_partition=TopicPartition(topic=\"test_topic\", partition=0),\n",
    "    offset=1,\n",
    "    timestamp=1680602752070,\n",
    "    timestamp_type=0,\n",
    "    log_start_offset=0,\n",
    ")\n",
    "actual = topic.write(msg, key=b\"123\")\n",
    "\n",
    "assert expected == actual, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = \"test_topic\"\n",
    "msgs = [b\"msg\" for _ in range(1000)]\n",
    "partition_num = 10\n",
    "\n",
    "topic = KafkaTopic(topic_name, partition_num)\n",
    "\n",
    "# write to topic\n",
    "for msg in msgs:\n",
    "    topic.write(msg)\n",
    "\n",
    "# For each partition in topic check:\n",
    "for partition in range(partition_num):\n",
    "    topic_partition_expected = TopicPartition(topic=topic_name, partition=partition)\n",
    "    topic_partition_actual, data, _ = topic.read(partition=partition, offset=0)\n",
    "    \n",
    "    # Read returns correct TopicPartition key\n",
    "    assert topic_partition_actual == topic_partition_expected\n",
    "    \n",
    "    # Data is written into partition\n",
    "    assert len(data) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f492339",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = \"test_topic\"\n",
    "msgs = [b\"msg\" for _ in range(1000)]\n",
    "partition_num = 2\n",
    "\n",
    "topic = KafkaTopic(topic_name, partition_num)\n",
    "\n",
    "# write to topic with defined partition\n",
    "for msg in msgs:\n",
    "    topic.write(msg, partition=0)\n",
    "\n",
    "lengths = [\n",
    "    len(\n",
    "        topic.read(partition=i, offset=0)[1]\n",
    "    )\n",
    "    for i in range(partition_num)\n",
    "]\n",
    "\n",
    "assert [1000, 0] == lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff42583",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = \"test_topic\"\n",
    "msgs = [b\"msg\" for _ in range(1000)]\n",
    "partition_num = 3\n",
    "\n",
    "topic = KafkaTopic(topic_name, partition_num)\n",
    "\n",
    "# write to topic with defined key\n",
    "for msg in msgs[:450]:\n",
    "    topic.write(msg, key=b\"some_key\")\n",
    "\n",
    "for msg in msgs[450:]:\n",
    "    topic.write(msg, key=b\"some_key443\")\n",
    "\n",
    "lengths = [\n",
    "    len(\n",
    "        topic.read(partition=i, offset=0)[1]\n",
    "    )\n",
    "    for i in range(partition_num)\n",
    "]\n",
    "\n",
    "assert [0, 450, 550] == sorted(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0280992",
   "metadata": {},
   "source": [
    "## Group metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def split_list(list_to_split: List[Any], split_size: int) -> List[List[Any]]:\n",
    "    return [\n",
    "        list_to_split[start_index : start_index + split_size]\n",
    "        for start_index in range(0, len(list_to_split), split_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert split_list([1, 2, 3, 4, 5], 1) == [[1], [2], [3], [4], [5]]\n",
    "assert split_list([1, 2, 3, 4, 5], 2) == [[1, 2], [3, 4], [5]]\n",
    "assert split_list([1, 2, 3, 4, 5], 3) == [[1, 2, 3], [4, 5]]\n",
    "assert split_list([1, 2, 3, 4, 5], 5) == [[1, 2, 3, 4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d34107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class GroupMetadata:\n",
    "    def __init__(self, num_partitions: int):\n",
    "        self.num_partitions = num_partitions\n",
    "        self.partitions_offsets: Dict[int, int] = {}\n",
    "        self.consumer_ids: List[uuid.UUID] = list()\n",
    "        self.partition_assignments: Dict[uuid.UUID, List[int]] = {}\n",
    "\n",
    "    def subscribe(self, consumer_id: uuid.UUID) -> None:\n",
    "        self.consumer_ids.append(consumer_id)\n",
    "        self.rebalance()\n",
    "\n",
    "    def unsubscribe(self, consumer_id: uuid.UUID) -> None:\n",
    "        self.consumer_ids.remove(consumer_id)\n",
    "        self.rebalance()\n",
    "\n",
    "    def rebalance(self) -> None:\n",
    "        if len(self.consumer_ids) == 0:\n",
    "            self.partition_assignments = {}\n",
    "        else:\n",
    "            partitions_per_actor = self.num_partitions // len(self.consumer_ids)\n",
    "            if self.num_partitions % len(self.consumer_ids) != 0:\n",
    "                partitions_per_actor += 1\n",
    "            self.assign_partitions(partitions_per_actor)\n",
    "\n",
    "    def assign_partitions(self, partitions_per_actor: int) -> None:\n",
    "        partitions = [i for i in range(self.num_partitions)]\n",
    "\n",
    "        partitions_split = split_list(partitions, partitions_per_actor)\n",
    "        self.partition_assignments = {\n",
    "            self.consumer_ids[i]: partition_split\n",
    "            for i, partition_split in enumerate(partitions_split)\n",
    "        }\n",
    "\n",
    "    def get_partitions(\n",
    "        self, consumer_id: uuid.UUID\n",
    "    ) -> Tuple[List[int], Dict[int, Optional[int]]]:\n",
    "        partition_assignments = self.partition_assignments.get(consumer_id, [])\n",
    "        partition_offsets_assignments = {\n",
    "            partition: self.partitions_offsets.get(partition, None)\n",
    "            for partition in partition_assignments\n",
    "        }\n",
    "        return partition_assignments, partition_offsets_assignments\n",
    "\n",
    "    def set_offset(self, partition: int, offset: int) -> None:\n",
    "        self.partitions_offsets[partition] = offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a230027",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_meta = GroupMetadata(num_partitions=3)\n",
    "\n",
    "# subscribe first consumer\n",
    "consumer_id_1 = uuid.uuid4()\n",
    "group_meta.subscribe(consumer_id_1)\n",
    "# check partitions\n",
    "assert group_meta.get_partitions(consumer_id_1)[0] == [0, 1, 2]\n",
    "\n",
    "# subscribe second consumer\n",
    "consumer_id_2 = uuid.uuid4()\n",
    "group_meta.subscribe(consumer_id_2)\n",
    "# check partitions\n",
    "assert group_meta.get_partitions(consumer_id_1)[0] == [0, 1]\n",
    "assert group_meta.get_partitions(consumer_id_2)[0] == [2]\n",
    "\n",
    "# subscribe third consumer\n",
    "consumer_id_3 = uuid.uuid4()\n",
    "group_meta.subscribe(consumer_id_3)\n",
    "# check partitions\n",
    "assert group_meta.get_partitions(consumer_id_1)[0] == [0]\n",
    "assert group_meta.get_partitions(consumer_id_2)[0] == [1]\n",
    "assert group_meta.get_partitions(consumer_id_3)[0] == [2]\n",
    "\n",
    "# subscribe fourth consumer\n",
    "# subscribe third consumer\n",
    "consumer_id_4 = uuid.uuid4()\n",
    "group_meta.subscribe(consumer_id_4)\n",
    "# check partitions\n",
    "assert group_meta.get_partitions(consumer_id_1)[0] == [0]\n",
    "assert group_meta.get_partitions(consumer_id_2)[0] == [1]\n",
    "assert group_meta.get_partitions(consumer_id_3)[0] == [2]\n",
    "assert group_meta.get_partitions(consumer_id_4)[0] == [] # fourth consumer is starving\n",
    "\n",
    "# Unsubscribe one consumer\n",
    "group_meta.unsubscribe(consumer_id_3)\n",
    "# check partitions\n",
    "assert group_meta.get_partitions(consumer_id_1)[0] == [0]\n",
    "assert group_meta.get_partitions(consumer_id_2)[0] == [1]\n",
    "assert group_meta.get_partitions(consumer_id_4)[0] == [2], group_meta.get_partitions(consumer_id_4)\n",
    "\n",
    "# Unsubscribe all but one consumer\n",
    "group_meta.unsubscribe(consumer_id_1)\n",
    "group_meta.unsubscribe(consumer_id_4)\n",
    "assert group_meta.get_partitions(consumer_id_2)[0] == [0, 1, 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25de69",
   "metadata": {},
   "source": [
    "## Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e96510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@classcontextmanager()\n",
    "class InMemoryBroker:\n",
    "    def __init__(self, topics: Iterable[str], num_partitions: int = 1):\n",
    "        self.num_partitions = num_partitions\n",
    "        self.topics: Dict[str, KafkaTopic] = {\n",
    "            topic: KafkaTopic(topic, num_partitions) for topic in topics\n",
    "        }\n",
    "        self.topic_groups: Dict[Tuple[str, str], GroupMetadata] = {}\n",
    "        self.is_started: bool = False\n",
    "\n",
    "    def connect(self) -> uuid.UUID:\n",
    "        return uuid.uuid4()\n",
    "\n",
    "    def dissconnect(self, consumer_id: uuid.UUID) -> None:\n",
    "        pass\n",
    "\n",
    "    def subscribe(self, topic: str, group: str, consumer_id: uuid.UUID) -> None:\n",
    "        group_meta = self.topic_groups.get(\n",
    "            (topic, group), GroupMetadata(self.num_partitions)\n",
    "        )\n",
    "        group_meta.subscribe(consumer_id)\n",
    "        self.topic_groups[(topic, group)] = group_meta\n",
    "\n",
    "    def unsubscribe(self, topic: str, group: str, consumer_id: uuid.UUID) -> None:\n",
    "        self.topic_groups[(topic, group)].unsubscribe(consumer_id)\n",
    "\n",
    "    def read(  # type: ignore\n",
    "        self, *, topic: str, group: str, consumer_id: uuid.UUID, auto_offset_reset: str\n",
    "    ) -> Dict[TopicPartition, List[KafkaRecord]]:\n",
    "        group_meta = self.topic_groups[(topic, group)]\n",
    "        partitions, offsets = group_meta.get_partitions(consumer_id)\n",
    "\n",
    "        if len(partitions) == 0:\n",
    "            return {}\n",
    "\n",
    "        partitions_data = {}\n",
    "\n",
    "        for partition in partitions:\n",
    "            offset = offsets[partition]\n",
    "\n",
    "            if offset is None:\n",
    "                offset = (\n",
    "                    self.topics[topic].latest_offset(partition)\n",
    "                    if auto_offset_reset == \"latest\"\n",
    "                    else 0\n",
    "                )\n",
    "\n",
    "            topic_partition, data, offset = self.topics[topic].read(partition, offset)\n",
    "\n",
    "            partitions_data[topic_partition] = data\n",
    "            group_meta.set_offset(partition, offset)\n",
    "\n",
    "        return partitions_data\n",
    "\n",
    "    def write(  # type: ignore\n",
    "        self,\n",
    "        *,\n",
    "        topic: str,\n",
    "        value: bytes,\n",
    "        key: Optional[bytes] = None,\n",
    "        partition: Optional[int] = None,\n",
    "    ) -> RecordMetadata:\n",
    "        if topic in self.topics:\n",
    "            return self.topics[topic].write(value, key=key, partition=partition)\n",
    "        else:\n",
    "            # todo: log only once\n",
    "            logger.warning(\n",
    "                f\"Topic {topic} is not available during auto-create initialization\"\n",
    "            )\n",
    "\n",
    "    @contextmanager\n",
    "    def lifecycle(self) -> Iterator[\"InMemoryBroker\"]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    async def _start(self) -> str:\n",
    "        logger.info(\"InMemoryBroker._start() called\")\n",
    "        self.__enter__()  # type: ignore\n",
    "        return \"localbroker:0\"\n",
    "\n",
    "    async def _stop(self) -> None:\n",
    "        logger.info(\"InMemoryBroker._stop() called\")\n",
    "        self.__exit__(None, None, None)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2edf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check subscribing and reading from empty partitions for same group\n",
    "\n",
    "broker = InMemoryBroker(topics=[\"topic1\"], num_partitions=3)\n",
    "\n",
    "consumer_group = \"my_group\"\n",
    "\n",
    "consumer_id_1 = broker.connect()\n",
    "broker.subscribe(\"topic1\", consumer_group, consumer_id_1)\n",
    "\n",
    "assert broker.read(\n",
    "    topic=\"topic1\",\n",
    "    group=consumer_group,\n",
    "    consumer_id=consumer_id_1,\n",
    "    auto_offset_reset=\"latest\",\n",
    ") == {\n",
    "    TopicPartition(topic=\"topic1\", partition=0): [],\n",
    "    TopicPartition(topic=\"topic1\", partition=1): [],\n",
    "    TopicPartition(topic=\"topic1\", partition=2): [],\n",
    "}\n",
    "\n",
    "consumer_id_2 = broker.connect()\n",
    "broker.subscribe(\"topic1\", consumer_group, consumer_id_2)\n",
    "\n",
    "assert broker.read(\n",
    "    topic=\"topic1\",\n",
    "    group=consumer_group,\n",
    "    consumer_id=consumer_id_1,\n",
    "    auto_offset_reset=\"latest\",\n",
    ") == {\n",
    "    TopicPartition(topic=\"topic1\", partition=0): [],\n",
    "    TopicPartition(topic=\"topic1\", partition=1): [],\n",
    "}\n",
    "\n",
    "assert broker.read(\n",
    "    topic=\"topic1\",\n",
    "    group=consumer_group,\n",
    "    consumer_id=consumer_id_2,\n",
    "    auto_offset_reset=\"latest\",\n",
    ") == {\n",
    "    TopicPartition(topic=\"topic1\", partition=2): [],\n",
    "}\n",
    "\n",
    "broker.unsubscribe(\"topic1\", consumer_group, consumer_id_1)\n",
    "assert broker.read(\n",
    "    topic=\"topic1\",\n",
    "    group=consumer_group,\n",
    "    consumer_id=consumer_id_2,\n",
    "    auto_offset_reset=\"latest\",\n",
    ") == {\n",
    "    TopicPartition(topic=\"topic1\", partition=0): [],\n",
    "    TopicPartition(topic=\"topic1\", partition=1): [],\n",
    "    TopicPartition(topic=\"topic1\", partition=2): [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30de58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check writing to partitions\n",
    "\n",
    "topic_name = \"topic1\"\n",
    "\n",
    "broker = InMemoryBroker(topics=[topic_name], num_partitions=1)\n",
    "\n",
    "consumer_group = \"my_group\"\n",
    "\n",
    "consumer_id_1 = broker.connect()\n",
    "broker.subscribe(topic_name, consumer_group, consumer_id_1)\n",
    "\n",
    "record_meta = broker.write(topic=topic_name, value=b\"msg\")\n",
    "\n",
    "assert record_meta == RecordMetadata(\n",
    "    topic=topic_name,\n",
    "    partition=0,\n",
    "    topic_partition=TopicPartition(topic=topic_name, partition=0),\n",
    "    offset=0,\n",
    "    timestamp=1680602752070,\n",
    "    timestamp_type=0,\n",
    "    log_start_offset=0,\n",
    ")\n",
    "\n",
    "assert broker.read(\n",
    "    topic=topic_name,\n",
    "    consumer_id=consumer_id_1,\n",
    "    group=consumer_group,\n",
    "    auto_offset_reset=\"earliest\",\n",
    ") == {\n",
    "    TopicPartition(topic=\"topic1\", partition=0): [\n",
    "        KafkaRecord(topic=\"topic1\", partition=0, key=None, value=b\"msg\", offset=0)\n",
    "    ]\n",
    "}\n",
    "\n",
    "broker.write(topic=topic_name, value=b\"msg\")\n",
    "\n",
    "consumer_group_new = \"another_group\"\n",
    "\n",
    "consumer_id_2 = broker.connect()\n",
    "broker.subscribe(topic_name, consumer_group_new, consumer_id_2)\n",
    "\n",
    "assert broker.read(\n",
    "    topic=topic_name,\n",
    "    consumer_id=consumer_id_2,\n",
    "    group=consumer_group_new,\n",
    "    auto_offset_reset=\"latest\",\n",
    ") == {TopicPartition(topic=\"topic1\", partition=0): []}\n",
    "\n",
    "assert broker.read(\n",
    "    topic=topic_name,\n",
    "    consumer_id=consumer_id_1,\n",
    "    group=consumer_group,\n",
    "    auto_offset_reset=\"latest\",\n",
    ") == {\n",
    "    TopicPartition(topic=\"topic1\", partition=0): [\n",
    "        KafkaRecord(topic=\"topic1\", partition=0, key=None, value=b\"msg\", offset=1)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59295ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] __main__: Topic not_my_topic is not available during auto-create initialization\n"
     ]
    }
   ],
   "source": [
    "in_memory_broker = InMemoryBroker([\"my_topic\"])\n",
    "group = \"my_group\"\n",
    "\n",
    "consumer_id = in_memory_broker.connect()\n",
    "\n",
    "with pytest.raises(KeyError) as e:\n",
    "    in_memory_broker.read(topic=\"my_topic\", group=group, consumer_id=consumer_id, auto_offset_reset=\"latest\")\n",
    "\n",
    "in_memory_broker.subscribe(topic=\"my_topic\", group=group, consumer_id=consumer_id)\n",
    "\n",
    "msg = in_memory_broker.read(topic=\"my_topic\", group=group, consumer_id=consumer_id, auto_offset_reset=\"earliest\")\n",
    "assert msg == {TopicPartition(topic='my_topic', partition=0): []}, msg\n",
    "\n",
    "in_memory_broker.write(topic=\"not_my_topic\", value=b\"not my message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88761db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): entering...\n",
      "[WARNING] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): (<_UnixSelectorEventLoop running=True closed=False debug=False>) is already running!\n",
      "[WARNING] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): calling nest_asyncio.apply()\n",
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: <class 'fastkafka.testing.ApacheKafkaBroker'>.start(): returning 127.0.0.1:9092\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): exited.\n",
      "[WARNING] aiokafka.cluster: Topic not_my_topic is not available during auto-create initialization\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.stop(): entering...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 891510...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 891510 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 891131...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 891131 terminated.\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.stop(): exited.\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "with ApacheKafkaBroker([\"my_topic\"], apply_nest_asyncio=True) as bootstrap_servers:\n",
    "    producer = AIOKafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    await producer.start()\n",
    "    for _ in range(1000):\n",
    "        record = await producer.send(topic=\"not_my_topic\", value=b\"not my message\")\n",
    "    await producer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dcfab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): entering...\n",
      "[WARNING] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): (<_UnixSelectorEventLoop running=True closed=False debug=False>) is already running!\n",
      "[WARNING] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): calling nest_asyncio.apply()\n",
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: <class 'fastkafka.testing.ApacheKafkaBroker'>.start(): returning 127.0.0.1:9092\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): exited.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic'})\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic': 1}. \n",
      "getmany()...\n",
      "exiting...\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.stop(): entering...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 892863...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 892863 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 892484...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 892484 terminated.\n",
      "[INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.stop(): exited.\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "with ApacheKafkaBroker([\"my_topic\"], apply_nest_asyncio=True) as bootstrap_servers:\n",
    "    consumer = AIOKafkaConsumer(\"my_topic\", bootstrap_servers=bootstrap_servers)\n",
    "    await consumer.start()\n",
    "    print(\"getmany()...\")\n",
    "    msg = await consumer.getmany(timeout_ms=0)\n",
    "    print(\"exiting...\")\n",
    "    await consumer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5b3a9",
   "metadata": {},
   "source": [
    "## Consumer patching\n",
    "\n",
    "We need to patch AIOKafkaConsumer methods so that we can redirect the consumer to our local kafka broker.\n",
    "\n",
    "Patched methods:\n",
    "\n",
    "- [x] \\_\\_init\\_\\_\n",
    "- [x] start\n",
    "- [x] subscribe\n",
    "- [x] stop\n",
    "- [x] getmany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ca42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# InMemoryConsumer\n",
    "class InMemoryConsumer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        broker: InMemoryBroker,\n",
    "    ) -> None:\n",
    "        self.broker = broker\n",
    "        self._id: Optional[uuid.UUID] = None\n",
    "        self._auto_offset_reset: str = \"latest\"\n",
    "        self._group_id: Optional[str] = None\n",
    "        self._topics: List[str] = list()\n",
    "\n",
    "    @delegates(AIOKafkaConsumer)\n",
    "    def __call__(self, **kwargs: Any) -> \"InMemoryConsumer\":\n",
    "        defaults = _get_default_kwargs_from_sig(InMemoryConsumer.__call__, **kwargs)\n",
    "        consume_copy = InMemoryConsumer(self.broker)\n",
    "        consume_copy._auto_offset_reset = defaults[\"auto_offset_reset\"]\n",
    "        consume_copy._group_id = (\n",
    "            defaults[\"group_id\"]\n",
    "            if defaults[\"group_id\"] is not None\n",
    "            else \"\".join(random.choices(string.ascii_letters, k=10)) # nosec\n",
    "        )\n",
    "        return consume_copy\n",
    "\n",
    "    @delegates(AIOKafkaConsumer.start)\n",
    "    async def start(self, **kwargs: Any) -> None:\n",
    "        pass\n",
    "\n",
    "    @delegates(AIOKafkaConsumer.stop)\n",
    "    async def stop(self, **kwargs: Any) -> None:\n",
    "        pass\n",
    "\n",
    "    @delegates(AIOKafkaConsumer.subscribe)\n",
    "    def subscribe(self, topics: List[str], **kwargs: Any) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @delegates(AIOKafkaConsumer.getmany)\n",
    "    async def getmany(  # type: ignore\n",
    "        self, **kwargs: Any\n",
    "    ) -> Dict[TopicPartition, List[ConsumerRecord]]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677208e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] asyncio: Unclosed AIOKafkaConsumer\n",
      "consumer: <aiokafka.consumer.consumer.AIOKafkaConsumer object>\n",
      "[ERROR] asyncio: Unclosed AIOKafkaConsumer\n",
      "consumer: <aiokafka.consumer.consumer.AIOKafkaConsumer object>\n"
     ]
    }
   ],
   "source": [
    "broker = InMemoryBroker([\"topic\"])\n",
    "\n",
    "ConsumerClass = InMemoryConsumer(broker)\n",
    "\n",
    "for cls in [ConsumerClass, AIOKafkaConsumer]:\n",
    "    consumer = cls()\n",
    "    assert consumer._auto_offset_reset == \"latest\"\n",
    "\n",
    "    consumer = cls(auto_offset_reset=\"earliest\")\n",
    "    assert consumer._auto_offset_reset == \"earliest\", consumer._auto_offset_reset\n",
    "\n",
    "    consumer = cls(auto_offset_reset=\"whatever\")\n",
    "    assert consumer._auto_offset_reset == \"whatever\"\n",
    "    \n",
    "    await consumer.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e4df06",
   "metadata": {},
   "source": [
    "Patching start so that we don't try to start the real AIOKafkaConsumer instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "@delegates(AIOKafkaConsumer.start)\n",
    "async def start(self: InMemoryConsumer, **kwargs: Any) -> None:\n",
    "    logger.info(\"AIOKafkaConsumer patched start() called()\")\n",
    "    if self._id is not None:\n",
    "        raise RuntimeError(\n",
    "            \"Consumer start() already called! Run consumer stop() before running start() again\"\n",
    "        )\n",
    "    self._id = self.broker.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072dd1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: AIOKafkaConsumer patched start() called()\n"
     ]
    }
   ],
   "source": [
    "broker = InMemoryBroker([\"my_topic\"])\n",
    "\n",
    "ConsumerClass = InMemoryConsumer(broker)\n",
    "\n",
    "for cls in [ConsumerClass]:\n",
    "\n",
    "    consumer = cls()\n",
    "    await consumer.start()\n",
    "    await consumer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c11cd3",
   "metadata": {},
   "source": [
    "Patching subscribe so that we can connect to our Local, in-memory, Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de72311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "@delegates(AIOKafkaConsumer.subscribe)\n",
    "def subscribe(self: InMemoryConsumer, topics: List[str], **kwargs: Any) -> None:\n",
    "    logger.info(\"AIOKafkaConsumer patched subscribe() called\")\n",
    "    if self._id is None:\n",
    "        raise RuntimeError(\"Consumer start() not called! Run consumer start() first\")\n",
    "    logger.info(f\"AIOKafkaConsumer.subscribe(), subscribing to: {topics}\")\n",
    "    for topic in topics:\n",
    "        self.broker.subscribe(consumer_id=self._id, topic=topic, group=self._group_id)  # type: ignore\n",
    "        self._topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e08d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: AIOKafkaConsumer patched start() called()\n",
      "[INFO] __main__: AIOKafkaConsumer patched subscribe() called\n",
      "[INFO] __main__: AIOKafkaConsumer.subscribe(), subscribing to: ['my_topic']\n"
     ]
    }
   ],
   "source": [
    "broker = InMemoryBroker(topics=[\"my_topic\"])\n",
    "\n",
    "ConsumerClass = InMemoryConsumer(broker)\n",
    "consumer = ConsumerClass()\n",
    "\n",
    "await consumer.start()\n",
    "consumer.subscribe([\"my_topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80a4d5",
   "metadata": {},
   "source": [
    "Patching stop so that be dont break anything by calling the real AIOKafkaConsumer stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@patch\n",
    "@delegates(AIOKafkaConsumer.stop)\n",
    "async def stop(self: InMemoryConsumer, **kwargs: Any) -> None:\n",
    "    logger.info(\"AIOKafkaConsumer patched stop() called\")\n",
    "    if self._id is None:\n",
    "        raise RuntimeError(\n",
    "            \"Consumer start() not called! Run consumer start() first\"\n",
    "        )\n",
    "    for topic in self._topics:\n",
    "        self.broker.unsubscribe(topic=topic, group=self._group_id, consumer_id=self._id)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc667214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: AIOKafkaConsumer patched start() called()\n",
      "[INFO] __main__: AIOKafkaConsumer patched subscribe() called\n",
      "[INFO] __main__: AIOKafkaConsumer.subscribe(), subscribing to: ['my_topic']\n",
      "[INFO] __main__: AIOKafkaConsumer patched stop() called\n"
     ]
    }
   ],
   "source": [
    "broker = InMemoryBroker(topics=[\"my_topic\"])\n",
    "\n",
    "ConsumerClass = InMemoryConsumer(broker)\n",
    "consumer = ConsumerClass()\n",
    "\n",
    "await consumer.start()\n",
    "consumer.subscribe([\"my_topic\"])\n",
    "await consumer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c733b4",
   "metadata": {},
   "source": [
    "Patching getmany so that the messages are pulled from our Local, in-memory, Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "@delegates(AIOKafkaConsumer.getmany)\n",
    "async def getmany(  # type: ignore\n",
    "    self: InMemoryConsumer, **kwargs: Any\n",
    ") -> Dict[TopicPartition, List[ConsumerRecord]]:\n",
    "    for topic in self._topics:\n",
    "        return self.broker.read(\n",
    "            topic=topic,\n",
    "            consumer_id=self._id,  # type: ignore\n",
    "            group=self._group_id,  # type: ignore\n",
    "            auto_offset_reset=self._auto_offset_reset,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4940bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: AIOKafkaConsumer patched start() called()\n",
      "[INFO] __main__: AIOKafkaConsumer patched subscribe() called\n",
      "[INFO] __main__: AIOKafkaConsumer.subscribe(), subscribing to: ['my_topic']\n",
      "[INFO] __main__: AIOKafkaConsumer patched stop() called\n"
     ]
    }
   ],
   "source": [
    "broker = InMemoryBroker(topics=[\"my_topic\"])\n",
    "\n",
    "ConsumerClass = InMemoryConsumer(broker)\n",
    "consumer = ConsumerClass(auto_offset_reset=\"latest\")\n",
    "\n",
    "await consumer.start()\n",
    "\n",
    "consumer.subscribe([\"my_topic\"])\n",
    "await consumer.getmany()\n",
    "\n",
    "await consumer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723468f0",
   "metadata": {},
   "source": [
    "## Producer patching\n",
    "\n",
    "We need to patch AIOKafkaProducer methods so that we can redirect the producer to our local kafka broker\n",
    "\n",
    "- [x] \\_\\_init\\_\\_\n",
    "- [x] start\n",
    "- [x] stop\n",
    "- [x] send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class InMemoryProducer:\n",
    "    def __init__(self, broker: InMemoryBroker, **kwargs: Any) -> None:\n",
    "        self.broker = broker\n",
    "        self.id: Optional[uuid.UUID] = None\n",
    "\n",
    "    @delegates(AIOKafkaProducer)\n",
    "    def __call__(self, **kwargs: Any) -> \"InMemoryProducer\":\n",
    "        return InMemoryProducer(self.broker)\n",
    "\n",
    "    @delegates(AIOKafkaProducer.start)\n",
    "    async def start(self, **kwargs: Any) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @delegates(AIOKafkaProducer.stop)\n",
    "    async def stop(self, **kwargs: Any) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @delegates(AIOKafkaProducer.send)\n",
    "    async def send(  # type: ignore\n",
    "        self: AIOKafkaProducer,\n",
    "        topic: str,\n",
    "        msg: bytes,\n",
    "        key: Optional[bytes] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf322a",
   "metadata": {},
   "source": [
    "Patching AIOKafkaProducer start so that we mock the startup procedure of AIOKafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ac5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch # type: ignore\n",
    "@delegates(AIOKafkaProducer.start)\n",
    "async def start(self: InMemoryProducer, **kwargs: Any) -> None:\n",
    "    logger.info(\"AIOKafkaProducer patched start() called()\")\n",
    "    if self.id is not None:\n",
    "        raise RuntimeError(\n",
    "            \"Producer start() already called! Run producer stop() before running start() again\"\n",
    "        )\n",
    "    self.id = self.broker.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250c614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: AIOKafkaProducer patched start() called()\n"
     ]
    }
   ],
   "source": [
    "broker = InMemoryBroker(topics=[\"my_topic\"])\n",
    "\n",
    "ProducerClass = InMemoryProducer(broker)\n",
    "producer = ProducerClass()\n",
    "\n",
    "await producer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e9be9",
   "metadata": {},
   "source": [
    "Patching AIOKafkaProducerStop so that we don't uniintentionally try to stop a real instance of AIOKafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32412969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch # type: ignore\n",
    "@delegates(AIOKafkaProducer.stop)\n",
    "async def stop(self: InMemoryProducer, **kwargs: Any) -> None:\n",
    "    logger.info(\"AIOKafkaProducer patched stop() called\")\n",
    "    if self.id is None:\n",
    "        raise RuntimeError(\n",
    "            \"Producer start() not called! Run producer start() first\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a1fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: AIOKafkaProducer patched start() called()\n",
      "[INFO] __main__: AIOKafkaProducer patched stop() called\n"
     ]
    }
   ],
   "source": [
    "broker = InMemoryBroker(topics=[\"my_topic\"])\n",
    "\n",
    "ProducerClass = InMemoryProducer(broker)\n",
    "producer = ProducerClass()\n",
    "\n",
    "await producer.start()\n",
    "await producer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c77a56",
   "metadata": {},
   "source": [
    "Patching AIOKafkaProducer send so that we redirect sent messages to Local, in-memory, Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "@delegates(AIOKafkaProducer.send)\n",
    "async def send( # type: ignore\n",
    "    self: InMemoryProducer,\n",
    "    topic: str,\n",
    "    msg: bytes,\n",
    "    key: Optional[bytes] = None,\n",
    "    partition: Optional[int] = None,\n",
    "    **kwargs: Any,\n",
    "): #asyncio.Task[RecordMetadata]\n",
    "    if self.id is None:\n",
    "        raise RuntimeError(\n",
    "            \"Producer start() not called! Run producer start() first\"\n",
    "        )\n",
    "    record = self.broker.write(topic=topic, value=msg, key=key, partition=partition)\n",
    "\n",
    "    async def _f(record: ConsumerRecord = record) -> RecordMetadata: # type: ignore\n",
    "        return record\n",
    "\n",
    "    return asyncio.create_task(_f())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda1d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: AIOKafkaProducer patched start() called()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RecordMetadata(topic='my_topic', partition=0, topic_partition=TopicPartition(topic='my_topic', partition=0), offset=0, timestamp=1680602752070, timestamp_type=0, log_start_offset=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broker = InMemoryBroker(topics=[\"my_topic\"])\n",
    "\n",
    "ProducerClass = InMemoryProducer(broker)\n",
    "producer = ProducerClass()\n",
    "\n",
    "await producer.start()\n",
    "msg_fut = await producer.send(\"my_topic\", b\"some_msg\")\n",
    "await msg_fut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4d12b",
   "metadata": {},
   "source": [
    "## Add patching to InMemoryBroker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "@contextmanager\n",
    "def lifecycle(self: InMemoryBroker) -> Iterator[InMemoryBroker]:\n",
    "    logger.info(\n",
    "        \"InMemoryBroker._patch_consumers_and_producers(): Patching consumers and producers!\"\n",
    "    )\n",
    "    try:\n",
    "        logger.info(\"InMemoryBroker starting\")\n",
    "        \n",
    "        old_consumer_app = fastkafka._application.app.AIOKafkaConsumer\n",
    "        old_producer_app = fastkafka._application.app.AIOKafkaProducer\n",
    "        old_consumer_loop = fastkafka._components.aiokafka_consumer_loop.AIOKafkaConsumer\n",
    "        old_producer_manager = fastkafka._components.aiokafka_producer_manager.AIOKafkaProducer\n",
    "        \n",
    "        fastkafka._application.app.AIOKafkaConsumer = InMemoryConsumer(self)\n",
    "        fastkafka._application.app.AIOKafkaProducer = InMemoryProducer(self)\n",
    "        fastkafka._components.aiokafka_consumer_loop.AIOKafkaConsumer = InMemoryConsumer(self)\n",
    "        fastkafka._components.aiokafka_producer_manager.AIOKafkaProducer = InMemoryProducer(self)\n",
    "        \n",
    "        self.is_started = True\n",
    "        yield self\n",
    "    finally:\n",
    "        logger.info(\"InMemoryBroker stopping\")\n",
    "        \n",
    "        fastkafka._application.app.AIOKafkaConsumer = old_consumer_app\n",
    "        fastkafka._application.app.AIOKafkaProducer = old_producer_app\n",
    "        fastkafka._components.aiokafka_consumer_loop.AIOKafkaConsumer = old_consumer_loop\n",
    "        fastkafka._components.aiokafka_producer_manager.AIOKafkaProducer = old_producer_manager\n",
    "        \n",
    "        self.is_started = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55ccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: InMemoryBroker._patch_consumers_and_producers(): Patching consumers and producers!\n",
      "[INFO] __main__: InMemoryBroker starting\n",
      "[INFO] __main__: InMemoryBroker stopping\n"
     ]
    }
   ],
   "source": [
    "assert fastkafka._application.app.AIOKafkaConsumer == AIOKafkaConsumer\n",
    "assert fastkafka._application.app.AIOKafkaProducer == AIOKafkaProducer\n",
    "\n",
    "with InMemoryBroker([\"topic\"]) as broker:\n",
    "    assert isinstance(fastkafka._application.app.AIOKafkaConsumer, InMemoryConsumer)\n",
    "    assert isinstance(fastkafka._application.app.AIOKafkaProducer, InMemoryProducer)\n",
    "    assert fastkafka._application.app.AIOKafkaConsumer().broker == broker\n",
    "    assert fastkafka._application.app.AIOKafkaProducer().broker == broker\n",
    "    \n",
    "assert fastkafka._application.app.AIOKafkaConsumer == AIOKafkaConsumer\n",
    "assert fastkafka._application.app.AIOKafkaProducer == AIOKafkaProducer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335aea0",
   "metadata": {},
   "source": [
    "## Broker, consumer and producer integration tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4275bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@asynccontextmanager\n",
    "async def create_consumer_and_producer(\n",
    "    auto_offset_reset: str = \"latest\",\n",
    ") -> AsyncIterator[Tuple[AIOKafkaConsumer, AIOKafkaProducer]]:\n",
    "    consumer = fastkafka._application.app.AIOKafkaConsumer(auto_offset_reset=auto_offset_reset)\n",
    "    producer = fastkafka._application.app.AIOKafkaProducer()\n",
    "\n",
    "    await consumer.start()\n",
    "    await producer.start()\n",
    "\n",
    "    yield (consumer, producer)\n",
    "\n",
    "    await consumer.stop()\n",
    "    await producer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7688d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkEqual(L1, L2):\n",
    "    return len(L1) == len(L2) and sorted(L1) == sorted(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert checkEqual([1, 2], [3]) == False\n",
    "assert checkEqual([1, 2, 3], [3, 2, 1]) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc21a6e",
   "metadata": {},
   "source": [
    "Sanity check, let's see if the messages are sent to broker and received by the consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90249e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: InMemoryBroker._patch_consumers_and_producers(): Patching consumers and producers!\n",
      "[INFO] __main__: InMemoryBroker starting\n",
      "[INFO] __main__: AIOKafkaConsumer patched start() called()\n",
      "[INFO] __main__: AIOKafkaProducer patched start() called()\n",
      "[INFO] __main__: AIOKafkaConsumer patched subscribe() called\n",
      "[INFO] __main__: AIOKafkaConsumer.subscribe(), subscribing to: ['test_topic']\n",
      "[INFO] __main__: AIOKafkaConsumer patched stop() called\n",
      "[INFO] __main__: AIOKafkaProducer patched stop() called\n",
      "[INFO] __main__: InMemoryBroker stopping\n"
     ]
    }
   ],
   "source": [
    "topic = \"test_topic\"\n",
    "sent_msgs = [f\"msg{i}\".encode(\"UTF-8\") for i in range(320)]\n",
    "\n",
    "with InMemoryBroker([topic]) as broker:\n",
    "    async with create_consumer_and_producer(auto_offset_reset=\"earliest\") as (\n",
    "        consumer,\n",
    "        producer,\n",
    "    ):\n",
    "        [await producer.send(topic, msg) for msg in sent_msgs]\n",
    "        consumer.subscribe([topic])\n",
    "        received = await consumer.getmany()\n",
    "        received_msgs = [msg.value for _, msgs in received.items() for msg in msgs]\n",
    "    assert checkEqual(\n",
    "        received_msgs, sent_msgs\n",
    "    ), f\"{sent_msgs=}\\n{received_msgs=}\\n{data=}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40fa9ed",
   "metadata": {},
   "source": [
    "Check if only subscribed topic messages are received by the consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a6755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: InMemoryBroker._patch_consumers_and_producers(): Patching consumers and producers!\n",
      "[INFO] __main__: InMemoryBroker starting\n",
      "[INFO] __main__: AIOKafkaConsumer patched start() called()\n",
      "[INFO] __main__: AIOKafkaProducer patched start() called()\n",
      "[INFO] __main__: AIOKafkaConsumer patched subscribe() called\n",
      "[INFO] __main__: AIOKafkaConsumer.subscribe(), subscribing to: ['test_topic1']\n",
      "[INFO] __main__: AIOKafkaConsumer patched stop() called\n",
      "[INFO] __main__: AIOKafkaProducer patched stop() called\n",
      "[INFO] __main__: InMemoryBroker stopping\n"
     ]
    }
   ],
   "source": [
    "topic1 = \"test_topic1\"\n",
    "topic2 = \"test_topic2\"\n",
    "sent_msgs_1 = [(f\"msg{i}\" + topic1).encode(\"UTF-8\") for i in range(32)]\n",
    "sent_msgs_2 = [(f\"msg{i}\" + topic2).encode(\"UTF-8\") for i in range(32)]\n",
    "\n",
    "with InMemoryBroker([topic1, topic2]) as broker:\n",
    "    async with create_consumer_and_producer(auto_offset_reset=\"earliest\") as (\n",
    "        consumer,\n",
    "        producer,\n",
    "    ):\n",
    "        [await producer.send(topic1, msg) for msg in sent_msgs_1]\n",
    "        [await producer.send(topic2, msg) for msg in sent_msgs_2]\n",
    "\n",
    "        consumer.subscribe([topic1])\n",
    "        received = await consumer.getmany()\n",
    "        received_msgs = [msg.value for _, msgs in received.items() for msg in msgs]\n",
    "\n",
    "    assert checkEqual(sent_msgs_1, received_msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1c5c5",
   "metadata": {},
   "source": [
    "Check if msgs are received only after subscribing when auto_offset_reset is set to \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6bba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: InMemoryBroker._patch_consumers_and_producers(): Patching consumers and producers!\n",
      "[INFO] __main__: InMemoryBroker starting\n",
      "[INFO] __main__: AIOKafkaConsumer patched start() called()\n",
      "[INFO] __main__: AIOKafkaProducer patched start() called()\n",
      "[INFO] __main__: AIOKafkaConsumer patched subscribe() called\n",
      "[INFO] __main__: AIOKafkaConsumer.subscribe(), subscribing to: ['test_topic']\n",
      "[INFO] __main__: AIOKafkaConsumer patched stop() called\n",
      "[INFO] __main__: AIOKafkaProducer patched stop() called\n",
      "[INFO] __main__: InMemoryBroker stopping\n"
     ]
    }
   ],
   "source": [
    "topic = \"test_topic\"\n",
    "sent_msgs_before = [f\"msg{i}\".encode(\"UTF-8\") for i in range(32)]\n",
    "sent_msgs_after = [f\"msg{i}\".encode(\"UTF-8\") for i in range(32, 64)]\n",
    "\n",
    "with InMemoryBroker([topic]) as broker:\n",
    "    async with create_consumer_and_producer() as (consumer, producer):\n",
    "        [await producer.send(topic, msg) for msg in sent_msgs_before]\n",
    "\n",
    "        consumer.subscribe([topic])\n",
    "        received = await consumer.getmany()\n",
    "        [await producer.send(topic, msg) for msg in sent_msgs_after]\n",
    "        received = await consumer.getmany()\n",
    "        received_msgs = [msg.value for _, msgs in received.items() for msg in msgs]\n",
    "\n",
    "    assert checkEqual(sent_msgs_after, received_msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9f43a",
   "metadata": {},
   "source": [
    "Check two consumers different groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9c2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: InMemoryBroker._patch_consumers_and_producers(): Patching consumers and producers!\n",
      "[INFO] __main__: InMemoryBroker starting\n",
      "[INFO] __main__: AIOKafkaConsumer patched start() called()\n",
      "[INFO] __main__: AIOKafkaConsumer patched start() called()\n",
      "[INFO] __main__: AIOKafkaProducer patched start() called()\n",
      "[INFO] __main__: AIOKafkaConsumer patched subscribe() called\n",
      "[INFO] __main__: AIOKafkaConsumer.subscribe(), subscribing to: ['test_topic']\n",
      "[INFO] __main__: AIOKafkaConsumer patched subscribe() called\n",
      "[INFO] __main__: AIOKafkaConsumer.subscribe(), subscribing to: ['test_topic']\n",
      "[b'msg0', b'msg1', b'msg2', b'msg3', b'msg4', b'msg5', b'msg6', b'msg7', b'msg8', b'msg9', b'msg10', b'msg11', b'msg12', b'msg13', b'msg14', b'msg15', b'msg16', b'msg17', b'msg18', b'msg19', b'msg20', b'msg21', b'msg22', b'msg23', b'msg24', b'msg25', b'msg26', b'msg27', b'msg28', b'msg29', b'msg30', b'msg31']\n",
      "[b'msg0', b'msg1', b'msg2', b'msg3', b'msg4', b'msg5', b'msg6', b'msg7', b'msg8', b'msg9', b'msg10', b'msg11', b'msg12', b'msg13', b'msg14', b'msg15', b'msg16', b'msg17', b'msg18', b'msg19', b'msg20', b'msg21', b'msg22', b'msg23', b'msg24', b'msg25', b'msg26', b'msg27', b'msg28', b'msg29', b'msg30', b'msg31']\n",
      "[INFO] __main__: AIOKafkaConsumer patched stop() called\n",
      "[INFO] __main__: AIOKafkaConsumer patched stop() called\n",
      "[INFO] __main__: AIOKafkaProducer patched stop() called\n",
      "[INFO] __main__: InMemoryBroker stopping\n"
     ]
    }
   ],
   "source": [
    "topic = \"test_topic\"\n",
    "sent_msgs = [f\"msg{i}\".encode(\"UTF-8\") for i in range(32)]\n",
    "\n",
    "with InMemoryBroker([topic]) as broker:\n",
    "    consumer1 = fastkafka._application.app.AIOKafkaConsumer(auto_offset_reset=\"earliest\")\n",
    "    consumer2 = fastkafka._application.app.AIOKafkaConsumer(auto_offset_reset=\"earliest\")\n",
    "    producer = fastkafka._application.app.AIOKafkaProducer()\n",
    "\n",
    "    await consumer1.start()\n",
    "    await consumer2.start()\n",
    "    await producer.start()\n",
    "\n",
    "    [await producer.send(topic, msg) for msg in sent_msgs]\n",
    "\n",
    "    consumer1.subscribe([topic])\n",
    "    received1 = await consumer1.getmany()\n",
    "    \n",
    "    consumer2.subscribe([topic])\n",
    "    received2 = await consumer2.getmany()\n",
    "\n",
    "    received_msgs1 = [msg.value for _, msgs in received1.items() for msg in msgs]\n",
    "    received_msgs2 = [msg.value for _, msgs in received2.items() for msg in msgs]\n",
    "\n",
    "    await consumer1.stop()\n",
    "    await consumer2.stop()\n",
    "    await producer.stop()\n",
    "    \n",
    "    assert checkEqual(sent_msgs, received_msgs1), received_msgs1\n",
    "    assert checkEqual(sent_msgs, received_msgs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1687740a",
   "metadata": {},
   "source": [
    "Check two consumers same group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ed5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: InMemoryBroker._patch_consumers_and_producers(): Patching consumers and producers!\n",
      "[INFO] __main__: InMemoryBroker starting\n",
      "[INFO] __main__: AIOKafkaConsumer patched start() called()\n",
      "[INFO] __main__: AIOKafkaConsumer patched start() called()\n",
      "[INFO] __main__: AIOKafkaProducer patched start() called()\n",
      "[INFO] __main__: AIOKafkaConsumer patched subscribe() called\n",
      "[INFO] __main__: AIOKafkaConsumer.subscribe(), subscribing to: ['test_topic']\n",
      "[INFO] __main__: AIOKafkaConsumer patched subscribe() called\n",
      "[INFO] __main__: AIOKafkaConsumer.subscribe(), subscribing to: ['test_topic']\n",
      "[INFO] __main__: AIOKafkaConsumer patched stop() called\n",
      "[INFO] __main__: AIOKafkaConsumer patched stop() called\n",
      "[INFO] __main__: AIOKafkaProducer patched stop() called\n",
      "[b'msg0', b'msg1', b'msg12', b'msg14', b'msg17', b'msg20', b'msg22', b'msg23', b'msg25', b'msg26', b'msg27', b'msg29', b'msg3', b'msg30', b'msg31', b'msg4', b'msg6', b'msg8', b'msg9']\n",
      "[b'msg10', b'msg11', b'msg13', b'msg15', b'msg16', b'msg18', b'msg19', b'msg2', b'msg21', b'msg24', b'msg28', b'msg5', b'msg7']\n",
      "[INFO] __main__: InMemoryBroker stopping\n"
     ]
    }
   ],
   "source": [
    "topic = \"test_topic\"\n",
    "sent_msgs = [f\"msg{i}\".encode(\"UTF-8\") for i in range(32)]\n",
    "\n",
    "with InMemoryBroker([topic], 5) as broker:\n",
    "    consumer1 = fastkafka._application.app.AIOKafkaConsumer(group_id=\"my_group\", auto_offset_reset=\"earliest\")\n",
    "    consumer2 = fastkafka._application.app.AIOKafkaConsumer(group_id=\"my_group\", auto_offset_reset=\"earliest\")\n",
    "    producer = fastkafka._application.app.AIOKafkaProducer()\n",
    "\n",
    "    await consumer1.start()\n",
    "    await consumer2.start()\n",
    "    await producer.start()\n",
    "\n",
    "    [await producer.send(topic, msg) for msg in sent_msgs]\n",
    "\n",
    "    consumer1.subscribe([topic])\n",
    "    consumer2.subscribe([topic])\n",
    "    \n",
    "    received1 = await consumer1.getmany()\n",
    "    received2 = await consumer2.getmany()\n",
    "\n",
    "    received_msgs1 = [msg.value for _, msgs in received1.items() for msg in msgs]\n",
    "    received_msgs2 = [msg.value for _, msgs in received2.items() for msg in msgs]\n",
    "\n",
    "    await consumer1.stop()\n",
    "    await consumer2.stop()\n",
    "    await producer.stop()\n",
    "    \n",
    "    assert checkEqual(sent_msgs, received_msgs1 + received_msgs2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
