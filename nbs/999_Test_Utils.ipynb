{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc959176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from typing import List, Dict, Any, Optional, Callable, Tuple, Generator\n",
    "from os import environ\n",
    "from contextlib import contextmanager, asynccontextmanager\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import asyncio\n",
    "import hashlib\n",
    "\n",
    "import unittest\n",
    "\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from aiokafka import AIOKafkaProducer, AIOKafkaConsumer\n",
    "\n",
    "from fast_kafka_api._components.logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# allows async calls in notebooks\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2eb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3eee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__, level=20)\n",
    "logger.debug(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6902fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "kafka_server_url = (\n",
    "    environ[\"KAFKA_HOSTNAME\"] if \"KAFKA_HOSTNAME\" in environ else \"localhost\"\n",
    ")\n",
    "kafka_server_port = environ[\"KAFKA_PORT\"] if \"KAFKA_PORT\" in environ else \"9092\"\n",
    "\n",
    "kafka_config = {\n",
    "    \"bootstrap.servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n",
    "    # \"group.id\": f\"{kafka_server_url}:{kafka_server_port}_group\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf46a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def true_after(seconds: float):\n",
    "    \"\"\"Function returning True after a given number of seconds\"\"\"\n",
    "    t = datetime.now()\n",
    "\n",
    "    def _true_after(seconds=seconds, t=t):\n",
    "        return (datetime.now() - t) > timedelta(seconds=seconds)\n",
    "\n",
    "    return _true_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac939ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = true_after(1.1)\n",
    "assert not f()\n",
    "time.sleep(1)\n",
    "assert not f()\n",
    "time.sleep(0.1)\n",
    "assert f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "## TODO: Check if replication num is <= of number of brokers\n",
    "## TODO: Add tests for:\n",
    "#             - Replication factor (less than and greater than number of brokers)\n",
    "#             - Num partitions\n",
    "\n",
    "def create_missing_topics(\n",
    "    admin: AdminClient,\n",
    "    topic_names: List[str],\n",
    "    *,\n",
    "    num_partitions: Optional[int] = None,\n",
    "    replication_factor: Optional[int] = None,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    if not replication_factor:\n",
    "        replication_factor = len(admin.list_topics().brokers)\n",
    "    if not num_partitions:\n",
    "        num_partitions = replication_factor\n",
    "    existing_topics = list(admin.list_topics().topics.keys())\n",
    "    logger.debug(\n",
    "        f\"create_missing_topics({topic_names}): existing_topics={existing_topics}, num_partitions={num_partitions}, replication_factor={replication_factor}\"\n",
    "    )\n",
    "    new_topics = [\n",
    "        NewTopic(\n",
    "            topic,\n",
    "            num_partitions=num_partitions,\n",
    "            replication_factor=replication_factor,\n",
    "            **kwargs,\n",
    "        )\n",
    "        for topic in topic_names\n",
    "        if topic not in existing_topics\n",
    "    ]\n",
    "    if len(new_topics):\n",
    "        logger.info(f\"create_missing_topics({topic_names}): new_topics = {new_topics}\")\n",
    "        fs = admin.create_topics(new_topics)\n",
    "        while not set(topic_names).issubset(set(admin.list_topics().topics.keys())):\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-12-23 22:40:45.356 [INFO] __main__: create_missing_topics(['A', 'B', 'C']): new_topics = [NewTopic(topic=A,num_partitions=3), NewTopic(topic=B,num_partitions=3), NewTopic(topic=C,num_partitions=3)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if topics are created\n",
    "\n",
    "kafka_admin = AdminClient(kafka_config)\n",
    "topics = [\"A\", \"B\", \"C\"]\n",
    "create_missing_topics(kafka_admin, topics)\n",
    "\n",
    "existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "assert set([\"A\", \"B\", \"C\"]) <= existing_topics\n",
    "\n",
    "# Cleanup\n",
    "[await asyncio.wrap_future(topic, loop=None) for topic in kafka_admin.delete_topics(topics=topics).values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed690d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@contextmanager\n",
    "def create_testing_topic(\n",
    "    kafka_config: Dict[str, Any], topic_prefix: str, seed: Optional[int] = None\n",
    ") -> Generator[str, None, None]:\n",
    "    # create random topic name\n",
    "    random.seed(seed)\n",
    "    topic = topic_prefix + str(random.randint(0, 10**10)).zfill(3)\n",
    "\n",
    "    # delete topic if it already exists\n",
    "    admin = AdminClient(kafka_config)\n",
    "    existing_topics = admin.list_topics().topics.keys()\n",
    "    if topic in existing_topics:\n",
    "        logger.warning(f\"topic {topic} exists, deleting it...\")\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        results = {k: f.result() for k, f in fs.items()}\n",
    "        while topic in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)\n",
    "    try:\n",
    "        # create topic if needed\n",
    "        create_missing_topics(admin, [topic])\n",
    "        while topic not in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)\n",
    "        yield topic\n",
    "\n",
    "    finally:\n",
    "        pass\n",
    "        # cleanup if needed again\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        while topic in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a1bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-12-23 22:40:46.431 [INFO] __main__: create_missing_topics(['my_topic_9167024629']): new_topics = [NewTopic(topic=my_topic_9167024629,num_partitions=3)]\n"
     ]
    }
   ],
   "source": [
    "kafka_admin = AdminClient(kafka_config)\n",
    "\n",
    "with create_testing_topic(kafka_config, \"my_topic_\", 1) as topic:\n",
    "    # Check if topic is created and exists in topic list\n",
    "    existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "    assert topic in existing_topics\n",
    "\n",
    "# Check if topic is deleted after exiting context\n",
    "existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "assert topic not in existing_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@asynccontextmanager\n",
    "async def create_and_fill_testing_topic(\n",
    "    msgs: List[bytes], kafka_config: Dict[str, str] = kafka_config, *, seed: int\n",
    ") -> Generator[str, None, None]:\n",
    "\n",
    "    with create_testing_topic(kafka_config, \"my_topic_\", seed=seed) as topic:\n",
    "\n",
    "        producer = AIOKafkaProducer(\n",
    "            bootstrap_servers=kafka_config[\"bootstrap.servers\"]\n",
    "        )\n",
    "        logger.info(f\"Producer {producer} created.\")\n",
    "\n",
    "        await producer.start()\n",
    "        logger.info(f\"Producer {producer} started.\")\n",
    "        try:\n",
    "            fx = [\n",
    "                producer.send(topic, msg, key=f\"{i % 17}\".encode(\"utf-8\"), )\n",
    "                for i, msg in enumerate(msgs)\n",
    "            ]\n",
    "            await producer.flush()\n",
    "            sent_msgs = [await f for f in fx]\n",
    "            msg_statuses = [await s for s in sent_msgs]\n",
    "            logger.info(f\"Sent messages: len(sent_msgs)={len(sent_msgs)}\")\n",
    "\n",
    "            yield topic\n",
    "        finally:\n",
    "            await producer.stop()\n",
    "            logger.info(f\"Producer {producer} stoped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2424896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-12-23 22:40:48.458 [INFO] __main__: create_missing_topics(['my_topic_9167024629']): new_topics = [NewTopic(topic=my_topic_9167024629,num_partitions=3)]\n",
      "22-12-23 22:40:49.461 [INFO] __main__: Producer <aiokafka.producer.producer.AIOKafkaProducer object> created.\n",
      "22-12-23 22:40:49.469 [INFO] __main__: Producer <aiokafka.producer.producer.AIOKafkaProducer object> started.\n",
      "22-12-23 22:40:49.580 [INFO] __main__: Sent messages: len(sent_msgs)=317\n",
      "22-12-23 22:40:49.581 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_9167024629'})\n",
      "22-12-23 22:40:49.581 [INFO] __main__: Consumer <aiokafka.consumer.consumer.AIOKafkaConsumer object> created.\n",
      "22-12-23 22:40:49.586 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_9167024629': 3}. \n",
      "22-12-23 22:40:49.588 [INFO] __main__: Consumer <aiokafka.consumer.consumer.AIOKafkaConsumer object> started.\n",
      "Total messages received: 317\n",
      "22-12-23 22:40:54.636 [INFO] __main__: Consumer <aiokafka.consumer.consumer.AIOKafkaConsumer object> stopped.\n",
      "22-12-23 22:40:54.637 [INFO] __main__: Producer <aiokafka.producer.producer.AIOKafkaProducer object> stoped.\n"
     ]
    }
   ],
   "source": [
    "msgs_sent = 317\n",
    "msgs = [f\"Hello world {i:05d}\".encode(\"utf-8\") for i in range(msgs_sent)]\n",
    "\n",
    "async with create_and_fill_testing_topic(msgs, seed=1) as topic:\n",
    "    consumer = AIOKafkaConsumer(\n",
    "        topic,\n",
    "        bootstrap_servers=kafka_config[\"bootstrap.servers\"],\n",
    "        auto_offset_reset=\"earliest\",\n",
    "        max_poll_records=100,\n",
    "    )\n",
    "    logger.info(f\"Consumer {consumer} created.\")\n",
    "    await consumer.start()\n",
    "    logger.info(f\"Consumer {consumer} started.\")\n",
    "    is_shutting_down_f = true_after(5)\n",
    "    msgs_received = 0\n",
    "    try:\n",
    "        while True:\n",
    "            msgs = await consumer.getmany(timeout_ms=100)\n",
    "            for k, v in msgs.items():\n",
    "                msgs_received = msgs_received + len(v)\n",
    "            if is_shutting_down_f():\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        assert msgs_received == msgs_sent\n",
    "        print(f\"Total messages received: {msgs_received}\")\n",
    "        await consumer.stop()\n",
    "        logger.info(f\"Consumer {consumer} stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Send repeatedly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f69103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def nb_safe_seed(s: str) -> Callable[[int], int]:\n",
    "    \"\"\" Gets a unique seed function for a notebook\n",
    "    \n",
    "    Params:\n",
    "        s: name of the notebook used to initialize the seed function\n",
    "        \n",
    "    Returns:\n",
    "        A unique seed function\n",
    "    \"\"\"\n",
    "    init_seed = int(hashlib.sha1(s.encode(\"utf-8\")).hexdigest(), 16) % (10 ** 8)\n",
    "    \n",
    "    def _get_seed(x:int = 0, *, init_seed:int = init_seed) -> int:\n",
    "        return init_seed + x\n",
    "        \n",
    "    return _get_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = nb_safe_seed(\"999_test_utils\")\n",
    "\n",
    "assert seed() == seed(0)\n",
    "assert seed()+1 == seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@contextmanager\n",
    "def mock_AIOKafkaProducer_send():\n",
    "    \"\"\" Mocks **send** method of **AIOKafkaProducer**\"\"\"\n",
    "    with unittest.mock.patch(\"__main__.AIOKafkaProducer.send\") as mock:\n",
    "\n",
    "        async def _f():\n",
    "            pass\n",
    "\n",
    "        mock.return_value = asyncio.create_task(_f())\n",
    "\n",
    "        yield mock"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
