{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc959176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39bc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import asyncio\n",
    "import contextlib\n",
    "import hashlib\n",
    "import os\n",
    "import random\n",
    "import shlex\n",
    "\n",
    "# [B404:blacklist] Consider possible security implications associated with the subprocess module.\n",
    "import requests\n",
    "import shutil\n",
    "import signal\n",
    "import subprocess  # nosec\n",
    "import textwrap\n",
    "import time\n",
    "import typer\n",
    "import unittest\n",
    "import unittest.mock\n",
    "from contextlib import asynccontextmanager, contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import *\n",
    "\n",
    "import asyncer\n",
    "import uvicorn\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from fastcore.meta import delegates\n",
    "from pydantic import BaseModel\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from fastkafka.server import _import_from_string\n",
    "from fastkafka._components.helpers import combine_params, use_parameters_of\n",
    "from fastkafka._components.logger import get_logger, supress_timestamps\n",
    "from fastkafka.helpers import (\n",
    "    consumes_messages,\n",
    "    create_admin_client,\n",
    "    create_missing_topics,\n",
    "    in_notebook,\n",
    "    tqdm,\n",
    "    trange,\n",
    "    produce_messages,\n",
    ")\n",
    "from fastkafka.application import FastKafka\n",
    "from fastkafka._components.helpers import _import_from_string\n",
    "from fastkafka.helpers import in_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "484ce24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "if in_notebook():\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "else:\n",
    "    from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911a1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "\n",
    "import nest_asyncio\n",
    "from nbdev_mkdocs.docstring import run_examples_from_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c64116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "# allows async calls in notebooks\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a2eb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f3eee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "supress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b6902fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "kafka_server_url = (\n",
    "    os.environ[\"KAFKA_HOSTNAME\"] if \"KAFKA_HOSTNAME\" in os.environ else \"localhost\"\n",
    ")\n",
    "kafka_server_port = os.environ[\"KAFKA_PORT\"] if \"KAFKA_PORT\" in os.environ else \"9092\"\n",
    "\n",
    "aiokafka_config = {\n",
    "    \"bootstrap_servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48f69103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def nb_safe_seed(s: str) -> Callable[[int], int]:\n",
    "    \"\"\"Gets a unique seed function for a notebook\n",
    "\n",
    "    Params:\n",
    "        s: name of the notebook used to initialize the seed function\n",
    "\n",
    "    Returns:\n",
    "        A unique seed function\n",
    "    \"\"\"\n",
    "    init_seed = int(hashlib.sha256(s.encode(\"utf-8\")).hexdigest(), 16) % (10**8)\n",
    "\n",
    "    def _get_seed(x: int = 0, *, init_seed: int = init_seed) -> int:\n",
    "        return init_seed + x\n",
    "\n",
    "    return _get_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfba6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = nb_safe_seed(\"999_test_utils\")\n",
    "\n",
    "assert seed() == seed(0)\n",
    "assert seed() + 1 == seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf46a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def true_after(seconds: float) -> Callable[[], bool]:\n",
    "    \"\"\"Function returning True after a given number of seconds\"\"\"\n",
    "    t = datetime.now()\n",
    "\n",
    "    def _true_after(seconds: float = seconds, t: datetime = t) -> bool:\n",
    "        return (datetime.now() - t) > timedelta(seconds=seconds)\n",
    "\n",
    "    return _true_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac939ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = true_after(1.1)\n",
    "assert not f()\n",
    "time.sleep(1)\n",
    "assert not f()\n",
    "time.sleep(0.1)\n",
    "assert f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed690d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "@delegates(create_missing_topics)  # type: ignore\n",
    "def create_testing_topic(\n",
    "    *,\n",
    "    topic_prefix: str = \"test_topic_\",\n",
    "    seed: Optional[int] = None,\n",
    "    **kwargs: Dict[str, Any],\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"Create testing topic\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        from os import environ\n",
    "        from fastkafka.testing import create_testing_topic, create_admin_client\n",
    "\n",
    "        kafka_server_url = environ[\"KAFKA_HOSTNAME\"]\n",
    "        aiokafka_config = {\"bootstrap_servers\": f\"{kafka_server_url}:9092\"}\n",
    "\n",
    "        with create_testing_topic(\n",
    "            topic_prefix=\"my_topic_for_create_testing_topic_\",\n",
    "            seed=746855,\n",
    "            num_partitions=1,\n",
    "            **aiokafka_config\n",
    "        ) as topic:\n",
    "            # Check if topic is created and exists in topic list\n",
    "            kafka_admin = create_admin_client(**aiokafka_config)\n",
    "            existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "            assert topic in existing_topics\n",
    "\n",
    "        # Check if topic is deleted after exiting context\n",
    "        existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "        assert topic not in existing_topics\n",
    "        ```\n",
    "\n",
    "    Args:\n",
    "        topic_prefix: topic name prefix which will be augumented with a randomly generated sufix\n",
    "        seed: seed used to generate radnom sufix\n",
    "        topic_names: a list of topic names\n",
    "        num_partitions: Number of partitions to create\n",
    "        replication_factor: Replication factor of partitions, or -1 if replica_assignment is used.\n",
    "        replica_assignment: List of lists with the replication assignment for each new partition.\n",
    "        new_topic_config: topic level config parameters as defined here: https://kafka.apache.org/documentation.html#topicconfigs\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "            ``host[:port]`` strings that the producer should contact to\n",
    "            bootstrap initial cluster metadata. This does not have to be the\n",
    "            full node list.  It just needs to have at least one broker that will\n",
    "            respond to a Metadata API Request. Default port is 9092. If no\n",
    "            servers are specified, will default to ``localhost:9092``.\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "            Default: ``PLAINTEXT``.\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "            are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "\n",
    "    Returns:\n",
    "        Generator returning the generated name of the created topic\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # create random topic name\n",
    "    random.seed(seed)\n",
    "    # [B311:blacklist] Standard pseudo-random generators are not suitable for security/cryptographic purposes.\n",
    "    suffix = str(random.randint(0, 10**10))  # nosec\n",
    "\n",
    "    topic = topic_prefix + suffix.zfill(3)\n",
    "\n",
    "    # delete topic if it already exists\n",
    "    admin = create_admin_client(**kwargs)  # type: ignore\n",
    "    existing_topics = admin.list_topics().topics.keys()\n",
    "    if topic in existing_topics:\n",
    "        logger.warning(f\"topic {topic} exists, deleting it...\")\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        results = {k: f.result() for k, f in fs.items()}\n",
    "        while topic in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)\n",
    "    try:\n",
    "        # create topic if needed\n",
    "        create_missing_topics([topic], **kwargs)\n",
    "        while topic not in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)\n",
    "        yield topic\n",
    "\n",
    "    finally:\n",
    "        pass\n",
    "        # cleanup if needed again\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        while topic in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b6b0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(create_testing_topic, create_missing_topics).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed9c859e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n",
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n",
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n",
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace;font-size:.68rem\">Example:\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────── </span>code<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────────────────────────</span>\n",
       "\n",
       "    from os import environ\n",
       "    from fastkafka.testing import create_testing_topic, create_admin_client\n",
       "\n",
       "    kafka_server_url = environ<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"KAFKA_HOSTNAME\"</span><span style=\"font-weight: bold\">]</span>\n",
       "    aiokafka_config = <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"bootstrap_servers\"</span>: f\"<span style=\"font-weight: bold\">{</span>kafka_server_url<span style=\"font-weight: bold\">}</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9092</span>\"<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "    with <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_testing_topic</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">topic_prefix</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"my_topic_for_create_testing_topic_\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">seed</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">746855</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">num_partitions</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        **aiokafka_config\n",
       "    <span style=\"font-weight: bold\">)</span> as topic:\n",
       "        # Check if topic is created and exists in topic list\n",
       "        kafka_admin = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_admin_client</span><span style=\"font-weight: bold\">(</span>**aiokafka_config<span style=\"font-weight: bold\">)</span>\n",
       "        existing_topics = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">kafka_admin.list_topics</span><span style=\"font-weight: bold\">()</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.topics.keys</span><span style=\"font-weight: bold\">()</span>\n",
       "        assert topic in existing_topics\n",
       "\n",
       "    # Check if topic is deleted after exiting context\n",
       "    existing_topics = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">kafka_admin.list_topics</span><span style=\"font-weight: bold\">()</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.topics.keys</span><span style=\"font-weight: bold\">()</span>\n",
       "    assert topic not in existing_topics\n",
       "\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────── </span>stdout supressed<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────</span>\n",
       "N/A\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────────────── </span>stderr<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Example:\n",
       "\u001b[92m───────────────────────────────────── \u001b[0mcode\u001b[92m ─────────────────────────────────────\u001b[0m\n",
       "\n",
       "    from os import environ\n",
       "    from fastkafka.testing import create_testing_topic, create_admin_client\n",
       "\n",
       "    kafka_server_url = environ\u001b[1m[\u001b[0m\u001b[32m\"KAFKA_HOSTNAME\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "    aiokafka_config = \u001b[1m{\u001b[0m\u001b[32m\"bootstrap_servers\"\u001b[0m: f\"\u001b[1m{\u001b[0mkafka_server_url\u001b[1m}\u001b[0m:\u001b[1;36m9092\u001b[0m\"\u001b[1m}\u001b[0m\n",
       "\n",
       "    with \u001b[1;35mcreate_testing_topic\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mtopic_prefix\u001b[0m=\u001b[32m\"my_topic_for_create_testing_topic_\"\u001b[0m,\n",
       "        \u001b[33mseed\u001b[0m=\u001b[1;36m746855\u001b[0m,\n",
       "        \u001b[33mnum_partitions\u001b[0m=\u001b[1;36m1\u001b[0m,\n",
       "        **aiokafka_config\n",
       "    \u001b[1m)\u001b[0m as topic:\n",
       "        # Check if topic is created and exists in topic list\n",
       "        kafka_admin = \u001b[1;35mcreate_admin_client\u001b[0m\u001b[1m(\u001b[0m**aiokafka_config\u001b[1m)\u001b[0m\n",
       "        existing_topics = \u001b[1;35mkafka_admin.list_topics\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1;35m.topics.keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        assert topic in existing_topics\n",
       "\n",
       "    # Check if topic is deleted after exiting context\n",
       "    existing_topics = \u001b[1;35mkafka_admin.list_topics\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1;35m.topics.keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    assert topic not in existing_topics\n",
       "\n",
       "\u001b[92m─────────────────────────────── \u001b[0mstdout supressed\u001b[92m ───────────────────────────────\u001b[0m\n",
       "N/A\n",
       "\u001b[92m──────────────────────────────────── \u001b[0mstderr\u001b[92m ────────────────────────────────────\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_examples_from_docstring(create_testing_topic(width=120), supress_stdout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "961f147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "@delegates(produce_messages)  # type: ignore\n",
    "@delegates(create_testing_topic, keep=True)  # type: ignore\n",
    "async def create_and_fill_testing_topic(**kwargs: Dict[str, str]) -> AsyncIterator[str]:\n",
    "    \"\"\"Create testing topic with a random sufix in the same and fill it will messages\n",
    "\n",
    "    Args:\n",
    "        topic_names: a list of topic names\n",
    "        num_partitions: Number of partitions to create\n",
    "        replication_factor: Replication factor of partitions, or -1 if replica_assignment is used.\n",
    "        replica_assignment: List of lists with the replication assignment for each new partition.\n",
    "        new_topic_config: topic level config parameters as defined here: https://kafka.apache.org/documentation.html#topicconfigs\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "            ``host[:port]`` strings that the producer should contact to\n",
    "            bootstrap initial cluster metadata. This does not have to be the\n",
    "            full node list.  It just needs to have at least one broker that will\n",
    "            respond to a Metadata API Request. Default port is 9092. If no\n",
    "            servers are specified, will default to ``localhost:9092``.\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "            Default: ``PLAINTEXT``.\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "            are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        topic: Topic name\n",
    "        msgs: a list of messages to produce\n",
    "        client_id (str): a name for this client. This string is passed in\n",
    "            each request to servers and can be used to identify specific\n",
    "            server-side log entries that correspond to this client.\n",
    "            Default: ``aiokafka-producer-#`` (appended with a unique number\n",
    "            per instance)\n",
    "        key_serializer (Callable): used to convert user-supplied keys to bytes\n",
    "            If not :data:`None`, called as ``f(key),`` should return\n",
    "            :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        value_serializer (Callable): used to convert user-supplied message\n",
    "            values to :class:`bytes`. If not :data:`None`, called as\n",
    "            ``f(value)``, should return :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        acks (Any): one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
    "            the producer requires the leader to have received before considering a\n",
    "            request complete. This controls the durability of records that are\n",
    "            sent. The following settings are common:\n",
    "\n",
    "            * ``0``: Producer will not wait for any acknowledgment from the server\n",
    "              at all. The message will immediately be added to the socket\n",
    "              buffer and considered sent. No guarantee can be made that the\n",
    "              server has received the record in this case, and the retries\n",
    "              configuration will not take effect (as the client won't\n",
    "              generally know of any failures). The offset given back for each\n",
    "              record will always be set to -1.\n",
    "            * ``1``: The broker leader will write the record to its local log but\n",
    "              will respond without awaiting full acknowledgement from all\n",
    "              followers. In this case should the leader fail immediately\n",
    "              after acknowledging the record but before the followers have\n",
    "              replicated it then the record will be lost.\n",
    "            * ``all``: The broker leader will wait for the full set of in-sync\n",
    "              replicas to acknowledge the record. This guarantees that the\n",
    "              record will not be lost as long as at least one in-sync replica\n",
    "              remains alive. This is the strongest available guarantee.\n",
    "\n",
    "            If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
    "            :data:`True` defaults to ``acks=all``\n",
    "        compression_type (str): The compression type for all data generated by\n",
    "            the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
    "            or :data:`None`.\n",
    "            Compression is of full batches of data, so the efficacy of batching\n",
    "            will also impact the compression ratio (more batching means better\n",
    "            compression). Default: :data:`None`.\n",
    "        max_batch_size (int): Maximum size of buffered data per partition.\n",
    "            After this amount :meth:`send` coroutine will block until batch is\n",
    "            drained.\n",
    "            Default: 16384\n",
    "        linger_ms (int): The producer groups together any records that arrive\n",
    "            in between request transmissions into a single batched request.\n",
    "            Normally this occurs only under load when records arrive faster\n",
    "            than they can be sent out. However in some circumstances the client\n",
    "            may want to reduce the number of requests even under moderate load.\n",
    "            This setting accomplishes this by adding a small amount of\n",
    "            artificial delay; that is, if first request is processed faster,\n",
    "            than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
    "            Default: 0 (i.e. no delay).\n",
    "        partitioner (Callable): Callable used to determine which partition\n",
    "            each message is assigned to. Called (after key serialization):\n",
    "            ``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
    "            The default partitioner implementation hashes each non-None key\n",
    "            using the same murmur2 algorithm as the Java client so that\n",
    "            messages with the same key are assigned to the same partition.\n",
    "            When a key is :data:`None`, the message is delivered to a random partition\n",
    "            (filtered to partitions with available leaders only, if possible).\n",
    "        max_request_size (int): The maximum size of a request. This is also\n",
    "            effectively a cap on the maximum record size. Note that the server\n",
    "            has its own cap on record size which may be different from this.\n",
    "            This setting will limit the number of record batches the producer\n",
    "            will send in a single request to avoid sending huge requests.\n",
    "            Default: 1048576.\n",
    "        metadata_max_age_ms (int): The period of time in milliseconds after\n",
    "            which we force a refresh of metadata even if we haven't seen any\n",
    "            partition leadership changes to proactively discover any new\n",
    "            brokers or partitions. Default: 300000\n",
    "        request_timeout_ms (int): Produce request timeout in milliseconds.\n",
    "            As it's sent as part of\n",
    "            :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
    "            call), maximum waiting time can be up to ``2 *\n",
    "            request_timeout_ms``.\n",
    "            Default: 40000.\n",
    "        retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
    "            errors. Default: 100.\n",
    "        api_version (str): specify which kafka API version to use.\n",
    "            If set to ``auto``, will attempt to infer the broker version by\n",
    "            probing various APIs. Default: ``auto``\n",
    "        ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
    "            for wrapping socket connections. Directly passed into asyncio's\n",
    "            :meth:`~asyncio.loop.create_connection`. For more\n",
    "            information see :ref:`ssl_auth`.\n",
    "            Default: :data:`None`\n",
    "        connections_max_idle_ms (int): Close idle connections after the number\n",
    "            of milliseconds specified by this config. Specifying :data:`None` will\n",
    "            disable idle checks. Default: 540000 (9 minutes).\n",
    "        enable_idempotence (bool): When set to :data:`True`, the producer will\n",
    "            ensure that exactly one copy of each message is written in the\n",
    "            stream. If :data:`False`, producer retries due to broker failures,\n",
    "            etc., may write duplicates of the retried message in the stream.\n",
    "            Note that enabling idempotence acks to set to ``all``. If it is not\n",
    "            explicitly set by the user it will be chosen. If incompatible\n",
    "            values are set, a :exc:`ValueError` will be thrown.\n",
    "            New in version 0.5.0.\n",
    "        sasl_oauth_token_provider (: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
    "            OAuthBearer token provider instance. (See\n",
    "            :mod:`kafka.oauth.abstract`).\n",
    "            Default: :data:`None`\n",
    "    \"\"\"\n",
    "\n",
    "    with create_testing_topic(\n",
    "        **use_parameters_of(create_testing_topic, **kwargs)\n",
    "    ) as topic:\n",
    "        await produce_messages(\n",
    "            topic=topic, **use_parameters_of(produce_messages, **kwargs)\n",
    "        )\n",
    "\n",
    "        yield topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0930893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(combine_params(create_and_fill_testing_topic, create_missing_topics), produce_messages).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00149fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka.helpers: create_missing_topics(['my_topic_test_create_and_fill_testing_topic_9167024629']): new_topics = [NewTopic(topic=my_topic_test_create_and_fill_testing_topic_9167024629,num_partitions=3)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0debe609374005b99a1cbc332a31a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "producing to 'my_topic_test_create_and_fill_testing_topic_9167024629':   0%|          | 0/120000 [00:00<?, ?it…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_test_create_and_fill_testing_topic_9167024629'})\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_test_create_and_fill_testing_topic_9167024629': 3}. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe53f4a95e9841c5b5d24f7c878c85d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consuming from 'my_topic_test_create_and_fill_testing_topic_9167024629':   0%|          | 0/120000 [00:00<?, ?…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "msgs_count = 120_000\n",
    "\n",
    "\n",
    "class Hello(BaseModel):\n",
    "    msg: str\n",
    "\n",
    "\n",
    "msgs_count = 120_000\n",
    "msgs = (\n",
    "    [b\"Hello world bytes\" for _ in range(msgs_count // 3)]\n",
    "    + [f\"Hello world as string for the {i+1}. time!\" for i in range(msgs_count // 3)]\n",
    "    + [\n",
    "        Hello(msg=\"Hello workd as Pydantic object for the {i+1}. time!\")\n",
    "        for i in range(msgs_count // 3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "async with create_and_fill_testing_topic(\n",
    "    topic_prefix=\"my_topic_test_create_and_fill_testing_topic_\",\n",
    "    msgs=msgs,\n",
    "    seed=1,\n",
    "    **aiokafka_config,\n",
    ") as topic:\n",
    "    await consumes_messages(\n",
    "        topic=topic,\n",
    "        msgs_count=msgs_count,\n",
    "        auto_offset_reset=\"earliest\",\n",
    "        #         group_id=\"test_group\",\n",
    "        **aiokafka_config,\n",
    "    )\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45fa0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Send repeatedly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b04d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mock_AIOKafkaProducer_send() -> Generator[unittest.mock.Mock, None, None]:\n",
    "    \"\"\"Mocks **send** method of **AIOKafkaProducer**\"\"\"\n",
    "    with unittest.mock.patch(\"__main__.AIOKafkaProducer.send\") as mock:\n",
    "\n",
    "        async def _f():\n",
    "            pass\n",
    "\n",
    "        mock.return_value = asyncio.create_task(_f())\n",
    "\n",
    "        yield mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "254e1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def change_dir(d: str) -> Generator[None, None, None]:\n",
    "    curdir = os.getcwd()\n",
    "    os.chdir(d)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac1eac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tests\n",
    "with TemporaryDirectory() as d:\n",
    "    original_wd = os.getcwd()\n",
    "    assert original_wd != d\n",
    "    with change_dir(d):\n",
    "        assert os.getcwd() == d\n",
    "    assert os.getcwd() == original_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85420005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "async def run_script_and_cancel(\n",
    "    script: str,\n",
    "    *,\n",
    "    script_file: Optional[str] = None,\n",
    "    cmd: Optional[str] = None,\n",
    "    cancel_after: int = 10,\n",
    "    app_name: str = \"app\",\n",
    "    kafka_app_name: str = \"kafka_app\",\n",
    "    generate_docs: bool = False,\n",
    ") -> Tuple[int, bytes]:\n",
    "    \"\"\"Run script and cancel after predefined time\n",
    "\n",
    "    Args:\n",
    "        script: a python source code to be executed in a separate subprocess\n",
    "        script_file: name of the script where script source will be saved\n",
    "        cmd: command to execute. If None, it will be set to 'python3 -m {Path(script_file).stem}'\n",
    "        cancel_after: number of seconds before sending SIGTERM signal\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing exit code and combined stdout and stderr as a binary string\n",
    "    \"\"\"\n",
    "    if script_file is None:\n",
    "        script_file = \"script.py\"\n",
    "\n",
    "    if cmd is None:\n",
    "        cmd = f\"python3 -m {Path(script_file).stem}\"\n",
    "\n",
    "    with TemporaryDirectory() as d:\n",
    "        consumer_script = Path(d) / script_file\n",
    "\n",
    "        with open(consumer_script, \"w\") as file:\n",
    "            file.write(script)\n",
    "\n",
    "        with change_dir(d):\n",
    "            if generate_docs:\n",
    "                logger.info(\n",
    "                    f\"Generating docs for: {Path(script_file).stem}:{kafka_app_name}\"\n",
    "                )\n",
    "                try:\n",
    "                    kafka_app: FastKafka = _import_from_string(\n",
    "                        f\"{Path(script_file).stem}:{kafka_app_name}\"\n",
    "                    )\n",
    "                    await asyncer.asyncify(kafka_app.create_docs)()\n",
    "                except Exception as e:\n",
    "                    logger.warning(\n",
    "                        f\"Generating docs failed for: {Path(script_file).stem}:{kafka_app_name}, ignoring it for now.\"\n",
    "                    )\n",
    "\n",
    "            proc = subprocess.Popen(  # nosec: [B603:subprocess_without_shell_equals_true] subprocess call - check for execution of untrusted input.\n",
    "                shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
    "            )\n",
    "            await asyncio.sleep(cancel_after)\n",
    "            proc.terminate()\n",
    "            output, _ = proc.communicate()\n",
    "\n",
    "        return (proc.returncode, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09054da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exit code 0\n",
    "script = \"\"\"\n",
    "from time import sleep\n",
    "print(\"hello\")\n",
    "sleep({t})\n",
    "\"\"\"\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script.format(t=0), cancel_after=2)\n",
    "assert exit_code == 0, exit_code\n",
    "assert output.decode(\"utf-8\") == \"hello\\n\", output.decode(\"utf-8\")\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script.format(t=5), cancel_after=2)\n",
    "assert exit_code < 0, exit_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c8484fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exit code 1\n",
    "script = \"exit(1)\"\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script, cancel_after=1)\n",
    "\n",
    "assert exit_code == 1\n",
    "assert output.decode(\"utf-8\") == \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40aaf329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exit code 0 and output to stdout and stderr\n",
    "script = \"\"\"\n",
    "import sys\n",
    "sys.stderr.write(\"hello from stderr\\\\n\")\n",
    "sys.stderr.flush()\n",
    "print(\"hello, exiting with exit code 0\")\n",
    "exit(0)\n",
    "\"\"\"\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script, cancel_after=1)\n",
    "\n",
    "assert exit_code == 0, exit_code\n",
    "assert (\n",
    "    output.decode(\"utf-8\") == \"hello from stderr\\nhello, exiting with exit code 0\\n\"\n",
    "), output.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b346af63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Check random exit code and output\n",
    "script = \"\"\"\n",
    "print(\"hello\\\\nexiting with exit code 143\")\n",
    "exit(143)\n",
    "\"\"\"\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script, cancel_after=1)\n",
    "\n",
    "assert exit_code == 143\n",
    "assert output.decode(\"utf-8\") == \"hello\\nexiting with exit code 143\\n\"\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fded319",
   "metadata": {},
   "source": [
    "### Local Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9bf939a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class LocalKafkaBroker():\n",
    "    def __init__(self, *, port: Optional[int]=None, **kwargs: Dict[str, Any]):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @classmethod\n",
    "    def _install(cls) -> None:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def start(self) -> Dict[str, Any]:\n",
    "        LocalKafkaBroker._install()\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def stop(self) -> None:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __enter__(self) -> Dict[str, Any]:\n",
    "        return self.start()\n",
    "        \n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        self.stop()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09cececa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mLocalKafkaBroker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9999\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m config:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(config)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m config \u001b[38;5;241m==\u001b[39m {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbootstrap_servers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m127.0.0.1:9999\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[50], line 5\u001b[0m, in \u001b[0;36mLocalKafkaBroker.__init__\u001b[0;34m(self, port, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, port: Optional[\u001b[38;5;28mint\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any]):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with LocalKafkaBroker(port=9999) as config:\n",
    "    print(config)\n",
    "    assert config == {\n",
    "        \"bootstrap_servers\": f\"127.0.0.1:9999\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e7fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2660277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7480229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def install_java():\n",
    "    if not shutil.which(\"java\"):\n",
    "        logger.info(\"Installing Java...\")\n",
    "        logger.info(\" - installing install-jdk...\")\n",
    "        subprocess.run(\"pip install install-jdk\", shell=True, check=True)\n",
    "        import jdk\n",
    "        logger.info(\" - installing jdk...\")\n",
    "        jdk_bin_path = jdk.install(\"11\")\n",
    "        jdk_bin_path\n",
    "        os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{jdk_bin_path}/bin\"\n",
    "        logger.info(\"Java installed.\")\n",
    "    else:\n",
    "        logger.info(\"Java is already installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4499887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Installing Java...\n",
      "[INFO] __main__:  - installing install-jdk...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: install-jdk in /home/davor/.local/lib/python3.10/site-packages (0.3.0)\n",
      "[INFO] __main__:  - installing jdk...\n",
      "[INFO] __main__: Java installed.\n",
      "[INFO] __main__: Java is already installed.\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "install_java()\n",
    "assert shutil.which(\"java\")\n",
    "install_java()\n",
    "assert shutil.which(\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05a57f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def install_kafka():\n",
    "    if not shutil.which(\"kafka-server-start.sh\"):\n",
    "        logger.info(\"Installing Kafka...\")\n",
    "        kafka_version = \"3.3.2\"\n",
    "        kafka_fname = f\"kafka_2.13-{kafka_version}\"\n",
    "        kafka_url = f\"https://dlcdn.apache.org/kafka/{kafka_version}/{kafka_fname}.tgz\"\n",
    "        local_path = Path(os.environ[\"HOME\"]) / \".local\"\n",
    "        local_path.mkdir(exist_ok=True, parents=True)\n",
    "        tgz_path = local_path / f\"{kafka_fname}.tgz\"\n",
    "        kafka_path = local_path / f\"{kafka_fname}\"\n",
    "\n",
    "        response = requests.get(kafka_url, stream=True, )\n",
    "        try:\n",
    "            total = response.raw.length_remaining // 128\n",
    "        except Exception:\n",
    "            total = None\n",
    "\n",
    "        with open(tgz_path, \"wb\") as f:\n",
    "            for data in tqdm(response.iter_content(chunk_size=128), total=total):\n",
    "                f.write(data)\n",
    "\n",
    "        tar = tarfile.open(tgz_path)\n",
    "        tar.extractall(local_path)\n",
    "        tar.close()\n",
    "\n",
    "        os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{kafka_path}/bin\"\n",
    "        logger.info(f\"Kafka installed in {kafka_path}.\")\n",
    "    else:\n",
    "        logger.info(\"Kafka is already installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07f3399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Installing Kafka...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cc191071cf4f4a860de2a26e707d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/832968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Kafka installed in /home/davor/.local/kafka_2.13-3.3.2.\n",
      "[INFO] __main__: Kafka is already installed\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "install_kafka()\n",
    "assert shutil.which(\"kafka-server-start.sh\")\n",
    "install_kafka()\n",
    "assert shutil.which(\"kafka-server-start.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "167099d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def install_kafka_and_deps():\n",
    "    install_java()\n",
    "    install_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4600ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "install_kafka_and_deps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e14a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "async def run_localkafka() -> None:\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    HANDLED_SIGNALS = (\n",
    "        signal.SIGINT,  # Unix signal 2. Sent by Ctrl+C.\n",
    "        signal.SIGTERM,  # Unix signal 15. Sent by `kill <pid>`.\n",
    "    )\n",
    "\n",
    "    d = {\"should_exit\": False}\n",
    "\n",
    "    def handle_exit(sig: int, d: Dict[str, bool] = d) -> None:\n",
    "        d[\"should_exit\"] = True\n",
    "\n",
    "    for sig in HANDLED_SIGNALS:\n",
    "        loop.add_signal_handler(sig, handle_exit, sig)\n",
    "\n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        zookeeper_properties_path = str(\n",
    "            (\n",
    "                Path(shutil.which(\"zookeeper-server-start.sh\"))\n",
    "                / \"..\"\n",
    "                / \"..\"\n",
    "                / \"config\"\n",
    "                / \"zookeeper.properties\"\n",
    "            ).resolve()\n",
    "        )\n",
    "        server_properties_path = str(\n",
    "            (\n",
    "                Path(shutil.which(\"kafka-server-start.sh\"))\n",
    "                / \"..\"\n",
    "                / \"..\"\n",
    "                / \"config\"\n",
    "                / \"server.properties\"\n",
    "            ).resolve()\n",
    "        )\n",
    "\n",
    "        #         subprocess.run([\"zookeeper-server-start.sh\", zookeeper_properties_path], check=True)\n",
    "        logger.info(\"Starting zookeeper and waiting for 10 seconds...\")\n",
    "        tasks = [\n",
    "            tg.soonify(asyncio.create_subprocess_exec)(\n",
    "                \"zookeeper-server-start.sh\",\n",
    "                zookeeper_properties_path,\n",
    "                stdout=asyncio.subprocess.PIPE,\n",
    "                stdin=asyncio.subprocess.PIPE,\n",
    "            )\n",
    "        ]\n",
    "        await asyncio.sleep(10)\n",
    "        \n",
    "        logger.info(\"Starting Kafka server and waiting for another 10 seconds...\")\n",
    "        tasks = tasks + [\n",
    "            tg.soonify(asyncio.create_subprocess_exec)(\n",
    "                \"kafka-server-start.sh\",\n",
    "                server_properties_path,\n",
    "                stdout=asyncio.subprocess.PIPE,\n",
    "                stdin=asyncio.subprocess.PIPE,\n",
    "            )\n",
    "        ]\n",
    "        await asyncio.sleep(10)\n",
    "\n",
    "    procs = [task.value for task in tasks]\n",
    "\n",
    "    async def log_output(\n",
    "        output: Optional[asyncio.StreamReader], pid: int, d: Dict[str, bool] = d\n",
    "    ) -> None:\n",
    "        if output is None:\n",
    "            raise RuntimeError(\"Expected StreamReader, got None. Is stdout piped?\")\n",
    "        while not output.at_eof():\n",
    "            outs = await output.readline()\n",
    "            if outs != b\"\":\n",
    "                typer.echo(f\"[{pid:03d}]: \" + outs.decode(\"utf-8\"), nl=False)\n",
    "\n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        for proc in procs:\n",
    "            tg.soonify(log_output)(proc.stdout, proc.pid)\n",
    "\n",
    "        while not d[\"should_exit\"]:\n",
    "            await asyncio.sleep(0.2)\n",
    "\n",
    "        typer.echo(\"Starting process cleanup, this may take a few seconds...\")\n",
    "        for proc in procs:\n",
    "            tg.soonify(terminate_asyncio_process)(proc)\n",
    "\n",
    "    for proc in procs:\n",
    "        output, _ = await proc.communicate()\n",
    "        if output:\n",
    "            typer.echo(f\"[{proc.pid:03d}]: \" + output.decode(\"utf-8\"), nl=False)\n",
    "\n",
    "    returncodes = [proc.returncode for proc in procs]\n",
    "    if not returncodes == [0] * len(procs):\n",
    "        typer.secho(\n",
    "            f\"Return codes are not all zero: {returncodes}\",\n",
    "            err=True,\n",
    "            fg=typer.colors.RED,\n",
    "        )\n",
    "        raise typer.Exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9d55f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Starting zookeeper and waiting for 10 seconds...\n",
      "[INFO] __main__: Starting Kafka server and waiting for another 10 seconds...\n",
      "[4751]: [2023-02-07 15:21:33,206] INFO Reading configuration from: /home/davor/.local/kafka_2.13-3.3.2/config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[4751]: [2023-02-07 15:21:33,209] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[4751]: [2023-02-07 15:21:33,209] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[4751]: [2023-02-07 15:21:33,209] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[4751]: [2023-02-07 15:21:33,209] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[4751]: [2023-02-07 15:21:33,210] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[4751]: [2023-02-07 15:21:33,210] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[4751]: [2023-02-07 15:21:33,210] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[4751]: [2023-02-07 15:21:33,210] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)\n",
      "[4751]: [2023-02-07 15:21:33,211] INFO Log4j 1.2 jmx support not found; jmx disabled. (org.apache.zookeeper.jmx.ManagedUtil)\n",
      "[4751]: [2023-02-07 15:21:33,211] INFO Reading configuration from: /home/davor/.local/kafka_2.13-3.3.2/config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[4751]: [2023-02-07 15:21:33,211] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[4751]: [2023-02-07 15:21:33,212] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[4751]: [2023-02-07 15:21:33,212] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[4751]: [2023-02-07 15:21:33,212] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[4751]: [2023-02-07 15:21:33,212] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)\n",
      "[4751]: [2023-02-07 15:21:33,218] INFO ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@503d687a (org.apache.zookeeper.server.ServerMetrics)\n",
      "[4751]: [2023-02-07 15:21:33,220] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[4751]: [2023-02-07 15:21:33,226] INFO  (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,226] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,226] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,227] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,227] INFO    / /    / _ \\   / _ \\  | |/ /  / _ \\  / _ \\ | '_ \\   / _ \\ | '__| (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,227] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,227] INFO  /_____|  \\___/   \\___/  |_|\\_\\  \\___|  \\___| | .__/   \\___| |_| (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,227] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,227] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,227] INFO  (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,227] INFO Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:host.name=davor-fastkafka-devel (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:java.version=11.0.18 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:java.vendor=Eclipse Adoptium (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:java.home=/home/davor/.jdk/jdk-11.0.18+10 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:java.class.path=/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/activation-1.1.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/argparse4j-0.7.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/audience-annotations-0.5.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/commons-cli-1.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.12.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.8.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-api-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-basic-auth-extension-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-json-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-client-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-runtime-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-transforms-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/hk2-api-2.6.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/hk2-locator-2.6.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/hk2-utils-2.6.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-annotations-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-core-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-databind-2.13.4.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-dataformat-csv-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-datatype-jdk8-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-base-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-json-provider-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-jaxb-annotations-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-scala_2.13-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.activation-api-1.2.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.inject-2.6.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/javassist-3.27.0-GA.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/javax.servlet-api-3.1.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jaxb-api-2.3.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jersey-client-2.34.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jersey-common-2.34.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-2.34.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-core-2.34.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jersey-hk2-2.34.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jersey-server-2.34.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-client-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-continuation-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-http-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-io-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-security-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-server-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlet-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlets-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-ajax-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jline-3.21.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jopt-simple-5.0.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jose4j-0.7.9.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-clients-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-log4j-appender-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-metadata-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-raft-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-server-common-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-shell-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-api-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-examples-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-scala_2.13-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-test-utils-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-tools-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka_2.13-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/lz4-java-1.8.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/maven-artifact-3.8.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-2.2.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-4.1.12.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-buffer-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-codec-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-common-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-handler-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-resolver-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-classes-epoll-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-epoll-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-unix-common-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/paranamer-2.8.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/plexus-utils-3.3.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/reflections-0.9.12.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/reload4j-1.2.19.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/rocksdbjni-7.1.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/scala-collection-compat_2.13-2.6.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/scala-library-2.13.8.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/scala-logging_2.13-3.9.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/scala-reflect-2.13.8.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-api-1.7.36.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-reload4j-1.7.36.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/snappy-java-1.1.8.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/swagger-annotations-2.2.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/trogdor-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-3.6.3.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-jute-3.6.3.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/zstd-jni-1.5.2-1.jar (org.apache.zookeeper.server.ZooKeeperServer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:os.version=5.15.0-58-generic (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:user.name=davor (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:user.home=/home/davor (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:user.dir=/work/fastkafka/nbs (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:os.memory.free=488MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO zookeeper.enableEagerACLCheck = false (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO zookeeper.digest.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO zookeeper.closeSessionTxn.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO zookeeper.flushDelay=0 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO zookeeper.maxWriteQueuePollTime=0 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,228] INFO zookeeper.maxBatchSize=1000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,229] INFO zookeeper.intBufferStartingSizeBytes = 1024 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,229] INFO Weighed connection throttling is disabled (org.apache.zookeeper.server.BlueThrottle)\n",
      "[4751]: [2023-02-07 15:21:33,230] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,230] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,230] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)\n",
      "[4751]: [2023-02-07 15:21:33,230] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)\n",
      "[4751]: [2023-02-07 15:21:33,231] INFO zookeeper.pathStats.slotCapacity = 60 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[4751]: [2023-02-07 15:21:33,231] INFO zookeeper.pathStats.slotDuration = 15 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[4751]: [2023-02-07 15:21:33,231] INFO zookeeper.pathStats.maxDepth = 6 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[4751]: [2023-02-07 15:21:33,231] INFO zookeeper.pathStats.initialDelay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[4751]: [2023-02-07 15:21:33,231] INFO zookeeper.pathStats.delay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[4751]: [2023-02-07 15:21:33,231] INFO zookeeper.pathStats.enabled = false (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[4751]: [2023-02-07 15:21:33,233] INFO The max bytes for all large requests are set to 104857600 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,233] INFO The large request threshold is set to -1 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,233] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 clientPortListenBacklog -1 datadir /tmp/zookeeper/version-2 snapdir /tmp/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,246] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)\n",
      "[4751]: [2023-02-07 15:21:33,246] WARN maxCnxns is not configured, using default value 0. (org.apache.zookeeper.server.ServerCnxnFactory)\n",
      "[4751]: [2023-02-07 15:21:33,247] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 5 selector thread(s), 128 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)\n",
      "[4751]: [2023-02-07 15:21:33,250] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)\n",
      "[4751]: [2023-02-07 15:21:33,260] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)\n",
      "[4751]: [2023-02-07 15:21:33,260] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)\n",
      "[4751]: [2023-02-07 15:21:33,261] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[4751]: [2023-02-07 15:21:33,261] INFO zookeeper.commitLogCount=500 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[4751]: [2023-02-07 15:21:33,264] INFO zookeeper.snapshot.compression.method = CHECKED (org.apache.zookeeper.server.persistence.SnapStream)\n",
      "[4751]: [2023-02-07 15:21:33,264] INFO Snapshotting: 0x0 to /tmp/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[4751]: [2023-02-07 15:21:33,266] INFO Snapshot loaded in 6 ms, highest zxid is 0x0, digest is 1371985504 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[4751]: [2023-02-07 15:21:33,266] INFO Snapshotting: 0x0 to /tmp/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[4751]: [2023-02-07 15:21:33,267] INFO Snapshot taken in 0 ms (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[4751]: [2023-02-07 15:21:33,273] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)\n",
      "[4751]: [2023-02-07 15:21:33,274] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)\n",
      "[4751]: [2023-02-07 15:21:33,284] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)\n",
      "[4751]: [2023-02-07 15:21:33,285] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)\n",
      "[4751]: [2023-02-07 15:21:43,548] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)\n",
      "[5135]: [2023-02-07 15:21:43,222] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\n",
      "[5135]: [2023-02-07 15:21:43,431] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\n",
      "[5135]: [2023-02-07 15:21:43,507] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\n",
      "[5135]: [2023-02-07 15:21:43,508] INFO starting (kafka.server.KafkaServer)\n",
      "[5135]: [2023-02-07 15:21:43,509] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)\n",
      "[5135]: [2023-02-07 15:21:43,517] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:host.name=davor-fastkafka-devel (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:java.version=11.0.18 (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:java.vendor=Eclipse Adoptium (org.apache.zookeeper.ZooKeeper)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:java.home=/home/davor/.jdk/jdk-11.0.18+10 (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:java.class.path=/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/activation-1.1.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/argparse4j-0.7.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/audience-annotations-0.5.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/commons-cli-1.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.12.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.8.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-api-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-basic-auth-extension-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-json-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-client-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-runtime-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/connect-transforms-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/hk2-api-2.6.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/hk2-locator-2.6.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/hk2-utils-2.6.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-annotations-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-core-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-databind-2.13.4.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-dataformat-csv-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-datatype-jdk8-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-base-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-json-provider-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-jaxb-annotations-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-scala_2.13-2.13.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.activation-api-1.2.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.inject-2.6.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/javassist-3.27.0-GA.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/javax.servlet-api-3.1.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jaxb-api-2.3.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jersey-client-2.34.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jersey-common-2.34.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-2.34.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-core-2.34.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jersey-hk2-2.34.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jersey-server-2.34.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-client-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-continuation-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-http-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-io-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-security-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-server-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlet-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlets-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-ajax-9.4.48.v20220622.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jline-3.21.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jopt-simple-5.0.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/jose4j-0.7.9.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-clients-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-log4j-appender-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-metadata-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-raft-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-server-common-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-shell-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-api-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-examples-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-scala_2.13-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-test-utils-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka-tools-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/kafka_2.13-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/lz4-java-1.8.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/maven-artifact-3.8.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-2.2.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-4.1.12.1.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-buffer-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-codec-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-common-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-handler-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-resolver-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-classes-epoll-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-epoll-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-unix-common-4.1.78.Final.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/paranamer-2.8.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/plexus-utils-3.3.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/reflections-0.9.12.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/reload4j-1.2.19.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/rocksdbjni-7.1.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/scala-collection-compat_2.13-2.6.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/scala-library-2.13.8.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/scala-logging_2.13-3.9.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/scala-reflect-2.13.8.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-api-1.7.36.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-reload4j-1.7.36.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/snappy-java-1.1.8.4.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/swagger-annotations-2.2.0.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/trogdor-3.3.2.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-3.6.3.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-jute-3.6.3.jar:/home/davor/.local/kafka_2.13-3.3.2/bin/../libs/zstd-jni-1.5.2-1.jar (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:os.version=5.15.0-58-generic (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:user.name=davor (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:user.home=/home/davor (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:user.dir=/work/fastkafka/nbs (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:os.memory.free=1009MB (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,521] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,523] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@56f0cc85 (org.apache.zookeeper.ZooKeeper)\n",
      "[5135]: [2023-02-07 15:21:43,526] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)\n",
      "[5135]: [2023-02-07 15:21:43,529] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)\n",
      "[5135]: [2023-02-07 15:21:43,531] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)\n",
      "[5135]: [2023-02-07 15:21:43,532] INFO Opening socket connection to server localhost/127.0.0.1:2181. (org.apache.zookeeper.ClientCnxn)\n",
      "[5135]: [2023-02-07 15:21:43,536] INFO Socket connection established, initiating session, client: /127.0.0.1:46460, server: localhost/127.0.0.1:2181 (org.apache.zookeeper.ClientCnxn)\n",
      "[5135]: [2023-02-07 15:21:43,566] INFO Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x10033395a130000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)\n",
      "[5135]: [2023-02-07 15:21:43,570] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)\n",
      "[5135]: [2023-02-07 15:21:43,763] INFO Cluster ID = 1sq_h0oPSW-vRiF4f1Oeog (kafka.server.KafkaServer)\n",
      "[5135]: [2023-02-07 15:21:43,766] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)\n",
      "[5135]: [2023-02-07 15:21:43,803] INFO KafkaConfig values: \n",
      "[5135]: \tadvertised.listeners = null\n",
      "[5135]: \talter.config.policy.class.name = null\n",
      "[5135]: \talter.log.dirs.replication.quota.window.num = 11\n",
      "[5135]: \talter.log.dirs.replication.quota.window.size.seconds = 1\n",
      "[5135]: \tauthorizer.class.name = \n",
      "[5135]: \tauto.create.topics.enable = true\n",
      "[5135]: \tauto.leader.rebalance.enable = true\n",
      "[5135]: \tbackground.threads = 10\n",
      "[5135]: \tbroker.heartbeat.interval.ms = 2000\n",
      "[5135]: \tbroker.id = 0\n",
      "[5135]: \tbroker.id.generation.enable = true\n",
      "[5135]: \tbroker.rack = null\n",
      "[5135]: \tbroker.session.timeout.ms = 9000\n",
      "[5135]: \tclient.quota.callback.class = null\n",
      "[5135]: \tcompression.type = producer\n",
      "[5135]: \tconnection.failed.authentication.delay.ms = 100\n",
      "[5135]: \tconnections.max.idle.ms = 600000\n",
      "[5135]: \tconnections.max.reauth.ms = 0\n",
      "[5135]: \tcontrol.plane.listener.name = null\n",
      "[5135]: \tcontrolled.shutdown.enable = true\n",
      "[5135]: \tcontrolled.shutdown.max.retries = 3\n",
      "[5135]: \tcontrolled.shutdown.retry.backoff.ms = 5000\n",
      "[5135]: \tcontroller.listener.names = null\n",
      "[5135]: \tcontroller.quorum.append.linger.ms = 25\n",
      "[5135]: \tcontroller.quorum.election.backoff.max.ms = 1000\n",
      "[5135]: \tcontroller.quorum.election.timeout.ms = 1000\n",
      "[5135]: \tcontroller.quorum.fetch.timeout.ms = 2000\n",
      "[5135]: \tcontroller.quorum.request.timeout.ms = 2000\n",
      "[5135]: \tcontroller.quorum.retry.backoff.ms = 20\n",
      "[5135]: \tcontroller.quorum.voters = []\n",
      "[5135]: \tcontroller.quota.window.num = 11\n",
      "[5135]: \tcontroller.quota.window.size.seconds = 1\n",
      "[5135]: \tcontroller.socket.timeout.ms = 30000\n",
      "[5135]: \tcreate.topic.policy.class.name = null\n",
      "[5135]: \tdefault.replication.factor = 1\n",
      "[5135]: \tdelegation.token.expiry.check.interval.ms = 3600000\n",
      "[5135]: \tdelegation.token.expiry.time.ms = 86400000\n",
      "[5135]: \tdelegation.token.master.key = null\n",
      "[5135]: \tdelegation.token.max.lifetime.ms = 604800000\n",
      "[5135]: \tdelegation.token.secret.key = null\n",
      "[5135]: \tdelete.records.purgatory.purge.interval.requests = 1\n",
      "[5135]: \tdelete.topic.enable = true\n",
      "[5135]: \tearly.start.listeners = null\n",
      "[5135]: \tfetch.max.bytes = 57671680\n",
      "[5135]: \tfetch.purgatory.purge.interval.requests = 1000\n",
      "[5135]: \tgroup.initial.rebalance.delay.ms = 0\n",
      "[5135]: \tgroup.max.session.timeout.ms = 1800000\n",
      "[5135]: \tgroup.max.size = 2147483647\n",
      "[5135]: \tgroup.min.session.timeout.ms = 6000\n",
      "[5135]: \tinitial.broker.registration.timeout.ms = 60000\n",
      "[5135]: \tinter.broker.listener.name = null\n",
      "[5135]: \tinter.broker.protocol.version = 3.3-IV3\n",
      "[5135]: \tkafka.metrics.polling.interval.secs = 10\n",
      "[5135]: \tkafka.metrics.reporters = []\n",
      "[5135]: \tleader.imbalance.check.interval.seconds = 300\n",
      "[5135]: \tleader.imbalance.per.broker.percentage = 10\n",
      "[5135]: \tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n",
      "[5135]: \tlisteners = PLAINTEXT://:9092\n",
      "[5135]: \tlog.cleaner.backoff.ms = 15000\n",
      "[5135]: \tlog.cleaner.dedupe.buffer.size = 134217728\n",
      "[5135]: \tlog.cleaner.delete.retention.ms = 86400000\n",
      "[5135]: \tlog.cleaner.enable = true\n",
      "[5135]: \tlog.cleaner.io.buffer.load.factor = 0.9\n",
      "[5135]: \tlog.cleaner.io.buffer.size = 524288\n",
      "[5135]: \tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n",
      "[5135]: \tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n",
      "[5135]: \tlog.cleaner.min.cleanable.ratio = 0.5\n",
      "[5135]: \tlog.cleaner.min.compaction.lag.ms = 0\n",
      "[5135]: \tlog.cleaner.threads = 1\n",
      "[5135]: \tlog.cleanup.policy = [delete]\n",
      "[5135]: \tlog.dir = /tmp/kafka-logs\n",
      "[5135]: \tlog.dirs = /tmp/kafka-logs\n",
      "[5135]: \tlog.flush.interval.messages = 9223372036854775807\n",
      "[5135]: \tlog.flush.interval.ms = null\n",
      "[5135]: \tlog.flush.offset.checkpoint.interval.ms = 60000\n",
      "[5135]: \tlog.flush.scheduler.interval.ms = 9223372036854775807\n",
      "[5135]: \tlog.flush.start.offset.checkpoint.interval.ms = 60000\n",
      "[5135]: \tlog.index.interval.bytes = 4096\n",
      "[5135]: \tlog.index.size.max.bytes = 10485760\n",
      "[5135]: \tlog.message.downconversion.enable = true\n",
      "[5135]: \tlog.message.format.version = 3.0-IV1\n",
      "[5135]: \tlog.message.timestamp.difference.max.ms = 9223372036854775807\n",
      "[5135]: \tlog.message.timestamp.type = CreateTime\n",
      "[5135]: \tlog.preallocate = false\n",
      "[5135]: \tlog.retention.bytes = -1\n",
      "[5135]: \tlog.retention.check.interval.ms = 300000\n",
      "[5135]: \tlog.retention.hours = 168\n",
      "[5135]: \tlog.retention.minutes = null\n",
      "[5135]: \tlog.retention.ms = null\n",
      "[5135]: \tlog.roll.hours = 168\n",
      "[5135]: \tlog.roll.jitter.hours = 0\n",
      "[5135]: \tlog.roll.jitter.ms = null\n",
      "[5135]: \tlog.roll.ms = null\n",
      "[5135]: \tlog.segment.bytes = 1073741824\n",
      "[5135]: \tlog.segment.delete.delay.ms = 60000\n",
      "[5135]: \tmax.connection.creation.rate = 2147483647\n",
      "[5135]: \tmax.connections = 2147483647\n",
      "[5135]: \tmax.connections.per.ip = 2147483647\n",
      "[5135]: \tmax.connections.per.ip.overrides = \n",
      "[5135]: \tmax.incremental.fetch.session.cache.slots = 1000\n",
      "[5135]: \tmessage.max.bytes = 1048588\n",
      "[5135]: \tmetadata.log.dir = null\n",
      "[5135]: \tmetadata.log.max.record.bytes.between.snapshots = 20971520\n",
      "[5135]: \tmetadata.log.segment.bytes = 1073741824\n",
      "[5135]: \tmetadata.log.segment.min.bytes = 8388608\n",
      "[5135]: \tmetadata.log.segment.ms = 604800000\n",
      "[5135]: \tmetadata.max.idle.interval.ms = 500\n",
      "[5135]: \tmetadata.max.retention.bytes = -1\n",
      "[5135]: \tmetadata.max.retention.ms = 604800000\n",
      "[5135]: \tmetric.reporters = []\n",
      "[5135]: \tmetrics.num.samples = 2\n",
      "[5135]: \tmetrics.recording.level = INFO\n",
      "[5135]: \tmetrics.sample.window.ms = 30000\n",
      "[5135]: \tmin.insync.replicas = 1\n",
      "[5135]: \tnode.id = 0\n",
      "[5135]: \tnum.io.threads = 8\n",
      "[5135]: \tnum.network.threads = 3\n",
      "[5135]: \tnum.partitions = 1\n",
      "[5135]: \tnum.recovery.threads.per.data.dir = 1\n",
      "[5135]: \tnum.replica.alter.log.dirs.threads = null\n",
      "[5135]: \tnum.replica.fetchers = 1\n",
      "[5135]: \toffset.metadata.max.bytes = 4096\n",
      "[5135]: \toffsets.commit.required.acks = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5135]: \toffsets.commit.timeout.ms = 5000\n",
      "[5135]: \toffsets.load.buffer.size = 5242880\n",
      "[5135]: \toffsets.retention.check.interval.ms = 600000\n",
      "[5135]: \toffsets.retention.minutes = 10080\n",
      "[5135]: \toffsets.topic.compression.codec = 0\n",
      "[5135]: \toffsets.topic.num.partitions = 50\n",
      "[5135]: \toffsets.topic.replication.factor = 1\n",
      "[5135]: \toffsets.topic.segment.bytes = 104857600\n",
      "[5135]: \tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n",
      "[5135]: \tpassword.encoder.iterations = 4096\n",
      "[5135]: \tpassword.encoder.key.length = 128\n",
      "[5135]: \tpassword.encoder.keyfactory.algorithm = null\n",
      "[5135]: \tpassword.encoder.old.secret = null\n",
      "[5135]: \tpassword.encoder.secret = null\n",
      "[5135]: \tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n",
      "[5135]: \tprocess.roles = []\n",
      "[5135]: \tproducer.purgatory.purge.interval.requests = 1000\n",
      "[5135]: \tqueued.max.request.bytes = -1\n",
      "[5135]: \tqueued.max.requests = 500\n",
      "[5135]: \tquota.window.num = 11\n",
      "[5135]: \tquota.window.size.seconds = 1\n",
      "[5135]: \tremote.log.index.file.cache.total.size.bytes = 1073741824\n",
      "[5135]: \tremote.log.manager.task.interval.ms = 30000\n",
      "[5135]: \tremote.log.manager.task.retry.backoff.max.ms = 30000\n",
      "[5135]: \tremote.log.manager.task.retry.backoff.ms = 500\n",
      "[5135]: \tremote.log.manager.task.retry.jitter = 0.2\n",
      "[5135]: \tremote.log.manager.thread.pool.size = 10\n",
      "[5135]: \tremote.log.metadata.manager.class.name = null\n",
      "[5135]: \tremote.log.metadata.manager.class.path = null\n",
      "[5135]: \tremote.log.metadata.manager.impl.prefix = null\n",
      "[5135]: \tremote.log.metadata.manager.listener.name = null\n",
      "[5135]: \tremote.log.reader.max.pending.tasks = 100\n",
      "[5135]: \tremote.log.reader.threads = 10\n",
      "[5135]: \tremote.log.storage.manager.class.name = null\n",
      "[5135]: \tremote.log.storage.manager.class.path = null\n",
      "[5135]: \tremote.log.storage.manager.impl.prefix = null\n",
      "[5135]: \tremote.log.storage.system.enable = false\n",
      "[5135]: \treplica.fetch.backoff.ms = 1000\n",
      "[5135]: \treplica.fetch.max.bytes = 1048576\n",
      "[5135]: \treplica.fetch.min.bytes = 1\n",
      "[5135]: \treplica.fetch.response.max.bytes = 10485760\n",
      "[5135]: \treplica.fetch.wait.max.ms = 500\n",
      "[5135]: \treplica.high.watermark.checkpoint.interval.ms = 5000\n",
      "[5135]: \treplica.lag.time.max.ms = 30000\n",
      "[5135]: \treplica.selector.class = null\n",
      "[5135]: \treplica.socket.receive.buffer.bytes = 65536\n",
      "[5135]: \treplica.socket.timeout.ms = 30000\n",
      "[5135]: \treplication.quota.window.num = 11\n",
      "[5135]: \treplication.quota.window.size.seconds = 1\n",
      "[5135]: \trequest.timeout.ms = 30000\n",
      "[5135]: \treserved.broker.max.id = 1000\n",
      "[5135]: \tsasl.client.callback.handler.class = null\n",
      "[5135]: \tsasl.enabled.mechanisms = [GSSAPI]\n",
      "[5135]: \tsasl.jaas.config = null\n",
      "[5135]: \tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "[5135]: \tsasl.kerberos.min.time.before.relogin = 60000\n",
      "[5135]: \tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n",
      "[5135]: \tsasl.kerberos.service.name = null\n",
      "[5135]: \tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "[5135]: \tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "[5135]: \tsasl.login.callback.handler.class = null\n",
      "[5135]: \tsasl.login.class = null\n",
      "[5135]: \tsasl.login.connect.timeout.ms = null\n",
      "[5135]: \tsasl.login.read.timeout.ms = null\n",
      "[5135]: \tsasl.login.refresh.buffer.seconds = 300\n",
      "[5135]: \tsasl.login.refresh.min.period.seconds = 60\n",
      "[5135]: \tsasl.login.refresh.window.factor = 0.8\n",
      "[5135]: \tsasl.login.refresh.window.jitter = 0.05\n",
      "[5135]: \tsasl.login.retry.backoff.max.ms = 10000\n",
      "[5135]: \tsasl.login.retry.backoff.ms = 100\n",
      "[5135]: \tsasl.mechanism.controller.protocol = GSSAPI\n",
      "[5135]: \tsasl.mechanism.inter.broker.protocol = GSSAPI\n",
      "[5135]: \tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "[5135]: \tsasl.oauthbearer.expected.audience = null\n",
      "[5135]: \tsasl.oauthbearer.expected.issuer = null\n",
      "[5135]: \tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "[5135]: \tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "[5135]: \tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "[5135]: \tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "[5135]: \tsasl.oauthbearer.scope.claim.name = scope\n",
      "[5135]: \tsasl.oauthbearer.sub.claim.name = sub\n",
      "[5135]: \tsasl.oauthbearer.token.endpoint.url = null\n",
      "[5135]: \tsasl.server.callback.handler.class = null\n",
      "[5135]: \tsasl.server.max.receive.size = 524288\n",
      "[5135]: \tsecurity.inter.broker.protocol = PLAINTEXT\n",
      "[5135]: \tsecurity.providers = null\n",
      "[5135]: \tsocket.connection.setup.timeout.max.ms = 30000\n",
      "[5135]: \tsocket.connection.setup.timeout.ms = 10000\n",
      "[5135]: \tsocket.listen.backlog.size = 50\n",
      "[5135]: \tsocket.receive.buffer.bytes = 102400\n",
      "[5135]: \tsocket.request.max.bytes = 104857600\n",
      "[5135]: \tsocket.send.buffer.bytes = 102400\n",
      "[5135]: \tssl.cipher.suites = []\n",
      "[5135]: \tssl.client.auth = none\n",
      "[5135]: \tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "[5135]: \tssl.endpoint.identification.algorithm = https\n",
      "[5135]: \tssl.engine.factory.class = null\n",
      "[5135]: \tssl.key.password = null\n",
      "[5135]: \tssl.keymanager.algorithm = SunX509\n",
      "[5135]: \tssl.keystore.certificate.chain = null\n",
      "[5135]: \tssl.keystore.key = null\n",
      "[5135]: \tssl.keystore.location = null\n",
      "[5135]: \tssl.keystore.password = null\n",
      "[5135]: \tssl.keystore.type = JKS\n",
      "[5135]: \tssl.principal.mapping.rules = DEFAULT\n",
      "[5135]: \tssl.protocol = TLSv1.3\n",
      "[5135]: \tssl.provider = null\n",
      "[5135]: \tssl.secure.random.implementation = null\n",
      "[5135]: \tssl.trustmanager.algorithm = PKIX\n",
      "[5135]: \tssl.truststore.certificates = null\n",
      "[5135]: \tssl.truststore.location = null\n",
      "[5135]: \tssl.truststore.password = null\n",
      "[5135]: \tssl.truststore.type = JKS\n",
      "[5135]: \ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n",
      "[5135]: \ttransaction.max.timeout.ms = 900000\n",
      "[5135]: \ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n",
      "[5135]: \ttransaction.state.log.load.buffer.size = 5242880\n",
      "[5135]: \ttransaction.state.log.min.isr = 1\n",
      "[5135]: \ttransaction.state.log.num.partitions = 50\n",
      "[5135]: \ttransaction.state.log.replication.factor = 1\n",
      "[5135]: \ttransaction.state.log.segment.bytes = 104857600\n",
      "[5135]: \ttransactional.id.expiration.ms = 604800000\n",
      "[5135]: \tunclean.leader.election.enable = false\n",
      "[5135]: \tzookeeper.clientCnxnSocket = null\n",
      "[5135]: \tzookeeper.connect = localhost:2181\n",
      "[5135]: \tzookeeper.connection.timeout.ms = 18000\n",
      "[5135]: \tzookeeper.max.in.flight.requests = 10\n",
      "[5135]: \tzookeeper.session.timeout.ms = 18000\n",
      "[5135]: \tzookeeper.set.acl = false\n",
      "[5135]: \tzookeeper.ssl.cipher.suites = null\n",
      "[5135]: \tzookeeper.ssl.client.enable = false\n",
      "[5135]: \tzookeeper.ssl.crl.enable = false\n",
      "[5135]: \tzookeeper.ssl.enabled.protocols = null\n",
      "[5135]: \tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n",
      "[5135]: \tzookeeper.ssl.keystore.location = null\n",
      "[5135]: \tzookeeper.ssl.keystore.password = null\n",
      "[5135]: \tzookeeper.ssl.keystore.type = null\n",
      "[5135]: \tzookeeper.ssl.ocsp.enable = false\n",
      "[5135]: \tzookeeper.ssl.protocol = TLSv1.2\n",
      "[5135]: \tzookeeper.ssl.truststore.location = null\n",
      "[5135]: \tzookeeper.ssl.truststore.password = null\n",
      "[5135]: \tzookeeper.ssl.truststore.type = null\n",
      "[5135]:  (kafka.server.KafkaConfig)\n",
      "[5135]: [2023-02-07 15:21:43,826] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[5135]: [2023-02-07 15:21:43,827] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[5135]: [2023-02-07 15:21:43,827] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[5135]: [2023-02-07 15:21:43,829] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[5135]: [2023-02-07 15:21:43,840] INFO Log directory /tmp/kafka-logs not found, creating it. (kafka.log.LogManager)\n",
      "[5135]: [2023-02-07 15:21:43,856] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\n",
      "[5135]: [2023-02-07 15:21:43,857] INFO Attempting recovery for all logs in /tmp/kafka-logs since no clean shutdown file was found (kafka.log.LogManager)\n",
      "[5135]: [2023-02-07 15:21:43,864] INFO Loaded 0 logs in 8ms. (kafka.log.LogManager)\n",
      "[5135]: [2023-02-07 15:21:43,864] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\n",
      "[5135]: [2023-02-07 15:21:43,865] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\n",
      "[5135]: [2023-02-07 15:21:43,919] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5135]: [2023-02-07 15:21:43,928] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)\n",
      "[5135]: [2023-02-07 15:21:43,958] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)\n",
      "[5135]: [2023-02-07 15:21:44,176] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n",
      "[5135]: [2023-02-07 15:21:44,179] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\n",
      "[5135]: [2023-02-07 15:21:44,199] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n",
      "[5135]: [2023-02-07 15:21:44,204] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)\n",
      "[5135]: [2023-02-07 15:21:44,223] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:21:44,224] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:21:44,224] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:21:44,225] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:21:44,234] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[5135]: [2023-02-07 15:21:44,255] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)\n",
      "[5135]: [2023-02-07 15:21:44,281] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1675783304265,1675783304265,1,0,0,72113915455275008,226,0,25\n",
      "[5135]:  (kafka.zk.KafkaZkClient)\n",
      "[5135]: [2023-02-07 15:21:44,281] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://davor-fastkafka-devel:9092, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)\n",
      "[5135]: [2023-02-07 15:21:44,328] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:21:44,332] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:21:44,332] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:21:44,340] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)\n",
      "[5135]: [2023-02-07 15:21:44,344] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)\n",
      "[5135]: [2023-02-07 15:21:44,349] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\n",
      "[5135]: [2023-02-07 15:21:44,355] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)\n",
      "[5135]: [2023-02-07 15:21:44,361] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[5135]: [2023-02-07 15:21:44,365] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[5135]: [2023-02-07 15:21:44,365] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[5135]: [2023-02-07 15:21:44,377] INFO [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0). (kafka.server.metadata.ZkMetadataCache)\n",
      "[5135]: [2023-02-07 15:21:44,388] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:21:44,406] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[5135]: [2023-02-07 15:21:44,418] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing. (kafka.network.SocketServer)\n",
      "[5135]: [2023-02-07 15:21:44,426] INFO Kafka version: 3.3.2 (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[5135]: [2023-02-07 15:21:44,426] INFO Kafka commitId: b66af662e61082cb (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[5135]: [2023-02-07 15:21:44,426] INFO Kafka startTimeMs: 1675783304421 (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[5135]: [2023-02-07 15:21:44,428] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)\n",
      "[5135]: [2023-02-07 15:21:44,506] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node davor-fastkafka-devel:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n",
      "[5135]: [2023-02-07 15:21:44,571] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node davor-fastkafka-devel:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n",
      "[5135]: [2023-02-07 15:22:03,818] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)\n",
      "[5135]: [2023-02-07 15:22:03,821] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)\n",
      "[5135]: [2023-02-07 15:22:03,823] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)\n",
      "[5135]: [2023-02-07 15:22:03,838] INFO [KafkaServer id=0] Controlled shutdown request returned successfully after 11ms (kafka.server.KafkaServer)\n",
      "[5135]: [2023-02-07 15:22:03,840] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[5135]: [2023-02-07 15:22:03,840] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[5135]: [2023-02-07 15:22:03,840] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[5135]: [2023-02-07 15:22:03,841] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer)\n",
      "[5135]: [2023-02-07 15:22:03,848] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer)\n",
      "[5135]: [2023-02-07 15:22:03,849] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)\n",
      "[5135]: [2023-02-07 15:22:03,851] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)\n",
      "[5135]: [2023-02-07 15:22:03,855] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,857] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,857] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,857] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)\n",
      "[5135]: [2023-02-07 15:22:03,857] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,858] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,858] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,859] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[5135]: [2023-02-07 15:22:03,860] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)\n",
      "[5135]: [2023-02-07 15:22:03,860] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[5135]: [2023-02-07 15:22:03,861] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[5135]: [2023-02-07 15:22:03,861] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5135]: [2023-02-07 15:22:03,861] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[5135]: [2023-02-07 15:22:03,862] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)\n",
      "[5135]: [2023-02-07 15:22:03,863] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,863] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,863] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,864] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,865] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,865] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,865] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)\n",
      "[5135]: [2023-02-07 15:22:03,866] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)\n",
      "[5135]: [2023-02-07 15:22:03,866] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[5135]: [2023-02-07 15:22:03,867] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[5135]: [2023-02-07 15:22:03,867] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[5135]: [2023-02-07 15:22:03,867] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)\n",
      "[5135]: [2023-02-07 15:22:03,868] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)\n",
      "[5135]: [2023-02-07 15:22:03,868] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)\n",
      "[5135]: [2023-02-07 15:22:03,868] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)\n",
      "[5135]: [2023-02-07 15:22:03,868] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,869] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,869] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,869] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,870] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,870] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,870] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,871] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,871] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,871] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,872] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,872] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[5135]: [2023-02-07 15:22:03,877] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)\n",
      "[5135]: [2023-02-07 15:22:03,878] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down (kafka.server.BrokerToControllerRequestThread)\n",
      "[5135]: [2023-02-07 15:22:03,878] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped (kafka.server.BrokerToControllerRequestThread)\n",
      "[5135]: [2023-02-07 15:22:03,878] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)\n",
      "[5135]: [2023-02-07 15:22:03,879] INFO Broker to controller channel manager for alterPartition shutdown (kafka.server.BrokerToControllerChannelManagerImpl)\n",
      "[5135]: [2023-02-07 15:22:03,880] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread)\n",
      "[5135]: [2023-02-07 15:22:03,880] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread)\n",
      "[5135]: [2023-02-07 15:22:03,880] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)\n",
      "[5135]: [2023-02-07 15:22:03,880] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl)\n",
      "[5135]: [2023-02-07 15:22:03,881] INFO Shutting down. (kafka.log.LogManager)\n",
      "[5135]: [2023-02-07 15:22:03,912] INFO Shutdown complete. (kafka.log.LogManager)\n",
      "[5135]: [2023-02-07 15:22:03,918] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[5135]: [2023-02-07 15:22:03,918] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[5135]: [2023-02-07 15:22:03,918] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[5135]: [2023-02-07 15:22:03,919] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)\n",
      "Starting process cleanup, this may take a few seconds...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'terminate_asyncio_process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_localkafka()\n",
      "Cell \u001b[0;32mIn[36], line 84\u001b[0m, in \u001b[0;36mrun_localkafka\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     typer\u001b[38;5;241m.\u001b[39mecho(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting process cleanup, this may take a few seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m proc \u001b[38;5;129;01min\u001b[39;00m procs:\n\u001b[0;32m---> 84\u001b[0m         tg\u001b[38;5;241m.\u001b[39msoonify(\u001b[43mterminate_asyncio_process\u001b[49m)(proc)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m proc \u001b[38;5;129;01min\u001b[39;00m procs:\n\u001b[1;32m     87\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mcommunicate()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'terminate_asyncio_process' is not defined"
     ]
    }
   ],
   "source": [
    "await run_localkafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e0e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zookeeper_properties_path = str((Path(shutil.which(\"zookeeper-server-start.sh\")) / \"..\" / \"..\" / \"config\" / \"zookeeper.properties\").resolve())\n",
    "server_properties_path = str((Path(shutil.which(\"kafka-server-start.sh\")) / \"..\" / \"..\" / \"config\" / \"server.properties\").resolve())\n",
    "\n",
    "subprocess.run([\"zookeeper-server-start.sh\", zookeeper_properties_path], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3f9391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b45658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !./{kafka_fname}/bin/zookeeper-server-start.sh ./{kafka_fname}/config/zookeeper.properties\n",
    "# !./{kafka_fname}/bin/zookeeper-server-start.sh -daemon ./{kafka_fname}/config/zookeeper.properties\n",
    "# !sleep 3\n",
    "# !./{kafka_fname}/bin/kafka-server-start.sh -daemon ./{kafka_fname}/config/server.properties\n",
    "!./{kafka_fname}/bin/kafka-server-start.sh ./{kafka_fname}/config/server.properties\n",
    "# !echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
    "# !sleep 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ddaf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps -ef | grep kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70080d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if shutil.which(\"java\"):\n",
    "    print(\"Java is installed on the system\")\n",
    "else:\n",
    "    print(\"I guess not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install install-jdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820639d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdk\n",
    "\n",
    "jdk.install('11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "environ[\"PATH\"] = environ[\"PATH\"] + \":/home/davor/.jdk/jdk-11.0.18+10/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env | grep PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487945e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
