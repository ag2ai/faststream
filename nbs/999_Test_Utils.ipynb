{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc959176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def in_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "\n",
    "        if \"IPKernelApp\" not in get_ipython().config:\n",
    "            return False\n",
    "    except ImportError:\n",
    "        return False\n",
    "    except AttributeError:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9760c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import asyncio\n",
    "import contextlib\n",
    "import hashlib\n",
    "import os\n",
    "import random\n",
    "import shlex\n",
    "import textwrap\n",
    "\n",
    "# [B404:blacklist] Consider possible security implications associated with the subprocess module.\n",
    "import subprocess  # nosec\n",
    "import time\n",
    "import unittest\n",
    "import unittest.mock\n",
    "from contextlib import asynccontextmanager, contextmanager\n",
    "from fastcore.meta import delegates\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Any, Callable, Dict, Generator, List, Optional, Tuple, AsyncIterator, Union\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "if in_notebook():\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "else:\n",
    "    from tqdm import tqdm, trange\n",
    "    \n",
    "from fast_kafka_api._components.logger import get_logger, supress_timestamps\n",
    "from fast_kafka_api._components.helpers import combine_params, use_parameters_of\n",
    "from fast_kafka_api.helpers import create_admin_client, create_missing_topics, produce_messages, consumes_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "\n",
    "import nest_asyncio\n",
    "from nbdev_mkdocs.docstring import run_examples_from_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "# allows async calls in notebooks\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2eb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3eee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "supress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6902fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "kafka_server_url = (\n",
    "    os.environ[\"KAFKA_HOSTNAME\"] if \"KAFKA_HOSTNAME\" in os.environ else \"localhost\"\n",
    ")\n",
    "kafka_server_port = os.environ[\"KAFKA_PORT\"] if \"KAFKA_PORT\" in os.environ else \"9092\"\n",
    "\n",
    "aiokafka_config = {\n",
    "    \"bootstrap_servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f69103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def nb_safe_seed(s: str) -> Callable[[int], int]:\n",
    "    \"\"\"Gets a unique seed function for a notebook\n",
    "\n",
    "    Params:\n",
    "        s: name of the notebook used to initialize the seed function\n",
    "\n",
    "    Returns:\n",
    "        A unique seed function\n",
    "    \"\"\"\n",
    "    init_seed = int(hashlib.sha256(s.encode(\"utf-8\")).hexdigest(), 16) % (10**8)\n",
    "\n",
    "    def _get_seed(x: int = 0, *, init_seed: int = init_seed) -> int:\n",
    "        return init_seed + x\n",
    "\n",
    "    return _get_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = nb_safe_seed(\"999_test_utils\")\n",
    "\n",
    "assert seed() == seed(0)\n",
    "assert seed() + 1 == seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf46a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def true_after(seconds: float) -> Callable[[], bool]:\n",
    "    \"\"\"Function returning True after a given number of seconds\"\"\"\n",
    "    t = datetime.now()\n",
    "\n",
    "    def _true_after(seconds: float = seconds, t: datetime = t) -> bool:\n",
    "        return (datetime.now() - t) > timedelta(seconds=seconds)\n",
    "\n",
    "    return _true_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac939ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = true_after(1.1)\n",
    "assert not f()\n",
    "time.sleep(1)\n",
    "assert not f()\n",
    "time.sleep(0.1)\n",
    "assert f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed690d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "@delegates(create_missing_topics) # type: ignore\n",
    "def create_testing_topic(\n",
    "    *, topic_prefix: str=\"test_topic_\", seed: Optional[int] = None, **kwargs: Dict[str, Any]\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"Create testing topic\n",
    "    \n",
    "    Example:\n",
    "        ```python\n",
    "        from os import environ\n",
    "        from fast_kafka_api.testing import create_testing_topic, create_admin_client\n",
    "        \n",
    "        kafka_server_url = environ[\"KAFKA_HOSTNAME\"]\n",
    "        aiokafka_config = {\"bootstrap_servers\": f\"{kafka_server_url}:9092\"}\n",
    "        \n",
    "        with create_testing_topic(\n",
    "            topic_prefix=\"my_topic_for_create_testing_topic_\",\n",
    "            seed=746855,\n",
    "            num_partitions=1,\n",
    "            **aiokafka_config\n",
    "        ) as topic:\n",
    "            # Check if topic is created and exists in topic list\n",
    "            kafka_admin = create_admin_client(**aiokafka_config)\n",
    "            existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "            assert topic in existing_topics\n",
    "\n",
    "        # Check if topic is deleted after exiting context\n",
    "        existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "        assert topic not in existing_topics\n",
    "        ```\n",
    "        \n",
    "    Args:\n",
    "        topic_prefix: topic name prefix which will be augumented with a randomly generated sufix\n",
    "        seed: seed used to generate radnom sufix\n",
    "        topic_names: a list of topic names\n",
    "        num_partitions: Number of partitions to create\n",
    "        replication_factor: Replication factor of partitions, or -1 if replica_assignment is used.\n",
    "        replica_assignment: List of lists with the replication assignment for each new partition.\n",
    "        new_topic_config: topic level config parameters as defined here: https://kafka.apache.org/documentation.html#topicconfigs\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "            ``host[:port]`` strings that the producer should contact to\n",
    "            bootstrap initial cluster metadata. This does not have to be the\n",
    "            full node list.  It just needs to have at least one broker that will\n",
    "            respond to a Metadata API Request. Default port is 9092. If no\n",
    "            servers are specified, will default to ``localhost:9092``.\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "            Default: ``PLAINTEXT``.\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "            are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        \n",
    "    Returns:\n",
    "        Generator returning the generated name of the created topic\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    # create random topic name\n",
    "    random.seed(seed)\n",
    "    # [B311:blacklist] Standard pseudo-random generators are not suitable for security/cryptographic purposes.\n",
    "    suffix = str(random.randint(0, 10**10))  # nosec\n",
    "\n",
    "    topic = topic_prefix + suffix.zfill(3)\n",
    "\n",
    "    # delete topic if it already exists\n",
    "    admin = create_admin_client(**kwargs) # type: ignore\n",
    "    existing_topics = admin.list_topics().topics.keys()\n",
    "    if topic in existing_topics:\n",
    "        logger.warning(f\"topic {topic} exists, deleting it...\")\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        results = {k: f.result() for k, f in fs.items()}\n",
    "        while topic in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)\n",
    "    try:\n",
    "        # create topic if needed\n",
    "        create_missing_topics([topic], **kwargs)\n",
    "        while topic not in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)\n",
    "        yield topic\n",
    "\n",
    "    finally:\n",
    "        pass\n",
    "        # cleanup if needed again\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        while topic in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(create_testing_topic, create_missing_topics).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c859e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n",
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n",
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n",
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace;font-size:.68rem\">Example:\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────── </span>code<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────────────────────────</span>\n",
       "\n",
       "    from os import environ\n",
       "    from fast_kafka_api.testing import create_testing_topic, create_admin_client\n",
       "\n",
       "    kafka_server_url = environ<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"KAFKA_HOSTNAME\"</span><span style=\"font-weight: bold\">]</span>\n",
       "    aiokafka_config = <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"bootstrap_servers\"</span>: f\"<span style=\"font-weight: bold\">{</span>kafka_server_url<span style=\"font-weight: bold\">}</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9092</span>\"<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "    with <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_testing_topic</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">topic_prefix</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"my_topic_for_create_testing_topic_\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">seed</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">746855</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">num_partitions</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        **aiokafka_config\n",
       "    <span style=\"font-weight: bold\">)</span> as topic:\n",
       "        # Check if topic is created and exists in topic list\n",
       "        kafka_admin = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_admin_client</span><span style=\"font-weight: bold\">(</span>**aiokafka_config<span style=\"font-weight: bold\">)</span>\n",
       "        existing_topics = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">kafka_admin.list_topics</span><span style=\"font-weight: bold\">()</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.topics.keys</span><span style=\"font-weight: bold\">()</span>\n",
       "        assert topic in existing_topics\n",
       "\n",
       "    # Check if topic is deleted after exiting context\n",
       "    existing_topics = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">kafka_admin.list_topics</span><span style=\"font-weight: bold\">()</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.topics.keys</span><span style=\"font-weight: bold\">()</span>\n",
       "    assert topic not in existing_topics\n",
       "\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────── </span>stdout supressed<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────</span>\n",
       "N/A\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────────────── </span>stderr<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Example:\n",
       "\u001b[92m───────────────────────────────────── \u001b[0mcode\u001b[92m ─────────────────────────────────────\u001b[0m\n",
       "\n",
       "    from os import environ\n",
       "    from fast_kafka_api.testing import create_testing_topic, create_admin_client\n",
       "\n",
       "    kafka_server_url = environ\u001b[1m[\u001b[0m\u001b[32m\"KAFKA_HOSTNAME\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "    aiokafka_config = \u001b[1m{\u001b[0m\u001b[32m\"bootstrap_servers\"\u001b[0m: f\"\u001b[1m{\u001b[0mkafka_server_url\u001b[1m}\u001b[0m:\u001b[1;36m9092\u001b[0m\"\u001b[1m}\u001b[0m\n",
       "\n",
       "    with \u001b[1;35mcreate_testing_topic\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mtopic_prefix\u001b[0m=\u001b[32m\"my_topic_for_create_testing_topic_\"\u001b[0m,\n",
       "        \u001b[33mseed\u001b[0m=\u001b[1;36m746855\u001b[0m,\n",
       "        \u001b[33mnum_partitions\u001b[0m=\u001b[1;36m1\u001b[0m,\n",
       "        **aiokafka_config\n",
       "    \u001b[1m)\u001b[0m as topic:\n",
       "        # Check if topic is created and exists in topic list\n",
       "        kafka_admin = \u001b[1;35mcreate_admin_client\u001b[0m\u001b[1m(\u001b[0m**aiokafka_config\u001b[1m)\u001b[0m\n",
       "        existing_topics = \u001b[1;35mkafka_admin.list_topics\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1;35m.topics.keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        assert topic in existing_topics\n",
       "\n",
       "    # Check if topic is deleted after exiting context\n",
       "    existing_topics = \u001b[1;35mkafka_admin.list_topics\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1;35m.topics.keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    assert topic not in existing_topics\n",
       "\n",
       "\u001b[92m─────────────────────────────── \u001b[0mstdout supressed\u001b[92m ───────────────────────────────\u001b[0m\n",
       "N/A\n",
       "\u001b[92m──────────────────────────────────── \u001b[0mstderr\u001b[92m ────────────────────────────────────\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_examples_from_docstring(create_testing_topic(width=120), supress_stdout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "@delegates(produce_messages) # type: ignore\n",
    "@delegates(create_testing_topic, keep=True) # type: ignore\n",
    "async def create_and_fill_testing_topic(**kwargs: Dict[str, str]) -> AsyncIterator[str]:\n",
    "    \"\"\"Create testing topic with a random sufix in the same and fill it will messages\n",
    "    \n",
    "    Args:\n",
    "        topic_names: a list of topic names\n",
    "        num_partitions: Number of partitions to create\n",
    "        replication_factor: Replication factor of partitions, or -1 if replica_assignment is used.\n",
    "        replica_assignment: List of lists with the replication assignment for each new partition.\n",
    "        new_topic_config: topic level config parameters as defined here: https://kafka.apache.org/documentation.html#topicconfigs\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "            ``host[:port]`` strings that the producer should contact to\n",
    "            bootstrap initial cluster metadata. This does not have to be the\n",
    "            full node list.  It just needs to have at least one broker that will\n",
    "            respond to a Metadata API Request. Default port is 9092. If no\n",
    "            servers are specified, will default to ``localhost:9092``.\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "            Default: ``PLAINTEXT``.\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "            are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        topic: Topic name\n",
    "        msgs: a list of messages to produce\n",
    "        client_id (str): a name for this client. This string is passed in\n",
    "            each request to servers and can be used to identify specific\n",
    "            server-side log entries that correspond to this client.\n",
    "            Default: ``aiokafka-producer-#`` (appended with a unique number\n",
    "            per instance)\n",
    "        key_serializer (Callable): used to convert user-supplied keys to bytes\n",
    "            If not :data:`None`, called as ``f(key),`` should return\n",
    "            :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        value_serializer (Callable): used to convert user-supplied message\n",
    "            values to :class:`bytes`. If not :data:`None`, called as\n",
    "            ``f(value)``, should return :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        acks (Any): one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
    "            the producer requires the leader to have received before considering a\n",
    "            request complete. This controls the durability of records that are\n",
    "            sent. The following settings are common:\n",
    "\n",
    "            * ``0``: Producer will not wait for any acknowledgment from the server\n",
    "              at all. The message will immediately be added to the socket\n",
    "              buffer and considered sent. No guarantee can be made that the\n",
    "              server has received the record in this case, and the retries\n",
    "              configuration will not take effect (as the client won't\n",
    "              generally know of any failures). The offset given back for each\n",
    "              record will always be set to -1.\n",
    "            * ``1``: The broker leader will write the record to its local log but\n",
    "              will respond without awaiting full acknowledgement from all\n",
    "              followers. In this case should the leader fail immediately\n",
    "              after acknowledging the record but before the followers have\n",
    "              replicated it then the record will be lost.\n",
    "            * ``all``: The broker leader will wait for the full set of in-sync\n",
    "              replicas to acknowledge the record. This guarantees that the\n",
    "              record will not be lost as long as at least one in-sync replica\n",
    "              remains alive. This is the strongest available guarantee.\n",
    "\n",
    "            If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
    "            :data:`True` defaults to ``acks=all``\n",
    "        compression_type (str): The compression type for all data generated by\n",
    "            the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
    "            or :data:`None`.\n",
    "            Compression is of full batches of data, so the efficacy of batching\n",
    "            will also impact the compression ratio (more batching means better\n",
    "            compression). Default: :data:`None`.\n",
    "        max_batch_size (int): Maximum size of buffered data per partition.\n",
    "            After this amount :meth:`send` coroutine will block until batch is\n",
    "            drained.\n",
    "            Default: 16384\n",
    "        linger_ms (int): The producer groups together any records that arrive\n",
    "            in between request transmissions into a single batched request.\n",
    "            Normally this occurs only under load when records arrive faster\n",
    "            than they can be sent out. However in some circumstances the client\n",
    "            may want to reduce the number of requests even under moderate load.\n",
    "            This setting accomplishes this by adding a small amount of\n",
    "            artificial delay; that is, if first request is processed faster,\n",
    "            than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
    "            Default: 0 (i.e. no delay).\n",
    "        partitioner (Callable): Callable used to determine which partition\n",
    "            each message is assigned to. Called (after key serialization):\n",
    "            ``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
    "            The default partitioner implementation hashes each non-None key\n",
    "            using the same murmur2 algorithm as the Java client so that\n",
    "            messages with the same key are assigned to the same partition.\n",
    "            When a key is :data:`None`, the message is delivered to a random partition\n",
    "            (filtered to partitions with available leaders only, if possible).\n",
    "        max_request_size (int): The maximum size of a request. This is also\n",
    "            effectively a cap on the maximum record size. Note that the server\n",
    "            has its own cap on record size which may be different from this.\n",
    "            This setting will limit the number of record batches the producer\n",
    "            will send in a single request to avoid sending huge requests.\n",
    "            Default: 1048576.\n",
    "        metadata_max_age_ms (int): The period of time in milliseconds after\n",
    "            which we force a refresh of metadata even if we haven't seen any\n",
    "            partition leadership changes to proactively discover any new\n",
    "            brokers or partitions. Default: 300000\n",
    "        request_timeout_ms (int): Produce request timeout in milliseconds.\n",
    "            As it's sent as part of\n",
    "            :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
    "            call), maximum waiting time can be up to ``2 *\n",
    "            request_timeout_ms``.\n",
    "            Default: 40000.\n",
    "        retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
    "            errors. Default: 100.\n",
    "        api_version (str): specify which kafka API version to use.\n",
    "            If set to ``auto``, will attempt to infer the broker version by\n",
    "            probing various APIs. Default: ``auto``\n",
    "        ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
    "            for wrapping socket connections. Directly passed into asyncio's\n",
    "            :meth:`~asyncio.loop.create_connection`. For more\n",
    "            information see :ref:`ssl_auth`.\n",
    "            Default: :data:`None`\n",
    "        connections_max_idle_ms (int): Close idle connections after the number\n",
    "            of milliseconds specified by this config. Specifying :data:`None` will\n",
    "            disable idle checks. Default: 540000 (9 minutes).\n",
    "        enable_idempotence (bool): When set to :data:`True`, the producer will\n",
    "            ensure that exactly one copy of each message is written in the\n",
    "            stream. If :data:`False`, producer retries due to broker failures,\n",
    "            etc., may write duplicates of the retried message in the stream.\n",
    "            Note that enabling idempotence acks to set to ``all``. If it is not\n",
    "            explicitly set by the user it will be chosen. If incompatible\n",
    "            values are set, a :exc:`ValueError` will be thrown.\n",
    "            New in version 0.5.0.\n",
    "        sasl_oauth_token_provider (: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
    "            OAuthBearer token provider instance. (See\n",
    "            :mod:`kafka.oauth.abstract`).\n",
    "            Default: :data:`None`\n",
    "    \"\"\"\n",
    "\n",
    "    with create_testing_topic(**use_parameters_of(create_testing_topic, **kwargs)) as topic:\n",
    "\n",
    "        await produce_messages(\n",
    "            topic=topic, **use_parameters_of(produce_messages, **kwargs)\n",
    "        )\n",
    "\n",
    "        yield topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0930893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(combine_params(create_and_fill_testing_topic, create_missing_topics), produce_messages).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00149fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fast_kafka_api.helpers: create_missing_topics(['my_topic_test_create_and_fill_testing_topic_9167024629']): new_topics = [NewTopic(topic=my_topic_test_create_and_fill_testing_topic_9167024629,num_partitions=3)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e2807fc9a54adb8c16d00bfcadc79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "producing to 'my_topic_test_create_and_fill_testing_topic_9167024629':   0%|          | 0/120000 [00:00<?, ?it…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_test_create_and_fill_testing_topic_9167024629'})\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_test_create_and_fill_testing_topic_9167024629': 3}. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9b665a6aec4ffb8475faad50b5c785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consuming from 'my_topic_test_create_and_fill_testing_topic_9167024629':   0%|          | 0/120000 [00:00<?, ?…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "msgs_count = 120_000\n",
    "\n",
    "\n",
    "class Hello(BaseModel):\n",
    "    msg: str\n",
    "\n",
    "\n",
    "msgs_count = 120_000\n",
    "msgs = (\n",
    "    [b\"Hello world bytes\" for _ in range(msgs_count // 3)]\n",
    "    + [f\"Hello world as string for the {i+1}. time!\" for i in range(msgs_count // 3)]\n",
    "    + [\n",
    "        Hello(msg=\"Hello workd as Pydantic object for the {i+1}. time!\")\n",
    "        for i in range(msgs_count // 3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "async with create_and_fill_testing_topic(\n",
    "    topic_prefix=\"my_topic_test_create_and_fill_testing_topic_\",\n",
    "    msgs=msgs,\n",
    "    seed=1,\n",
    "    **aiokafka_config,\n",
    ") as topic:\n",
    "    await consumes_messages(\n",
    "        topic=topic,\n",
    "        msgs_count=msgs_count,\n",
    "        auto_offset_reset=\"earliest\",\n",
    "#         group_id=\"test_group\",\n",
    "        **aiokafka_config,\n",
    "    )\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Send repeatedly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mock_AIOKafkaProducer_send() -> Generator[unittest.mock.Mock, None, None]:\n",
    "    \"\"\"Mocks **send** method of **AIOKafkaProducer**\"\"\"\n",
    "    with unittest.mock.patch(\"__main__.AIOKafkaProducer.send\") as mock:\n",
    "\n",
    "        async def _f():\n",
    "            pass\n",
    "\n",
    "        mock.return_value = asyncio.create_task(_f())\n",
    "\n",
    "        yield mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def change_dir(d: str) -> Generator[None, None, None]:\n",
    "    curdir = os.getcwd()\n",
    "    os.chdir(d)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1eac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tests\n",
    "with TemporaryDirectory() as d:\n",
    "    original_wd = os.getcwd()\n",
    "    assert original_wd != d\n",
    "    with change_dir(d):\n",
    "        assert os.getcwd() == d\n",
    "    assert os.getcwd() == original_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a031f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def run_script_and_cancel(\n",
    "    *, script: str, script_file: str, cmd: str, cancel_after: int\n",
    ") -> Tuple[int, bytes]:\n",
    "    with TemporaryDirectory() as d:\n",
    "        consumer_script = Path(d) / script_file\n",
    "\n",
    "        with open(consumer_script, \"a+\") as file:\n",
    "            file.write(script)\n",
    "\n",
    "        # os.chdir(d)\n",
    "        with change_dir(d):\n",
    "            proc = subprocess.Popen(  # nosec: [B603:subprocess_without_shell_equals_true] subprocess call - check for execution of untrusted input.\n",
    "                shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
    "            )\n",
    "            time.sleep(cancel_after)\n",
    "            proc.terminate()\n",
    "            output, _ = proc.communicate()\n",
    "\n",
    "        return (proc.returncode, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f359e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "cmd = \"python3 -m test.py\"\n",
    "\n",
    "# Check exit code 0\n",
    "script = \"exit(0)\"\n",
    "\n",
    "exit_code, output = run_script_and_cancel(\n",
    "    script=script, script_file=\"test.py\", cmd=cmd, cancel_after=1\n",
    ")\n",
    "\n",
    "assert exit_code == 0\n",
    "assert output.decode(\"utf-8\") == \"\"\n",
    "\n",
    "\n",
    "# Check exit code 1\n",
    "script = \"exit(1)\"\n",
    "\n",
    "exit_code, output = run_script_and_cancel(\n",
    "    script=script, script_file=\"test.py\", cmd=cmd, cancel_after=1\n",
    ")\n",
    "\n",
    "assert exit_code == 1\n",
    "assert output.decode(\"utf-8\") == \"\"\n",
    "\n",
    "\n",
    "# Check exit code 0 and output to stdout and stderr\n",
    "script = \"\"\"\n",
    "import sys\n",
    "sys.stderr.write(\"hello from stderr\\\\n\")\n",
    "sys.stderr.flush()\n",
    "print(\"hello, exiting with exit code 0\")\n",
    "exit(0)\n",
    "\"\"\"\n",
    "\n",
    "exit_code, output = run_script_and_cancel(\n",
    "    script=script, script_file=\"test.py\", cmd=cmd, cancel_after=1\n",
    ")\n",
    "\n",
    "assert exit_code == 0, exit_code\n",
    "assert output.decode(\"utf-8\") == \"hello from stderr\\nhello, exiting with exit code 0\\n\", output.decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Check random exit code and output\n",
    "script = \"\"\"\n",
    "print(\"hello\\\\nexiting with exit code 143\")\n",
    "exit(143)\n",
    "\"\"\"\n",
    "\n",
    "exit_code, output = run_script_and_cancel(\n",
    "    script=script, script_file=\"test.py\", cmd=cmd, cancel_after=1\n",
    ")\n",
    "\n",
    "assert exit_code == 143\n",
    "assert output.decode(\"utf-8\") == \"hello\\nexiting with exit code 143\\n\"\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be219d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
