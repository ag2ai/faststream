{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc959176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import asyncio\n",
    "import contextlib\n",
    "import hashlib\n",
    "import os\n",
    "import random\n",
    "import shlex\n",
    "import multiprocessing\n",
    "from collections import namedtuple\n",
    "import functools\n",
    "\n",
    "# [B404:blacklist] Consider possible security implications associated with the subprocess module.\n",
    "import requests\n",
    "import shutil\n",
    "import signal\n",
    "import subprocess  # nosec\n",
    "import textwrap\n",
    "import time\n",
    "import typer\n",
    "import unittest\n",
    "import unittest.mock\n",
    "from contextlib import asynccontextmanager, contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import *\n",
    "import glob\n",
    "from unittest.mock import AsyncMock, MagicMock\n",
    "\n",
    "import asyncer\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.foundation import patch\n",
    "from pydantic import BaseModel, Field\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "import posix_ipc\n",
    "import nest_asyncio\n",
    "\n",
    "# from fastkafka.server import _import_from_string\n",
    "from fastkafka._components.helpers import combine_params, use_parameters_of\n",
    "from fastkafka._components.logger import get_logger, supress_timestamps\n",
    "from fastkafka.helpers import (\n",
    "    consumes_messages,\n",
    "    create_admin_client,\n",
    "    create_missing_topics,\n",
    "    in_notebook,\n",
    "    tqdm,\n",
    "    trange,\n",
    "    produce_messages,\n",
    ")\n",
    "from fastkafka._components._subprocess import terminate_asyncio_process\n",
    "from fastkafka.application import FastKafka, filter_using_signature\n",
    "from fastkafka._components.helpers import _import_from_string\n",
    "from fastkafka.helpers import in_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ce24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "if in_notebook():\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "else:\n",
    "    from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "\n",
    "import anyio\n",
    "import pytest\n",
    "from nbdev_mkdocs.docstring import run_examples_from_docstring\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from fastkafka.helpers import (\n",
    "    consumes_messages,\n",
    "    produce_messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "# allows async calls in notebooks\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2eb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3eee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "supress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6902fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "kafka_server_url = (\n",
    "    os.environ[\"KAFKA_HOSTNAME\"] if \"KAFKA_HOSTNAME\" in os.environ else \"localhost\"\n",
    ")\n",
    "kafka_server_port = os.environ[\"KAFKA_PORT\"] if \"KAFKA_PORT\" in os.environ else \"9092\"\n",
    "\n",
    "aiokafka_config = {\n",
    "    \"bootstrap_servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f69103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def nb_safe_seed(s: str) -> Callable[[int], int]:\n",
    "    \"\"\"Gets a unique seed function for a notebook\n",
    "\n",
    "    Params:\n",
    "        s: name of the notebook used to initialize the seed function\n",
    "\n",
    "    Returns:\n",
    "        A unique seed function\n",
    "    \"\"\"\n",
    "    init_seed = int(hashlib.sha256(s.encode(\"utf-8\")).hexdigest(), 16) % (10**8)\n",
    "\n",
    "    def _get_seed(x: int = 0, *, init_seed: int = init_seed) -> int:\n",
    "        return init_seed + x\n",
    "\n",
    "    return _get_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = nb_safe_seed(\"999_test_utils\")\n",
    "\n",
    "assert seed() == seed(0)\n",
    "assert seed() + 1 == seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf46a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def true_after(seconds: float) -> Callable[[], bool]:\n",
    "    \"\"\"Function returning True after a given number of seconds\"\"\"\n",
    "    t = datetime.now()\n",
    "\n",
    "    def _true_after(seconds: float = seconds, t: datetime = t) -> bool:\n",
    "        return (datetime.now() - t) > timedelta(seconds=seconds)\n",
    "\n",
    "    return _true_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac939ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = true_after(1.1)\n",
    "assert not f()\n",
    "time.sleep(1)\n",
    "assert not f()\n",
    "time.sleep(0.1)\n",
    "assert f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed690d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "@delegates(create_missing_topics)  # type: ignore\n",
    "def create_testing_topic(\n",
    "    *,\n",
    "    topic_prefix: str = \"test_topic_\",\n",
    "    seed: Optional[int] = None,\n",
    "    **kwargs: Dict[str, Any],\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"Create testing topic\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        from os import environ\n",
    "        from fastkafka.testing import create_testing_topic, create_admin_client\n",
    "\n",
    "        kafka_server_url = environ[\"KAFKA_HOSTNAME\"]\n",
    "        aiokafka_config = {\"bootstrap_servers\": f\"{kafka_server_url}:9092\"}\n",
    "\n",
    "        with create_testing_topic(\n",
    "            topic_prefix=\"my_topic_for_create_testing_topic_\",\n",
    "            seed=746855,\n",
    "            num_partitions=1,\n",
    "            **aiokafka_config\n",
    "        ) as topic:\n",
    "            # Check if topic is created and exists in topic list\n",
    "            kafka_admin = create_admin_client(**aiokafka_config)\n",
    "            existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "            assert topic in existing_topics\n",
    "\n",
    "        # Check if topic is deleted after exiting context\n",
    "        existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "        assert topic not in existing_topics\n",
    "        ```\n",
    "\n",
    "    Args:\n",
    "        topic_prefix: topic name prefix which will be augumented with a randomly generated sufix\n",
    "        seed: seed used to generate radnom sufix\n",
    "        topic_names: a list of topic names\n",
    "        num_partitions: Number of partitions to create\n",
    "        replication_factor: Replication factor of partitions, or -1 if replica_assignment is used.\n",
    "        replica_assignment: List of lists with the replication assignment for each new partition.\n",
    "        new_topic_config: topic level config parameters as defined here: https://kafka.apache.org/documentation.html#topicconfigs\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "            ``host[:port]`` strings that the producer should contact to\n",
    "            bootstrap initial cluster metadata. This does not have to be the\n",
    "            full node list.  It just needs to have at least one broker that will\n",
    "            respond to a Metadata API Request. Default port is 9092. If no\n",
    "            servers are specified, will default to ``localhost:9092``.\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "            Default: ``PLAINTEXT``.\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "            are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "\n",
    "    Returns:\n",
    "        Generator returning the generated name of the created topic\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # create random topic name\n",
    "    random.seed(seed)\n",
    "    # [B311:blacklist] Standard pseudo-random generators are not suitable for security/cryptographic purposes.\n",
    "    suffix = str(random.randint(0, 10**10))  # nosec\n",
    "\n",
    "    topic = topic_prefix + suffix.zfill(3)\n",
    "\n",
    "    # delete topic if it already exists\n",
    "    admin = create_admin_client(**kwargs)  # type: ignore\n",
    "    existing_topics = admin.list_topics().topics.keys()\n",
    "    if topic in existing_topics:\n",
    "        logger.warning(f\"topic {topic} exists, deleting it...\")\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        results = {k: f.result() for k, f in fs.items()}\n",
    "        while topic in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)\n",
    "    try:\n",
    "        # create topic if needed\n",
    "        create_missing_topics([topic], **kwargs)\n",
    "        while topic not in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)\n",
    "        yield topic\n",
    "\n",
    "    finally:\n",
    "        pass\n",
    "        # cleanup if needed again\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        while topic in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(create_testing_topic, create_missing_topics).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c859e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n",
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n",
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n",
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace;font-size:.68rem\">Example:\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────── </span>code<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────────────────────────</span>\n",
       "\n",
       "    from os import environ\n",
       "    from fastkafka.testing import create_testing_topic, create_admin_client\n",
       "\n",
       "    kafka_server_url = environ<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"KAFKA_HOSTNAME\"</span><span style=\"font-weight: bold\">]</span>\n",
       "    aiokafka_config = <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"bootstrap_servers\"</span>: f\"<span style=\"font-weight: bold\">{</span>kafka_server_url<span style=\"font-weight: bold\">}</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9092</span>\"<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "    with <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_testing_topic</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">topic_prefix</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"my_topic_for_create_testing_topic_\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">seed</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">746855</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">num_partitions</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        **aiokafka_config\n",
       "    <span style=\"font-weight: bold\">)</span> as topic:\n",
       "        # Check if topic is created and exists in topic list\n",
       "        kafka_admin = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_admin_client</span><span style=\"font-weight: bold\">(</span>**aiokafka_config<span style=\"font-weight: bold\">)</span>\n",
       "        existing_topics = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">kafka_admin.list_topics</span><span style=\"font-weight: bold\">()</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.topics.keys</span><span style=\"font-weight: bold\">()</span>\n",
       "        assert topic in existing_topics\n",
       "\n",
       "    # Check if topic is deleted after exiting context\n",
       "    existing_topics = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">kafka_admin.list_topics</span><span style=\"font-weight: bold\">()</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.topics.keys</span><span style=\"font-weight: bold\">()</span>\n",
       "    assert topic not in existing_topics\n",
       "\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────── </span>stdout supressed<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────</span>\n",
       "N/A\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────────────── </span>stderr<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Example:\n",
       "\u001b[92m───────────────────────────────────── \u001b[0mcode\u001b[92m ─────────────────────────────────────\u001b[0m\n",
       "\n",
       "    from os import environ\n",
       "    from fastkafka.testing import create_testing_topic, create_admin_client\n",
       "\n",
       "    kafka_server_url = environ\u001b[1m[\u001b[0m\u001b[32m\"KAFKA_HOSTNAME\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "    aiokafka_config = \u001b[1m{\u001b[0m\u001b[32m\"bootstrap_servers\"\u001b[0m: f\"\u001b[1m{\u001b[0mkafka_server_url\u001b[1m}\u001b[0m:\u001b[1;36m9092\u001b[0m\"\u001b[1m}\u001b[0m\n",
       "\n",
       "    with \u001b[1;35mcreate_testing_topic\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mtopic_prefix\u001b[0m=\u001b[32m\"my_topic_for_create_testing_topic_\"\u001b[0m,\n",
       "        \u001b[33mseed\u001b[0m=\u001b[1;36m746855\u001b[0m,\n",
       "        \u001b[33mnum_partitions\u001b[0m=\u001b[1;36m1\u001b[0m,\n",
       "        **aiokafka_config\n",
       "    \u001b[1m)\u001b[0m as topic:\n",
       "        # Check if topic is created and exists in topic list\n",
       "        kafka_admin = \u001b[1;35mcreate_admin_client\u001b[0m\u001b[1m(\u001b[0m**aiokafka_config\u001b[1m)\u001b[0m\n",
       "        existing_topics = \u001b[1;35mkafka_admin.list_topics\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1;35m.topics.keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        assert topic in existing_topics\n",
       "\n",
       "    # Check if topic is deleted after exiting context\n",
       "    existing_topics = \u001b[1;35mkafka_admin.list_topics\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1;35m.topics.keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    assert topic not in existing_topics\n",
       "\n",
       "\u001b[92m─────────────────────────────── \u001b[0mstdout supressed\u001b[92m ───────────────────────────────\u001b[0m\n",
       "N/A\n",
       "\u001b[92m──────────────────────────────────── \u001b[0mstderr\u001b[92m ────────────────────────────────────\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_examples_from_docstring(create_testing_topic(width=120), supress_stdout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "@delegates(produce_messages)  # type: ignore\n",
    "@delegates(create_testing_topic, keep=True)  # type: ignore\n",
    "async def create_and_fill_testing_topic(**kwargs: Dict[str, str]) -> AsyncIterator[str]:\n",
    "    \"\"\"Create testing topic with a random sufix in the same and fill it will messages\n",
    "\n",
    "    Args:\n",
    "        topic_names: a list of topic names\n",
    "        num_partitions: Number of partitions to create\n",
    "        replication_factor: Replication factor of partitions, or -1 if replica_assignment is used.\n",
    "        replica_assignment: List of lists with the replication assignment for each new partition.\n",
    "        new_topic_config: topic level config parameters as defined here: https://kafka.apache.org/documentation.html#topicconfigs\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "            ``host[:port]`` strings that the producer should contact to\n",
    "            bootstrap initial cluster metadata. This does not have to be the\n",
    "            full node list.  It just needs to have at least one broker that will\n",
    "            respond to a Metadata API Request. Default port is 9092. If no\n",
    "            servers are specified, will default to ``localhost:9092``.\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "            Default: ``PLAINTEXT``.\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "            are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        topic: Topic name\n",
    "        msgs: a list of messages to produce\n",
    "        client_id (str): a name for this client. This string is passed in\n",
    "            each request to servers and can be used to identify specific\n",
    "            server-side log entries that correspond to this client.\n",
    "            Default: ``aiokafka-producer-#`` (appended with a unique number\n",
    "            per instance)\n",
    "        key_serializer (Callable): used to convert user-supplied keys to bytes\n",
    "            If not :data:`None`, called as ``f(key),`` should return\n",
    "            :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        value_serializer (Callable): used to convert user-supplied message\n",
    "            values to :class:`bytes`. If not :data:`None`, called as\n",
    "            ``f(value)``, should return :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        acks (Any): one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
    "            the producer requires the leader to have received before considering a\n",
    "            request complete. This controls the durability of records that are\n",
    "            sent. The following settings are common:\n",
    "\n",
    "            * ``0``: Producer will not wait for any acknowledgment from the server\n",
    "              at all. The message will immediately be added to the socket\n",
    "              buffer and considered sent. No guarantee can be made that the\n",
    "              server has received the record in this case, and the retries\n",
    "              configuration will not take effect (as the client won't\n",
    "              generally know of any failures). The offset given back for each\n",
    "              record will always be set to -1.\n",
    "            * ``1``: The broker leader will write the record to its local log but\n",
    "              will respond without awaiting full acknowledgement from all\n",
    "              followers. In this case should the leader fail immediately\n",
    "              after acknowledging the record but before the followers have\n",
    "              replicated it then the record will be lost.\n",
    "            * ``all``: The broker leader will wait for the full set of in-sync\n",
    "              replicas to acknowledge the record. This guarantees that the\n",
    "              record will not be lost as long as at least one in-sync replica\n",
    "              remains alive. This is the strongest available guarantee.\n",
    "\n",
    "            If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
    "            :data:`True` defaults to ``acks=all``\n",
    "        compression_type (str): The compression type for all data generated by\n",
    "            the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
    "            or :data:`None`.\n",
    "            Compression is of full batches of data, so the efficacy of batching\n",
    "            will also impact the compression ratio (more batching means better\n",
    "            compression). Default: :data:`None`.\n",
    "        max_batch_size (int): Maximum size of buffered data per partition.\n",
    "            After this amount :meth:`send` coroutine will block until batch is\n",
    "            drained.\n",
    "            Default: 16384\n",
    "        linger_ms (int): The producer groups together any records that arrive\n",
    "            in between request transmissions into a single batched request.\n",
    "            Normally this occurs only under load when records arrive faster\n",
    "            than they can be sent out. However in some circumstances the client\n",
    "            may want to reduce the number of requests even under moderate load.\n",
    "            This setting accomplishes this by adding a small amount of\n",
    "            artificial delay; that is, if first request is processed faster,\n",
    "            than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
    "            Default: 0 (i.e. no delay).\n",
    "        partitioner (Callable): Callable used to determine which partition\n",
    "            each message is assigned to. Called (after key serialization):\n",
    "            ``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
    "            The default partitioner implementation hashes each non-None key\n",
    "            using the same murmur2 algorithm as the Java client so that\n",
    "            messages with the same key are assigned to the same partition.\n",
    "            When a key is :data:`None`, the message is delivered to a random partition\n",
    "            (filtered to partitions with available leaders only, if possible).\n",
    "        max_request_size (int): The maximum size of a request. This is also\n",
    "            effectively a cap on the maximum record size. Note that the server\n",
    "            has its own cap on record size which may be different from this.\n",
    "            This setting will limit the number of record batches the producer\n",
    "            will send in a single request to avoid sending huge requests.\n",
    "            Default: 1048576.\n",
    "        metadata_max_age_ms (int): The period of time in milliseconds after\n",
    "            which we force a refresh of metadata even if we haven't seen any\n",
    "            partition leadership changes to proactively discover any new\n",
    "            brokers or partitions. Default: 300000\n",
    "        request_timeout_ms (int): Produce request timeout in milliseconds.\n",
    "            As it's sent as part of\n",
    "            :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
    "            call), maximum waiting time can be up to ``2 *\n",
    "            request_timeout_ms``.\n",
    "            Default: 40000.\n",
    "        retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
    "            errors. Default: 100.\n",
    "        api_version (str): specify which kafka API version to use.\n",
    "            If set to ``auto``, will attempt to infer the broker version by\n",
    "            probing various APIs. Default: ``auto``\n",
    "        ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
    "            for wrapping socket connections. Directly passed into asyncio's\n",
    "            :meth:`~asyncio.loop.create_connection`. For more\n",
    "            information see :ref:`ssl_auth`.\n",
    "            Default: :data:`None`\n",
    "        connections_max_idle_ms (int): Close idle connections after the number\n",
    "            of milliseconds specified by this config. Specifying :data:`None` will\n",
    "            disable idle checks. Default: 540000 (9 minutes).\n",
    "        enable_idempotence (bool): When set to :data:`True`, the producer will\n",
    "            ensure that exactly one copy of each message is written in the\n",
    "            stream. If :data:`False`, producer retries due to broker failures,\n",
    "            etc., may write duplicates of the retried message in the stream.\n",
    "            Note that enabling idempotence acks to set to ``all``. If it is not\n",
    "            explicitly set by the user it will be chosen. If incompatible\n",
    "            values are set, a :exc:`ValueError` will be thrown.\n",
    "            New in version 0.5.0.\n",
    "        sasl_oauth_token_provider (: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
    "            OAuthBearer token provider instance. (See\n",
    "            :mod:`kafka.oauth.abstract`).\n",
    "            Default: :data:`None`\n",
    "    \"\"\"\n",
    "\n",
    "    with create_testing_topic(\n",
    "        **use_parameters_of(create_testing_topic, **kwargs)\n",
    "    ) as topic:\n",
    "        await produce_messages(\n",
    "            topic=topic, **use_parameters_of(produce_messages, **kwargs)\n",
    "        )\n",
    "\n",
    "        yield topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0930893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(combine_params(create_and_fill_testing_topic, create_missing_topics), produce_messages).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00149fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka.helpers: create_missing_topics(['my_topic_test_create_and_fill_testing_topic_9167024629']): new_topics = [NewTopic(topic=my_topic_test_create_and_fill_testing_topic_9167024629,num_partitions=3)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e02642a28345299c77a47b2b8f72c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "producing to 'my_topic_test_create_and_fill_testing_topic_9167024629':   0%|          | 0/120000 [00:00<?, ?it…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_test_create_and_fill_testing_topic_9167024629'})\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_test_create_and_fill_testing_topic_9167024629': 3}. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e53ebc1c92417d8bf137ced58ba7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consuming from 'my_topic_test_create_and_fill_testing_topic_9167024629':   0%|          | 0/120000 [00:00<?, ?…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "msgs_count = 120_000\n",
    "\n",
    "\n",
    "class Hello(BaseModel):\n",
    "    msg: str\n",
    "\n",
    "\n",
    "msgs_count = 120_000\n",
    "msgs = (\n",
    "    [b\"Hello world bytes\" for _ in range(msgs_count // 3)]\n",
    "    + [f\"Hello world as string for the {i+1}. time!\" for i in range(msgs_count // 3)]\n",
    "    + [\n",
    "        Hello(msg=\"Hello workd as Pydantic object for the {i+1}. time!\")\n",
    "        for i in range(msgs_count // 3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "async with create_and_fill_testing_topic(\n",
    "    topic_prefix=\"my_topic_test_create_and_fill_testing_topic_\",\n",
    "    msgs=msgs,\n",
    "    seed=1,\n",
    "    **aiokafka_config,\n",
    ") as topic:\n",
    "    await consumes_messages(\n",
    "        topic=topic,\n",
    "        msgs_count=msgs_count,\n",
    "        auto_offset_reset=\"earliest\",\n",
    "        #         group_id=\"test_group\",\n",
    "        **aiokafka_config,\n",
    "    )\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mock_AIOKafkaProducer_send() -> Generator[unittest.mock.Mock, None, None]:\n",
    "    \"\"\"Mocks **send** method of **AIOKafkaProducer**\"\"\"\n",
    "    with unittest.mock.patch(\"__main__.AIOKafkaProducer.send\") as mock:\n",
    "\n",
    "        async def _f():\n",
    "            pass\n",
    "\n",
    "        mock.return_value = asyncio.create_task(_f())\n",
    "\n",
    "        yield mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def change_dir(d: str) -> Generator[None, None, None]:\n",
    "    curdir = os.getcwd()\n",
    "    os.chdir(d)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1eac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tests\n",
    "with TemporaryDirectory() as d:\n",
    "    original_wd = os.getcwd()\n",
    "    assert original_wd != d\n",
    "    with change_dir(d):\n",
    "        assert os.getcwd() == d\n",
    "    assert os.getcwd() == original_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85420005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "async def run_script_and_cancel(\n",
    "    script: str,\n",
    "    *,\n",
    "    script_file: Optional[str] = None,\n",
    "    cmd: Optional[str] = None,\n",
    "    cancel_after: int = 10,\n",
    "    app_name: str = \"app\",\n",
    "    kafka_app_name: str = \"kafka_app\",\n",
    "    generate_docs: bool = False,\n",
    ") -> Tuple[int, bytes]:\n",
    "    \"\"\"Run script and cancel after predefined time\n",
    "\n",
    "    Args:\n",
    "        script: a python source code to be executed in a separate subprocess\n",
    "        script_file: name of the script where script source will be saved\n",
    "        cmd: command to execute. If None, it will be set to 'python3 -m {Path(script_file).stem}'\n",
    "        cancel_after: number of seconds before sending SIGTERM signal\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing exit code and combined stdout and stderr as a binary string\n",
    "    \"\"\"\n",
    "    if script_file is None:\n",
    "        script_file = \"script.py\"\n",
    "\n",
    "    if cmd is None:\n",
    "        cmd = f\"python3 -m {Path(script_file).stem}\"\n",
    "\n",
    "    with TemporaryDirectory() as d:\n",
    "        consumer_script = Path(d) / script_file\n",
    "\n",
    "        with open(consumer_script, \"w\") as file:\n",
    "            file.write(script)\n",
    "\n",
    "        if generate_docs:\n",
    "            logger.info(\n",
    "                f\"Generating docs for: {Path(script_file).stem}:{kafka_app_name}\"\n",
    "            )\n",
    "            try:\n",
    "                kafka_app: FastKafka = _import_from_string(\n",
    "                    f\"{Path(script_file).stem}:{kafka_app_name}\"\n",
    "                )\n",
    "                await asyncer.asyncify(kafka_app.create_docs)()\n",
    "            except Exception as e:\n",
    "                logger.warning(\n",
    "                    f\"Generating docs failed for: {Path(script_file).stem}:{kafka_app_name}, ignoring it for now.\"\n",
    "                )\n",
    "\n",
    "        proc = subprocess.Popen(  # nosec: [B603:subprocess_without_shell_equals_true] subprocess call - check for execution of untrusted input.\n",
    "            shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=d\n",
    "        )\n",
    "        await asyncio.sleep(cancel_after)\n",
    "        proc.terminate()\n",
    "        output, _ = proc.communicate()\n",
    "\n",
    "        return (proc.returncode, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09054da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exit code 0\n",
    "script = \"\"\"\n",
    "from time import sleep\n",
    "print(\"hello\")\n",
    "sleep({t})\n",
    "\"\"\"\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script.format(t=0), cancel_after=2)\n",
    "assert exit_code == 0, exit_code\n",
    "assert output.decode(\"utf-8\") == \"hello\\n\", output.decode(\"utf-8\")\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script.format(t=5), cancel_after=2)\n",
    "assert exit_code < 0, exit_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8484fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exit code 1\n",
    "script = \"exit(1)\"\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script, cancel_after=1)\n",
    "\n",
    "assert exit_code == 1\n",
    "assert output.decode(\"utf-8\") == \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aaf329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exit code 0 and output to stdout and stderr\n",
    "script = \"\"\"\n",
    "import sys\n",
    "sys.stderr.write(\"hello from stderr\\\\n\")\n",
    "sys.stderr.flush()\n",
    "print(\"hello, exiting with exit code 0\")\n",
    "exit(0)\n",
    "\"\"\"\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script, cancel_after=1)\n",
    "\n",
    "assert exit_code == 0, exit_code\n",
    "assert (\n",
    "    output.decode(\"utf-8\") == \"hello from stderr\\nhello, exiting with exit code 0\\n\"\n",
    "), output.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346af63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Check random exit code and output\n",
    "script = \"\"\"\n",
    "print(\"hello\\\\nexiting with exit code 143\")\n",
    "exit(143)\n",
    "\"\"\"\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script, cancel_after=1)\n",
    "\n",
    "assert exit_code == 143\n",
    "assert output.decode(\"utf-8\") == \"hello\\nexiting with exit code 143\\n\"\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fded319",
   "metadata": {},
   "source": [
    "### Local Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552eb74",
   "metadata": {},
   "source": [
    "#### Kafka and zookeeper config helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def get_zookeeper_config_string(\n",
    "    data_dir: Union[str, Path], # the directory where the snapshot is stored.\n",
    "    zookeeper_port: int = 2181, # the port at which the clients will connect\n",
    ") -> str:\n",
    "    \"\"\"Generates a zookeeeper configuration string that can be exported to file \n",
    "    and used to start a zookeeper instance.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "        zookeeper_port: Port for clients (Kafka brokes) to connect \n",
    "    Returns:\n",
    "        Zookeeper configuration string.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    zookeeper_config = f\"\"\"dataDir={data_dir}/zookeeper\n",
    "clientPort={zookeeper_port}\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    "\n",
    "    return zookeeper_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bbd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_zookeeper_config_string(data_dir=\"..\") == \"\"\"dataDir=../zookeeper\n",
    "clientPort=2181\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    "\n",
    "assert get_zookeeper_config_string(data_dir=\"..\", zookeeper_port = 100) == \"\"\"dataDir=../zookeeper\n",
    "clientPort=100\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d393bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def get_kafka_config_string(\n",
    "    data_dir: Union[str, Path],\n",
    "    zookeeper_port: int = 2181,\n",
    "    listener_port: int = 9092\n",
    ") -> str:\n",
    "    \"\"\"Generates a kafka broker configuration string that can be exported to file \n",
    "    and used to start a kafka broker instance.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory where the kafka broker instance will save data\n",
    "        zookeeper_port: Port on which the zookeeper instance is running\n",
    "        listener_port: Port on which the clients (producers and consumers) can connect\n",
    "    Returns:\n",
    "        Kafka broker configuration string.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    kafka_config = f\"\"\"broker.id=0\n",
    "\n",
    "############################# Socket Server Settings #############################\n",
    "\n",
    "# The address the socket server listens on. If not configured, the host name will be equal to the value of\n",
    "# java.net.InetAddress.getCanonicalHostName(), with PLAINTEXT listener name, and port 9092.\n",
    "#   FORMAT:\n",
    "#     listeners = listener_name://host_name:port\n",
    "#   EXAMPLE:\n",
    "#     listeners = PLAINTEXT://your.host.name:9092\n",
    "listeners=PLAINTEXT://:{listener_port}\n",
    "\n",
    "# Listener name, hostname and port the broker will advertise to clients.\n",
    "# If not set, it uses the value for \"listeners\".\n",
    "#advertised.listeners=PLAINTEXT://your.host.name:9092\n",
    "\n",
    "# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details\n",
    "#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n",
    "\n",
    "# The number of threads that the server uses for receiving requests from the network and sending responses to the network\n",
    "num.network.threads=3\n",
    "\n",
    "# The number of threads that the server uses for processing requests, which may include disk I/O\n",
    "num.io.threads=8\n",
    "\n",
    "# The send buffer (SO_SNDBUF) used by the socket server\n",
    "socket.send.buffer.bytes=102400\n",
    "\n",
    "# The receive buffer (SO_RCVBUF) used by the socket server\n",
    "socket.receive.buffer.bytes=102400\n",
    "\n",
    "# The maximum size of a request that the socket server will accept (protection against OOM)\n",
    "socket.request.max.bytes=104857600\n",
    "\n",
    "\n",
    "############################# Log Basics #############################\n",
    "\n",
    "# A comma separated list of directories under which to store log files\n",
    "log.dirs={data_dir}/kafka_logs\n",
    "\n",
    "# The default number of log partitions per topic. More partitions allow greater\n",
    "# parallelism for consumption, but this will also result in more files across\n",
    "# the brokers.\n",
    "num.partitions=1\n",
    "\n",
    "# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n",
    "# This value is recommended to be increased for installations with data dirs located in RAID array.\n",
    "num.recovery.threads.per.data.dir=1\n",
    "\n",
    "offsets.topic.replication.factor=1\n",
    "transaction.state.log.replication.factor=1\n",
    "transaction.state.log.min.isr=1\n",
    "\n",
    "# The number of messages to accept before forcing a flush of data to disk\n",
    "log.flush.interval.messages=10000\n",
    "\n",
    "# The maximum amount of time a message can sit in a log before we force a flush\n",
    "log.flush.interval.ms=1000\n",
    "\n",
    "# The minimum age of a log file to be eligible for deletion due to age\n",
    "log.retention.hours=168\n",
    "\n",
    "# A size-based retention policy for logs. Segments are pruned from the log unless the remaining\n",
    "# segments drop below log.retention.bytes. Functions independently of log.retention.hours.\n",
    "log.retention.bytes=1073741824\n",
    "\n",
    "# The maximum size of a log segment file. When this size is reached a new log segment will be created.\n",
    "log.segment.bytes=1073741824\n",
    "\n",
    "# The interval at which log segments are checked to see if they can be deleted according to the retention policies\n",
    "log.retention.check.interval.ms=300000\n",
    "\n",
    "# Zookeeper connection string (see zookeeper docs for details).\n",
    "zookeeper.connect=localhost:{zookeeper_port}\n",
    "\n",
    "# Timeout in ms for connecting to zookeeper\n",
    "zookeeper.connection.timeout.ms=18000\n",
    "\n",
    "# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.\n",
    "group.initial.rebalance.delay.ms=0\n",
    "\"\"\"\n",
    "\n",
    "    return kafka_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = get_kafka_config_string(data_dir=\"..\", listener_port=9999)\n",
    "assert \"log.dirs=../kafka_logs\" in actual\n",
    "assert \"listeners=PLAINTEXT://:9999\" in actual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c114df",
   "metadata": {},
   "source": [
    "#### LocalKafkaBroker class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf939a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class LocalKafkaBroker:\n",
    "    \"\"\"LocalKafkaBroker class, used for running unique kafka brokers in tests to prevent topic clashing.\n",
    "    \n",
    "    Attributes:\n",
    "        lock (ilock.Lock): Lock used for synchronizing the install process between multiple kafka brokers.\n",
    "    \"\"\"\n",
    "    lock = posix_ipc.Semaphore('install_lock:LocalKafkaBroker', posix_ipc.O_CREAT, initial_value=1)\n",
    "    \n",
    "    @delegates(get_kafka_config_string) # type: ignore\n",
    "    @delegates(get_zookeeper_config_string, keep=True) # type: ignore\n",
    "    def __init__(self, **kwargs: Dict[str, Any]):\n",
    "        \"\"\" Initialises the LocalKafkaBroker object\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "            zookeeper_port: Port for clients (Kafka brokes) to connect\n",
    "            listener_port: Port on which the clients (producers and consumers) can connect\n",
    "        \"\"\"\n",
    "        self.zookeeper_kwargs = filter_using_signature(get_zookeeper_config_string, **kwargs)\n",
    "        self.kafka_kwargs = filter_using_signature(get_kafka_config_string, **kwargs)\n",
    "        self.temporary_directory: Optional[TemporaryDirectory] = None\n",
    "        self.temporary_directory_path: Optional[Path] = None\n",
    "        self.kafka_task: Optional[asyncio.subprocess.Process] = None\n",
    "        self.zookeeper_task: Optional[asyncio.subprocess.Process] = None\n",
    "        self.started = True\n",
    "        \n",
    "    @classmethod\n",
    "    def _install(cls) -> None:\n",
    "        \"\"\"Prepares the environment for running Kafka brokers.\n",
    "            Returns:\n",
    "               None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start(self) -> str:\n",
    "        \"\"\"Starts a local kafka broker and zookeeper instance asynchronously\n",
    "            Returns:\n",
    "               Kafka broker bootstrap server address in string format: add:port\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def start(self) -> str:\n",
    "        \"\"\"Starts a local kafka broker and zookeeper instance synchronously\n",
    "            Returns:\n",
    "               Kafka broker bootstrap server address in string format: add:port\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "            Returns:\n",
    "               None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _stop(self) -> None:\n",
    "        \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "            Returns:\n",
    "               None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __enter__(self) -> str:\n",
    "#         LocalKafkaBroker._install()\n",
    "        return self.start()\n",
    "\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        self.stop()\n",
    "        \n",
    "    async def __aenter__(self) -> str:\n",
    "#         LocalKafkaBroker._install()\n",
    "        return await self._start()\n",
    "    \n",
    "    async def __aexit__(self, *args, **kwargs):\n",
    "        await self._stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(combine_params(LocalKafkaBroker, get_kafka_config_string), get_zookeeper_config_string).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac507521",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883a730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "*   retval=0\n",
      "\n",
      "[INFO] [PID=26271]: Entering: 1676552880.7352087\n",
      "[INFO] [PID=26271]:  - 2023-02-16 13:08:00.735535\n",
      "[INFO] [PID=26271]: Exiting: 1676552881.736793\n",
      "[INFO] [PID=26271]:  - 2023-02-16 13:08:01.737037\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "*   retval=0\n",
      "\n",
      "[INFO] [PID=26272]: Entering: 1676552879.7336717\n",
      "[INFO] [PID=26272]:  - 2023-02-16 13:07:59.733745\n",
      "[INFO] [PID=26272]: Exiting: 1676552880.734803\n",
      "[INFO] [PID=26272]:  - 2023-02-16 13:08:00.734992\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "*   retval=0\n",
      "\n",
      "[INFO] [PID=26273]: Entering: 1676552881.737286\n",
      "[INFO] [PID=26273]:  - 2023-02-16 13:08:01.737587\n",
      "[INFO] [PID=26273]: Exiting: 1676552882.7388067\n",
      "[INFO] [PID=26273]:  - 2023-02-16 13:08:02.739049\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "script = \"\"\"\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "from fastkafka._components.logger import get_logger\n",
    "from fastkafka.testing import LocalKafkaBroker\n",
    "\n",
    "pid = os.getpid()\n",
    "\n",
    "logger = get_logger(f\"[PID={pid}]\")\n",
    "\n",
    "@patch(cls_method=True) # type: ignore\n",
    "def check_cls_lock(cls: LocalKafkaBroker) -> None:\n",
    "    with cls.lock:\n",
    "       logger.info(f\"Entering: {time.time()}\")\n",
    "       logger.info(f\" - {datetime.now()}\")\n",
    "       time.sleep(1)\n",
    "       logger.info(f\"Exiting: {time.time()}\")\n",
    "       logger.info(f\" - {datetime.now()}\")\n",
    "\n",
    "broker = LocalKafkaBroker()\n",
    "broker.check_cls_lock()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_times(stdout: str) -> Tuple[float, float]:\n",
    "    stdout_lines = stdout.split(\"\\n\")\n",
    "    enter_time = float([line.split(\" \")[-1] for line in stdout_lines if \"Entering\" in line][0])\n",
    "    exit_time = float([line.split(\" \")[-1] for line in stdout_lines if \"Exiting\" in line][0])\n",
    "    return (enter_time, exit_time)\n",
    "    \n",
    "\n",
    "def check_overlap(intervals: List[Tuple[float]]) -> bool:\n",
    "    for i, (test_start, test_stop) in enumerate(intervals):\n",
    "        for start, stop in intervals[i+1:]:\n",
    "            if (test_start < start < test_stop or start < test_start < stop):\n",
    "                return True\n",
    "    return False\n",
    "    \n",
    "async with asyncer.create_task_group() as tg:\n",
    "    tx = [tg.soonify(run_script_and_cancel)(script, cancel_after=5) for _ in range(3)]\n",
    "retvals, stdouts = zip(*[t.value for t in tx])\n",
    "for retval, stdout in zip(retvals, stdouts):\n",
    "    print(\"*\"*100)\n",
    "    print(f\"*   {retval=}\")\n",
    "    print()\n",
    "    print(stdout.decode(\"utf-8\"))\n",
    "    print()\n",
    "    \n",
    "times = [get_times(stdout.decode(\"utf-8\")) for stdout in stdouts]\n",
    "assert not check_overlap(times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7480229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def install_java() -> None:\n",
    "    \"\"\"Checks if jdk-11 is installed on the machine and installs it if not\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    potential_jdk_path = list(Path(os.environ[\"HOME\"]+\"/.jdk\").glob(\"jdk-11*\"))\n",
    "    if potential_jdk_path != []:\n",
    "        logger.info(\"Java is already installed.\")\n",
    "        if not shutil.which(\"java\"):\n",
    "            logger.info(\"But not exported to PATH, exporting...\")\n",
    "            os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{potential_jdk_path[0]}/bin\"\n",
    "    else:\n",
    "        logger.info(\"Installing Java...\")\n",
    "        logger.info(\" - installing install-jdk...\")\n",
    "        subprocess.run([\"pip\", \"install\", \"install-jdk\"], check=True) # nosec\n",
    "        import jdk\n",
    "\n",
    "        logger.info(\" - installing jdk...\")\n",
    "        jdk_bin_path = jdk.install(\"11\")\n",
    "        print(jdk_bin_path)\n",
    "        os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{jdk_bin_path}/bin\"\n",
    "        logger.info(\"Java installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4499887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: But not exported to PATH, exporting...\n",
      "[INFO] __main__: Java is already installed.\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "install_java()\n",
    "assert shutil.which(\"java\")\n",
    "install_java()\n",
    "assert shutil.which(\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a57f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def install_kafka() -> None:\n",
    "    \"\"\"Checks if kafka is installed on the machine and installs it if not\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    kafka_version = \"3.3.2\"\n",
    "    kafka_fname = f\"kafka_2.13-{kafka_version}\"\n",
    "    kafka_url = f\"https://dlcdn.apache.org/kafka/{kafka_version}/{kafka_fname}.tgz\"\n",
    "    local_path = Path(os.environ[\"HOME\"]) / \".local\"\n",
    "    local_path.mkdir(exist_ok=True, parents=True)\n",
    "    tgz_path = local_path / f\"{kafka_fname}.tgz\"\n",
    "    kafka_path = local_path / f\"{kafka_fname}\"\n",
    "    \n",
    "    if (kafka_path / \"bin\").exists():\n",
    "        logger.info(\"Kafka is already installed.\")\n",
    "        if not shutil.which(\"kafka-server-start.sh\"):\n",
    "            logger.info(\"But not exported to PATH, exporting...\")\n",
    "            os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{kafka_path}/bin\"\n",
    "    else:\n",
    "        logger.info(\"Installing Kafka...\")\n",
    "\n",
    "        response = requests.get(kafka_url, stream=True, )\n",
    "        try:\n",
    "            total = response.raw.length_remaining // 128\n",
    "        except Exception:\n",
    "            total = None\n",
    "\n",
    "        with open(tgz_path, \"wb\") as f:\n",
    "            for data in tqdm(response.iter_content(chunk_size=128), total=total):\n",
    "                f.write(data)\n",
    "\n",
    "        with tarfile.open(tgz_path) as tar:\n",
    "            for tarinfo in tar:\n",
    "                tar.extract(tarinfo, local_path)\n",
    "\n",
    "        os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{kafka_path}/bin\"\n",
    "        logger.info(f\"Kafka installed in {kafka_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: But not exported to PATH, exporting...\n",
      "[INFO] __main__: Kafka is already installed.\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "install_kafka()\n",
    "assert shutil.which(\"kafka-server-start.sh\")\n",
    "install_kafka()\n",
    "assert shutil.which(\"kafka-server-start.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167099d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "@patch(cls_method=True) # type: ignore\n",
    "def _install(cls: LocalKafkaBroker) -> None:\n",
    "    with cls.lock:\n",
    "        install_java()\n",
    "        install_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4600ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n"
     ]
    }
   ],
   "source": [
    "broker = LocalKafkaBroker()\n",
    "broker._install()\n",
    "assert shutil.which(\"java\")\n",
    "assert shutil.which(\"kafka-server-start.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a076ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _start(self: LocalKafkaBroker) -> str:\n",
    "    self._install()\n",
    "\n",
    "    self.temporary_directory = TemporaryDirectory()\n",
    "    self.temporary_directory_path = Path(self.temporary_directory.__enter__())\n",
    "\n",
    "    async def write_config_and_run(\n",
    "        config: str, config_path: Union[str, Path], run_cmd: str\n",
    "    ) -> asyncio.subprocess.Process:\n",
    "        with open(config_path, \"w\") as f:\n",
    "            f.write(config)\n",
    "\n",
    "        return await asyncio.create_subprocess_exec(\n",
    "            run_cmd,\n",
    "            config_path,\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stdin=asyncio.subprocess.PIPE,\n",
    "        )\n",
    "\n",
    "    # start_zookeeper\n",
    "\n",
    "    logger.info(\"Starting zookeeper...\")\n",
    "    zookeeper_config_path = self.temporary_directory_path / \"zookeeper.properties\"\n",
    "    self.zookeeper_task = await write_config_and_run(\n",
    "        get_zookeeper_config_string(\n",
    "            data_dir=self.temporary_directory_path, **self.zookeeper_kwargs\n",
    "        ),\n",
    "        zookeeper_config_path,\n",
    "        \"zookeeper-server-start.sh\",\n",
    "    )\n",
    "\n",
    "    logger.info(\"Zookeeper started, sleeping for 5 seconds...\")\n",
    "    await asyncio.sleep(5)\n",
    "    if self.zookeeper_task.returncode is not None:\n",
    "        raise ValueError(f\"Could not start zookeeper with params: {self.zookeeper_kwargs}\")\n",
    "\n",
    "    # start_kafka\n",
    "\n",
    "    logger.info(\"Starting Kafka broker...\")\n",
    "    kafka_config_path = self.temporary_directory_path / \"kafka.properties\"\n",
    "    self.kafka_task = await write_config_and_run(\n",
    "        get_kafka_config_string(\n",
    "            data_dir=self.temporary_directory_path, **self.kafka_kwargs\n",
    "        ),\n",
    "        kafka_config_path,\n",
    "        \"kafka-server-start.sh\",\n",
    "    )\n",
    "\n",
    "    logger.info(\"Kafka broker started, sleeping for 5 seconds...\")\n",
    "    await asyncio.sleep(5)\n",
    "    if self.kafka_task.returncode is not None:\n",
    "        raise ValueError(f\"Could not start Kafka broker with params: {self.kafka_kwargs}\")\n",
    "\n",
    "    listener_port = self.kafka_kwargs.get(\"listener_port\", 9092)\n",
    "    retval = f\"127.0.0.1:{listener_port}\"\n",
    "    logger.info(f\"Local Kafka broker up and running on {retval}\")\n",
    "    return retval\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _stop(self: LocalKafkaBroker) -> None:\n",
    "    await terminate_asyncio_process(self.kafka_task)  # type: ignore\n",
    "    await terminate_asyncio_process(self.zookeeper_task)  # type: ignore\n",
    "    self.temporary_directory.__exit__(None, None, None)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "[INFO] __main__: Zookeeper started, sleeping for 5 seconds...\n",
      "[INFO] __main__: Starting Kafka broker...\n",
      "[INFO] __main__: Kafka broker started, sleeping for 5 seconds...\n",
      "[INFO] __main__: Local Kafka broker up and running on 127.0.0.1:9789\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 26640...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 26640 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 26274...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 26274 terminated.\n",
      "**************************************************ZOOKEEPER LOGS++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[2023-02-16 13:08:13,320] INFO Reading configuration from: /tmp/tmpve9khxd3/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:13,328] INFO clientPortAddress is 0.0.0.0:9998 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:13,329] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:13,330] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:13,334] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:13,341] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2023-02-16 13:08:13,342] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2023-02-16 13:08:13,342] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2023-02-16 13:08:13,342] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)\n",
      "[2023-02-16 13:08:13,344] INFO Log4j 1.2 jmx support not found; jmx disabled. (org.apache.zookeeper.jmx.ManagedUtil)\n",
      "[2023-02-16 13:08:13,344] INFO Reading configuration from: /tmp/tmpve9khxd3/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:13,345] INFO clientPortAddress is 0.0.0.0:9998 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:13,345] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:13,345] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:13,346] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:13,346] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)\n",
      "[2023-02-16 13:08:13,372] INFO ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@865dd6 (org.apache.zookeeper.server.ServerMetrics)\n",
      "[2023-02-16 13:08:13,381] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2023-02-16 13:08:13,403] INFO  (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,403] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,403] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,404] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,404] INFO    / /    / _ \\   / _ \\  | |/ /  / _ \\  / _ \\ | '_ \\   / _ \\ | '__| (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,404] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,404] INFO  /_____|  \\___/   \\___/  |_|\\_\\  \\___|  \\___| | .__/   \\___| |_| (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,404] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,404] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,405] INFO  (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,406] INFO Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,406] INFO Server environment:host.name=tvrtko-fastkafka-devel (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,407] INFO Server environment:java.version=11.0.18 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,407] INFO Server environment:java.vendor=Eclipse Adoptium (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,408] INFO Server environment:java.home=/home/tvrtko/.jdk/jdk-11.0.18+10 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,408] INFO Server environment:java.class.path=/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/activation-1.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/argparse4j-0.7.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/audience-annotations-0.5.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-cli-1.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.12.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.8.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-basic-auth-extension-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-json-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-client-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-runtime-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-transforms-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-api-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-locator-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-utils-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-core-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-databind-2.13.4.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-dataformat-csv-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-datatype-jdk8-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-base-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-json-provider-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-jaxb-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-scala_2.13-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.activation-api-1.2.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.inject-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javassist-3.27.0-GA.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.servlet-api-3.1.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jaxb-api-2.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-client-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-common-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-core-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-hk2-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-server-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-client-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-continuation-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-http-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-io-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-security-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-server-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlet-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlets-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-ajax-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jline-3.21.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jopt-simple-5.0.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jose4j-0.7.9.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-clients-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-log4j-appender-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-metadata-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-raft-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-server-common-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-shell-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-examples-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-scala_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-test-utils-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-tools-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/lz4-java-1.8.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/maven-artifact-3.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-4.1.12.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-buffer-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-codec-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-handler-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-resolver-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-classes-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-unix-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/paranamer-2.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/plexus-utils-3.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reflections-0.9.12.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reload4j-1.2.19.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/rocksdbjni-7.1.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-collection-compat_2.13-2.6.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-library-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-logging_2.13-3.9.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-reflect-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-api-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-reload4j-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/snappy-java-1.1.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/swagger-annotations-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/trogdor-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-jute-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zstd-jni-1.5.2-1.jar (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,417] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,417] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,418] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,418] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,418] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,418] INFO Server environment:os.version=5.15.0-58-generic (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,418] INFO Server environment:user.name=tvrtko (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,419] INFO Server environment:user.home=/home/tvrtko (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,419] INFO Server environment:user.dir=/work/fastkafka/nbs (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,419] INFO Server environment:os.memory.free=490MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,420] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,420] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,420] INFO zookeeper.enableEagerACLCheck = false (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,420] INFO zookeeper.digest.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,420] INFO zookeeper.closeSessionTxn.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,420] INFO zookeeper.flushDelay=0 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,420] INFO zookeeper.maxWriteQueuePollTime=0 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,421] INFO zookeeper.maxBatchSize=1000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,421] INFO zookeeper.intBufferStartingSizeBytes = 1024 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,422] INFO Weighed connection throttling is disabled (org.apache.zookeeper.server.BlueThrottle)\n",
      "[2023-02-16 13:08:13,424] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,424] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,427] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)\n",
      "[2023-02-16 13:08:13,428] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)\n",
      "[2023-02-16 13:08:13,429] INFO zookeeper.pathStats.slotCapacity = 60 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:13,429] INFO zookeeper.pathStats.slotDuration = 15 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:13,430] INFO zookeeper.pathStats.maxDepth = 6 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:13,430] INFO zookeeper.pathStats.initialDelay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:13,430] INFO zookeeper.pathStats.delay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:13,431] INFO zookeeper.pathStats.enabled = false (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:13,434] INFO The max bytes for all large requests are set to 104857600 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,434] INFO The large request threshold is set to -1 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,435] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 clientPortListenBacklog -1 datadir /tmp/tmpve9khxd3/zookeeper/version-2 snapdir /tmp/tmpve9khxd3/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,452] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)\n",
      "[2023-02-16 13:08:13,453] WARN maxCnxns is not configured, using default value 0. (org.apache.zookeeper.server.ServerCnxnFactory)\n",
      "[2023-02-16 13:08:13,455] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 8 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)\n",
      "[2023-02-16 13:08:13,461] INFO binding to port 0.0.0.0/0.0.0.0:9998 (org.apache.zookeeper.server.NIOServerCnxnFactory)\n",
      "[2023-02-16 13:08:13,529] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)\n",
      "[2023-02-16 13:08:13,529] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)\n",
      "[2023-02-16 13:08:13,532] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[2023-02-16 13:08:13,532] INFO zookeeper.commitLogCount=500 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[2023-02-16 13:08:13,549] INFO zookeeper.snapshot.compression.method = CHECKED (org.apache.zookeeper.server.persistence.SnapStream)\n",
      "[2023-02-16 13:08:13,549] INFO Snapshotting: 0x0 to /tmp/tmpve9khxd3/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2023-02-16 13:08:13,552] INFO Snapshot loaded in 10 ms, highest zxid is 0x0, digest is 1371985504 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[2023-02-16 13:08:13,552] INFO Snapshotting: 0x0 to /tmp/tmpve9khxd3/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2023-02-16 13:08:13,553] INFO Snapshot taken in 0 ms (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:13,566] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)\n",
      "[2023-02-16 13:08:13,566] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)\n",
      "[2023-02-16 13:08:13,588] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)\n",
      "[2023-02-16 13:08:13,589] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)\n",
      "[2023-02-16 13:08:17,839] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)\n",
      "\n",
      "**************************************************KAFKA LOGS++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[2023-02-16 13:08:17,431] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\n",
      "[2023-02-16 13:08:17,701] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\n",
      "[2023-02-16 13:08:17,783] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\n",
      "[2023-02-16 13:08:17,784] INFO starting (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:17,785] INFO Connecting to zookeeper on localhost:9998 (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:17,796] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:9998. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-02-16 13:08:17,801] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,801] INFO Client environment:host.name=tvrtko-fastkafka-devel (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,801] INFO Client environment:java.version=11.0.18 (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,801] INFO Client environment:java.vendor=Eclipse Adoptium (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,801] INFO Client environment:java.home=/home/tvrtko/.jdk/jdk-11.0.18+10 (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,801] INFO Client environment:java.class.path=/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/activation-1.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/argparse4j-0.7.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/audience-annotations-0.5.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-cli-1.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.12.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.8.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-basic-auth-extension-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-json-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-client-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-runtime-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-transforms-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-api-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-locator-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-utils-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-core-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-databind-2.13.4.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-dataformat-csv-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-datatype-jdk8-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-base-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-json-provider-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-jaxb-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-scala_2.13-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.activation-api-1.2.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.inject-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javassist-3.27.0-GA.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.servlet-api-3.1.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jaxb-api-2.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-client-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-common-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-core-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-hk2-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-server-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-client-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-continuation-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-http-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-io-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-security-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-server-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlet-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlets-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-ajax-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jline-3.21.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jopt-simple-5.0.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jose4j-0.7.9.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-clients-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-log4j-appender-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-metadata-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-raft-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-server-common-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-shell-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-examples-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-scala_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-test-utils-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-tools-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/lz4-java-1.8.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/maven-artifact-3.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-4.1.12.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-buffer-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-codec-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-handler-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-resolver-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-classes-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-unix-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/paranamer-2.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/plexus-utils-3.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reflections-0.9.12.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reload4j-1.2.19.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/rocksdbjni-7.1.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-collection-compat_2.13-2.6.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-library-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-logging_2.13-3.9.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-reflect-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-api-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-reload4j-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/snappy-java-1.1.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/swagger-annotations-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/trogdor-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-jute-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zstd-jni-1.5.2-1.jar (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,801] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,801] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,801] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,802] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,802] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,802] INFO Client environment:os.version=5.15.0-58-generic (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,802] INFO Client environment:user.name=tvrtko (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,802] INFO Client environment:user.home=/home/tvrtko (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,802] INFO Client environment:user.dir=/work/fastkafka/nbs (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,802] INFO Client environment:os.memory.free=1009MB (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,802] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,802] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,804] INFO Initiating client connection, connectString=localhost:9998 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@7b84fcf8 (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:17,808] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)\n",
      "[2023-02-16 13:08:17,813] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-02-16 13:08:17,819] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-02-16 13:08:17,820] INFO Opening socket connection to server localhost/127.0.0.1:9998. (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-02-16 13:08:17,828] INFO Socket connection established, initiating session, client: /127.0.0.1:48848, server: localhost/127.0.0.1:9998 (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-02-16 13:08:17,849] INFO Session establishment complete on server localhost/127.0.0.1:9998, session id = 0x10010d131570000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-02-16 13:08:17,852] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-02-16 13:08:18,089] INFO Cluster ID = JIiOSlw7SwKFKRwMDAeD9w (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:18,091] WARN No meta.properties file under dir /tmp/tmpve9khxd3/kafka_logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)\n",
      "[2023-02-16 13:08:18,133] INFO KafkaConfig values: \n",
      "\tadvertised.listeners = null\n",
      "\talter.config.policy.class.name = null\n",
      "\talter.log.dirs.replication.quota.window.num = 11\n",
      "\talter.log.dirs.replication.quota.window.size.seconds = 1\n",
      "\tauthorizer.class.name = \n",
      "\tauto.create.topics.enable = true\n",
      "\tauto.leader.rebalance.enable = true\n",
      "\tbackground.threads = 10\n",
      "\tbroker.heartbeat.interval.ms = 2000\n",
      "\tbroker.id = 0\n",
      "\tbroker.id.generation.enable = true\n",
      "\tbroker.rack = null\n",
      "\tbroker.session.timeout.ms = 9000\n",
      "\tclient.quota.callback.class = null\n",
      "\tcompression.type = producer\n",
      "\tconnection.failed.authentication.delay.ms = 100\n",
      "\tconnections.max.idle.ms = 600000\n",
      "\tconnections.max.reauth.ms = 0\n",
      "\tcontrol.plane.listener.name = null\n",
      "\tcontrolled.shutdown.enable = true\n",
      "\tcontrolled.shutdown.max.retries = 3\n",
      "\tcontrolled.shutdown.retry.backoff.ms = 5000\n",
      "\tcontroller.listener.names = null\n",
      "\tcontroller.quorum.append.linger.ms = 25\n",
      "\tcontroller.quorum.election.backoff.max.ms = 1000\n",
      "\tcontroller.quorum.election.timeout.ms = 1000\n",
      "\tcontroller.quorum.fetch.timeout.ms = 2000\n",
      "\tcontroller.quorum.request.timeout.ms = 2000\n",
      "\tcontroller.quorum.retry.backoff.ms = 20\n",
      "\tcontroller.quorum.voters = []\n",
      "\tcontroller.quota.window.num = 11\n",
      "\tcontroller.quota.window.size.seconds = 1\n",
      "\tcontroller.socket.timeout.ms = 30000\n",
      "\tcreate.topic.policy.class.name = null\n",
      "\tdefault.replication.factor = 1\n",
      "\tdelegation.token.expiry.check.interval.ms = 3600000\n",
      "\tdelegation.token.expiry.time.ms = 86400000\n",
      "\tdelegation.token.master.key = null\n",
      "\tdelegation.token.max.lifetime.ms = 604800000\n",
      "\tdelegation.token.secret.key = null\n",
      "\tdelete.records.purgatory.purge.interval.requests = 1\n",
      "\tdelete.topic.enable = true\n",
      "\tearly.start.listeners = null\n",
      "\tfetch.max.bytes = 57671680\n",
      "\tfetch.purgatory.purge.interval.requests = 1000\n",
      "\tgroup.initial.rebalance.delay.ms = 0\n",
      "\tgroup.max.session.timeout.ms = 1800000\n",
      "\tgroup.max.size = 2147483647\n",
      "\tgroup.min.session.timeout.ms = 6000\n",
      "\tinitial.broker.registration.timeout.ms = 60000\n",
      "\tinter.broker.listener.name = null\n",
      "\tinter.broker.protocol.version = 3.3-IV3\n",
      "\tkafka.metrics.polling.interval.secs = 10\n",
      "\tkafka.metrics.reporters = []\n",
      "\tleader.imbalance.check.interval.seconds = 300\n",
      "\tleader.imbalance.per.broker.percentage = 10\n",
      "\tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n",
      "\tlisteners = PLAINTEXT://:9789\n",
      "\tlog.cleaner.backoff.ms = 15000\n",
      "\tlog.cleaner.dedupe.buffer.size = 134217728\n",
      "\tlog.cleaner.delete.retention.ms = 86400000\n",
      "\tlog.cleaner.enable = true\n",
      "\tlog.cleaner.io.buffer.load.factor = 0.9\n",
      "\tlog.cleaner.io.buffer.size = 524288\n",
      "\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n",
      "\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n",
      "\tlog.cleaner.min.cleanable.ratio = 0.5\n",
      "\tlog.cleaner.min.compaction.lag.ms = 0\n",
      "\tlog.cleaner.threads = 1\n",
      "\tlog.cleanup.policy = [delete]\n",
      "\tlog.dir = /tmp/kafka-logs\n",
      "\tlog.dirs = /tmp/tmpve9khxd3/kafka_logs\n",
      "\tlog.flush.interval.messages = 10000\n",
      "\tlog.flush.interval.ms = 1000\n",
      "\tlog.flush.offset.checkpoint.interval.ms = 60000\n",
      "\tlog.flush.scheduler.interval.ms = 9223372036854775807\n",
      "\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n",
      "\tlog.index.interval.bytes = 4096\n",
      "\tlog.index.size.max.bytes = 10485760\n",
      "\tlog.message.downconversion.enable = true\n",
      "\tlog.message.format.version = 3.0-IV1\n",
      "\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n",
      "\tlog.message.timestamp.type = CreateTime\n",
      "\tlog.preallocate = false\n",
      "\tlog.retention.bytes = 1073741824\n",
      "\tlog.retention.check.interval.ms = 300000\n",
      "\tlog.retention.hours = 168\n",
      "\tlog.retention.minutes = null\n",
      "\tlog.retention.ms = null\n",
      "\tlog.roll.hours = 168\n",
      "\tlog.roll.jitter.hours = 0\n",
      "\tlog.roll.jitter.ms = null\n",
      "\tlog.roll.ms = null\n",
      "\tlog.segment.bytes = 1073741824\n",
      "\tlog.segment.delete.delay.ms = 60000\n",
      "\tmax.connection.creation.rate = 2147483647\n",
      "\tmax.connections = 2147483647\n",
      "\tmax.connections.per.ip = 2147483647\n",
      "\tmax.connections.per.ip.overrides = \n",
      "\tmax.incremental.fetch.session.cache.slots = 1000\n",
      "\tmessage.max.bytes = 1048588\n",
      "\tmetadata.log.dir = null\n",
      "\tmetadata.log.max.record.bytes.between.snapshots = 20971520\n",
      "\tmetadata.log.segment.bytes = 1073741824\n",
      "\tmetadata.log.segment.min.bytes = 8388608\n",
      "\tmetadata.log.segment.ms = 604800000\n",
      "\tmetadata.max.idle.interval.ms = 500\n",
      "\tmetadata.max.retention.bytes = -1\n",
      "\tmetadata.max.retention.ms = 604800000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tmin.insync.replicas = 1\n",
      "\tnode.id = 0\n",
      "\tnum.io.threads = 8\n",
      "\tnum.network.threads = 3\n",
      "\tnum.partitions = 1\n",
      "\tnum.recovery.threads.per.data.dir = 1\n",
      "\tnum.replica.alter.log.dirs.threads = null\n",
      "\tnum.replica.fetchers = 1\n",
      "\toffset.metadata.max.bytes = 4096\n",
      "\toffsets.commit.required.acks = -1\n",
      "\toffsets.commit.timeout.ms = 5000\n",
      "\toffsets.load.buffer.size = 5242880\n",
      "\toffsets.retention.check.interval.ms = 600000\n",
      "\toffsets.retention.minutes = 10080\n",
      "\toffsets.topic.compression.codec = 0\n",
      "\toffsets.topic.num.partitions = 50\n",
      "\toffsets.topic.replication.factor = 1\n",
      "\toffsets.topic.segment.bytes = 104857600\n",
      "\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n",
      "\tpassword.encoder.iterations = 4096\n",
      "\tpassword.encoder.key.length = 128\n",
      "\tpassword.encoder.keyfactory.algorithm = null\n",
      "\tpassword.encoder.old.secret = null\n",
      "\tpassword.encoder.secret = null\n",
      "\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n",
      "\tprocess.roles = []\n",
      "\tproducer.purgatory.purge.interval.requests = 1000\n",
      "\tqueued.max.request.bytes = -1\n",
      "\tqueued.max.requests = 500\n",
      "\tquota.window.num = 11\n",
      "\tquota.window.size.seconds = 1\n",
      "\tremote.log.index.file.cache.total.size.bytes = 1073741824\n",
      "\tremote.log.manager.task.interval.ms = 30000\n",
      "\tremote.log.manager.task.retry.backoff.max.ms = 30000\n",
      "\tremote.log.manager.task.retry.backoff.ms = 500\n",
      "\tremote.log.manager.task.retry.jitter = 0.2\n",
      "\tremote.log.manager.thread.pool.size = 10\n",
      "\tremote.log.metadata.manager.class.name = null\n",
      "\tremote.log.metadata.manager.class.path = null\n",
      "\tremote.log.metadata.manager.impl.prefix = null\n",
      "\tremote.log.metadata.manager.listener.name = null\n",
      "\tremote.log.reader.max.pending.tasks = 100\n",
      "\tremote.log.reader.threads = 10\n",
      "\tremote.log.storage.manager.class.name = null\n",
      "\tremote.log.storage.manager.class.path = null\n",
      "\tremote.log.storage.manager.impl.prefix = null\n",
      "\tremote.log.storage.system.enable = false\n",
      "\treplica.fetch.backoff.ms = 1000\n",
      "\treplica.fetch.max.bytes = 1048576\n",
      "\treplica.fetch.min.bytes = 1\n",
      "\treplica.fetch.response.max.bytes = 10485760\n",
      "\treplica.fetch.wait.max.ms = 500\n",
      "\treplica.high.watermark.checkpoint.interval.ms = 5000\n",
      "\treplica.lag.time.max.ms = 30000\n",
      "\treplica.selector.class = null\n",
      "\treplica.socket.receive.buffer.bytes = 65536\n",
      "\treplica.socket.timeout.ms = 30000\n",
      "\treplication.quota.window.num = 11\n",
      "\treplication.quota.window.size.seconds = 1\n",
      "\trequest.timeout.ms = 30000\n",
      "\treserved.broker.max.id = 1000\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.enabled.mechanisms = [GSSAPI]\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism.controller.protocol = GSSAPI\n",
      "\tsasl.mechanism.inter.broker.protocol = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsasl.server.callback.handler.class = null\n",
      "\tsasl.server.max.receive.size = 524288\n",
      "\tsecurity.inter.broker.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tsocket.listen.backlog.size = 50\n",
      "\tsocket.receive.buffer.bytes = 102400\n",
      "\tsocket.request.max.bytes = 104857600\n",
      "\tsocket.send.buffer.bytes = 102400\n",
      "\tssl.cipher.suites = []\n",
      "\tssl.client.auth = none\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.principal.mapping.rules = DEFAULT\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n",
      "\ttransaction.max.timeout.ms = 900000\n",
      "\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n",
      "\ttransaction.state.log.load.buffer.size = 5242880\n",
      "\ttransaction.state.log.min.isr = 1\n",
      "\ttransaction.state.log.num.partitions = 50\n",
      "\ttransaction.state.log.replication.factor = 1\n",
      "\ttransaction.state.log.segment.bytes = 104857600\n",
      "\ttransactional.id.expiration.ms = 604800000\n",
      "\tunclean.leader.election.enable = false\n",
      "\tzookeeper.clientCnxnSocket = null\n",
      "\tzookeeper.connect = localhost:9998\n",
      "\tzookeeper.connection.timeout.ms = 18000\n",
      "\tzookeeper.max.in.flight.requests = 10\n",
      "\tzookeeper.session.timeout.ms = 18000\n",
      "\tzookeeper.set.acl = false\n",
      "\tzookeeper.ssl.cipher.suites = null\n",
      "\tzookeeper.ssl.client.enable = false\n",
      "\tzookeeper.ssl.crl.enable = false\n",
      "\tzookeeper.ssl.enabled.protocols = null\n",
      "\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n",
      "\tzookeeper.ssl.keystore.location = null\n",
      "\tzookeeper.ssl.keystore.password = null\n",
      "\tzookeeper.ssl.keystore.type = null\n",
      "\tzookeeper.ssl.ocsp.enable = false\n",
      "\tzookeeper.ssl.protocol = TLSv1.2\n",
      "\tzookeeper.ssl.truststore.location = null\n",
      "\tzookeeper.ssl.truststore.password = null\n",
      "\tzookeeper.ssl.truststore.type = null\n",
      " (kafka.server.KafkaConfig)\n",
      "[2023-02-16 13:08:18,169] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:18,169] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:18,171] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:18,173] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:18,190] INFO Log directory /tmp/tmpve9khxd3/kafka_logs not found, creating it. (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:18,207] INFO Loading logs from log dirs ArraySeq(/tmp/tmpve9khxd3/kafka_logs) (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:18,210] INFO Attempting recovery for all logs in /tmp/tmpve9khxd3/kafka_logs since no clean shutdown file was found (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:18,221] INFO Loaded 0 logs in 13ms. (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:18,221] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:18,223] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:18,322] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-02-16 13:08:18,332] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)\n",
      "[2023-02-16 13:08:18,362] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:18,697] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n",
      "[2023-02-16 13:08:18,700] INFO Awaiting socket connections on 0.0.0.0:9789. (kafka.network.DataPlaneAcceptor)\n",
      "[2023-02-16 13:08:18,727] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n",
      "[2023-02-16 13:08:18,733] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:18,756] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:18,756] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:18,758] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:18,766] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:18,785] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-02-16 13:08:18,812] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)\n",
      "[2023-02-16 13:08:18,839] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1676552898824,1676552898824,1,0,0,72076084699922432,228,0,25\n",
      " (kafka.zk.KafkaZkClient)\n",
      "[2023-02-16 13:08:18,840] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://tvrtko-fastkafka-devel:9789, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)\n",
      "[2023-02-16 13:08:18,910] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:18,910] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)\n",
      "[2023-02-16 13:08:18,921] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:18,924] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:18,929] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)\n",
      "[2023-02-16 13:08:18,949] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-02-16 13:08:18,953] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-02-16 13:08:18,954] INFO [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0). (kafka.server.metadata.ZkMetadataCache)\n",
      "[2023-02-16 13:08:18,975] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-02-16 13:08:18,978] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-02-16 13:08:18,979] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-02-16 13:08:19,066] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:19,134] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-02-16 13:08:19,145] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing. (kafka.network.SocketServer)\n",
      "[2023-02-16 13:08:19,152] INFO Kafka version: 3.3.2 (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-02-16 13:08:19,152] INFO Kafka commitId: b66af662e61082cb (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-02-16 13:08:19,152] INFO Kafka startTimeMs: 1676552899147 (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-02-16 13:08:19,153] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:19,238] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node tvrtko-fastkafka-devel:9789 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:19,279] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node tvrtko-fastkafka-devel:9789 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:22,642] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)\n",
      "[2023-02-16 13:08:22,646] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:22,650] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:22,685] INFO [KafkaServer id=0] Controlled shutdown request returned successfully after 21ms (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:22,688] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-02-16 13:08:22,690] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-02-16 13:08:22,690] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-02-16 13:08:22,693] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer)\n",
      "[2023-02-16 13:08:22,702] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer)\n",
      "[2023-02-16 13:08:22,703] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)\n",
      "[2023-02-16 13:08:22,704] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)\n",
      "[2023-02-16 13:08:22,707] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,709] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,709] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,709] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)\n",
      "[2023-02-16 13:08:22,710] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,710] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,710] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,712] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-02-16 13:08:22,712] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)\n",
      "[2023-02-16 13:08:22,713] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-02-16 13:08:22,713] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-02-16 13:08:22,713] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-02-16 13:08:22,714] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-02-16 13:08:22,714] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-02-16 13:08:22,714] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,715] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,715] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,715] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,716] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,716] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,716] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-02-16 13:08:22,717] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)\n",
      "[2023-02-16 13:08:22,717] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-02-16 13:08:22,717] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-02-16 13:08:22,717] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-02-16 13:08:22,718] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)\n",
      "[2023-02-16 13:08:22,719] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)\n",
      "[2023-02-16 13:08:22,719] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)\n",
      "[2023-02-16 13:08:22,719] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)\n",
      "[2023-02-16 13:08:22,719] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,720] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,720] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,720] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,720] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,720] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,721] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,721] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,721] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,722] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,722] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,722] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:22,727] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)\n",
      "[2023-02-16 13:08:22,727] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:22,727] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:22,727] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:22,729] INFO Broker to controller channel manager for alterPartition shutdown (kafka.server.BrokerToControllerChannelManagerImpl)\n",
      "[2023-02-16 13:08:22,729] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:22,730] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:22,730] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:22,731] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl)\n",
      "[2023-02-16 13:08:22,731] INFO Shutting down. (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:22,748] INFO Shutdown complete. (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:22,753] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-02-16 13:08:22,753] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-02-16 13:08:22,753] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-02-16 13:08:22,754] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-02-16 13:08:22,759] WARN An exception was thrown while closing send thread for session 0x10010d131570000. (org.apache.zookeeper.ClientCnxn)\n",
      "EndOfStreamException: Unable to read additional data from server sessionid 0x10010d131570000, likely server has closed socket\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77)\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)\n",
      "\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)\n",
      "[2023-02-16 13:08:22,863] INFO Session: 0x10010d131570000 closed (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:22,863] INFO EventThread shut down for session: 0x10010d131570000 (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-02-16 13:08:22,868] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-02-16 13:08:22,869] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:22,873] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:22,873] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:22,874] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:22,874] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:22,875] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:22,875] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:22,875] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:22,875] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:22,876] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:22,876] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:22,876] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:22,879] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer)\n",
      "[2023-02-16 13:08:22,917] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer)\n",
      "[2023-02-16 13:08:22,918] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-02-16 13:08:22,918] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-02-16 13:08:22,918] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-02-16 13:08:22,919] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)\n",
      "[2023-02-16 13:08:22,920] INFO App info kafka.server for 0 unregistered (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-02-16 13:08:22,920] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broker = LocalKafkaBroker(zookeeper_port=9998, listener_port=9789)\n",
    "# await broker._start()\n",
    "# await broker._stop()\n",
    "async with broker:\n",
    "    pass\n",
    "\n",
    "print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "print(zookeeper_output.decode(\"UTF-8\"))\n",
    "\n",
    "print(\"*\" * 50 + \"KAFKA LOGS\" + \"+\" * 50)\n",
    "kafka_output, _ = await broker.kafka_task.communicate()\n",
    "print(kafka_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "[INFO] __main__: Zookeeper started, sleeping for 5 seconds...\n",
      "[INFO] __main__: Starting Kafka broker...\n",
      "[INFO] __main__: Kafka broker started, sleeping for 5 seconds...\n",
      "[INFO] __main__: Local Kafka broker up and running on 127.0.0.1:9941\n",
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "[INFO] __main__: Zookeeper started, sleeping for 5 seconds...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 27436...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 27436 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 27070...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 27070 terminated.\n",
      "**************************************************ZOOKEEPER LOGS++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[2023-02-16 13:08:35,698] INFO Reading configuration from: /tmp/tmp7trspik3/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:35,704] INFO clientPortAddress is 0.0.0.0:9939 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:35,704] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:35,704] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:35,705] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:35,706] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2023-02-16 13:08:35,706] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2023-02-16 13:08:35,706] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2023-02-16 13:08:35,706] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)\n",
      "[2023-02-16 13:08:35,708] INFO Log4j 1.2 jmx support not found; jmx disabled. (org.apache.zookeeper.jmx.ManagedUtil)\n",
      "[2023-02-16 13:08:35,709] INFO Reading configuration from: /tmp/tmp7trspik3/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:35,709] INFO clientPortAddress is 0.0.0.0:9939 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:35,709] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:35,709] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:35,709] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:35,710] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)\n",
      "[2023-02-16 13:08:35,721] INFO ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@1649b0e6 (org.apache.zookeeper.server.ServerMetrics)\n",
      "[2023-02-16 13:08:35,725] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2023-02-16 13:08:35,736] INFO  (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,736] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,736] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,736] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,736] INFO    / /    / _ \\   / _ \\  | |/ /  / _ \\  / _ \\ | '_ \\   / _ \\ | '__| (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,736] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,736] INFO  /_____|  \\___/   \\___/  |_|\\_\\  \\___|  \\___| | .__/   \\___| |_| (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,736] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,736] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,736] INFO  (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:host.name=tvrtko-fastkafka-devel (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:java.version=11.0.18 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:java.vendor=Eclipse Adoptium (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:java.home=/home/tvrtko/.jdk/jdk-11.0.18+10 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:java.class.path=/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/activation-1.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/argparse4j-0.7.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/audience-annotations-0.5.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-cli-1.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.12.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.8.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-basic-auth-extension-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-json-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-client-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-runtime-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-transforms-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-api-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-locator-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-utils-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-core-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-databind-2.13.4.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-dataformat-csv-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-datatype-jdk8-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-base-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-json-provider-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-jaxb-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-scala_2.13-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.activation-api-1.2.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.inject-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javassist-3.27.0-GA.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.servlet-api-3.1.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jaxb-api-2.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-client-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-common-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-core-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-hk2-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-server-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-client-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-continuation-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-http-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-io-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-security-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-server-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlet-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlets-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-ajax-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jline-3.21.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jopt-simple-5.0.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jose4j-0.7.9.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-clients-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-log4j-appender-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-metadata-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-raft-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-server-common-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-shell-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-examples-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-scala_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-test-utils-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-tools-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/lz4-java-1.8.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/maven-artifact-3.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-4.1.12.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-buffer-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-codec-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-handler-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-resolver-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-classes-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-unix-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/paranamer-2.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/plexus-utils-3.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reflections-0.9.12.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reload4j-1.2.19.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/rocksdbjni-7.1.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-collection-compat_2.13-2.6.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-library-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-logging_2.13-3.9.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-reflect-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-api-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-reload4j-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/snappy-java-1.1.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/swagger-annotations-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/trogdor-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-jute-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zstd-jni-1.5.2-1.jar (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:os.version=5.15.0-58-generic (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:user.name=tvrtko (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,738] INFO Server environment:user.home=/home/tvrtko (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,739] INFO Server environment:user.dir=/work/fastkafka/nbs (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,739] INFO Server environment:os.memory.free=490MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,739] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,739] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,739] INFO zookeeper.enableEagerACLCheck = false (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,739] INFO zookeeper.digest.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,739] INFO zookeeper.closeSessionTxn.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,739] INFO zookeeper.flushDelay=0 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,739] INFO zookeeper.maxWriteQueuePollTime=0 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,739] INFO zookeeper.maxBatchSize=1000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,739] INFO zookeeper.intBufferStartingSizeBytes = 1024 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,740] INFO Weighed connection throttling is disabled (org.apache.zookeeper.server.BlueThrottle)\n",
      "[2023-02-16 13:08:35,741] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,741] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,742] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)\n",
      "[2023-02-16 13:08:35,742] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)\n",
      "[2023-02-16 13:08:35,744] INFO zookeeper.pathStats.slotCapacity = 60 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:35,744] INFO zookeeper.pathStats.slotDuration = 15 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:35,744] INFO zookeeper.pathStats.maxDepth = 6 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:35,744] INFO zookeeper.pathStats.initialDelay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:35,744] INFO zookeeper.pathStats.delay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:35,744] INFO zookeeper.pathStats.enabled = false (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:35,747] INFO The max bytes for all large requests are set to 104857600 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,747] INFO The large request threshold is set to -1 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,747] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 clientPortListenBacklog -1 datadir /tmp/tmp7trspik3/zookeeper/version-2 snapdir /tmp/tmp7trspik3/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:35,755] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)\n",
      "[2023-02-16 13:08:35,756] WARN maxCnxns is not configured, using default value 0. (org.apache.zookeeper.server.ServerCnxnFactory)\n",
      "[2023-02-16 13:08:35,758] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 8 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)\n",
      "[2023-02-16 13:08:35,764] INFO binding to port 0.0.0.0/0.0.0.0:9939 (org.apache.zookeeper.server.NIOServerCnxnFactory)\n",
      "[2023-02-16 13:08:35,764] ERROR Unexpected exception, exiting abnormally (org.apache.zookeeper.server.ZooKeeperServerMain)\n",
      "java.net.BindException: Address already in use\n",
      "\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.bind(Net.java:459)\n",
      "\tat java.base/sun.nio.ch.Net.bind(Net.java:448)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:227)\n",
      "\tat java.base/sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:80)\n",
      "\tat java.base/sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:73)\n",
      "\tat org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:676)\n",
      "\tat org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:158)\n",
      "\tat org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:112)\n",
      "\tat org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:67)\n",
      "\tat org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:140)\n",
      "\tat org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:90)\n",
      "[2023-02-16 13:08:35,767] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)\n",
      "[2023-02-16 13:08:35,770] ERROR Exiting JVM with code 1 (org.apache.zookeeper.util.ServiceUtils)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "port = 9939\n",
    "\n",
    "broker_1 = LocalKafkaBroker(zookeeper_port=port, listener_port=9941)\n",
    "broker_2 = LocalKafkaBroker(zookeeper_port=port, listener_port=9942)\n",
    "async with broker_1:\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        async with broker_2:\n",
    "            pass\n",
    "\n",
    "expected = (\n",
    "    \"Could not start zookeeper with params: {\" + f\"'zookeeper_port': {port}\" + \"}\"\n",
    ")\n",
    "assert e.value.args == (expected, )\n",
    "\n",
    "for broker in [broker_2]:\n",
    "    print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "    zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "    print(zookeeper_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aff342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@patch # type: ignore\n",
    "def start(self: LocalKafkaBroker) -> str:\n",
    "    \"\"\"Starts a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           Kafka broker bootstrap server address in string format: add:port\n",
    "    \"\"\"\n",
    "    logger.info(f\"{self.__class__.__name__}.start(): entering...\")\n",
    "    try:\n",
    "        # get or create loop\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "        except RuntimeError as e:\n",
    "            logger.warning(f\"{self.__class__.__name__}.start(): RuntimeError raised when calling asyncio.get_event_loop(): {e}\")\n",
    "            logger.warning(f\"{self.__class__.__name__}.start(): asyncio.new_event_loop()\")\n",
    "            loop = asyncio.new_event_loop()\n",
    "            \n",
    "        # start zookeeper and kafka broker in the loop\n",
    "        try:\n",
    "            retval = loop.run_until_complete(self._start())\n",
    "            logger.info(f\"{self.__class__.__name__}.start(): returning {retval}\")\n",
    "            self.started = True\n",
    "            return retval\n",
    "        except RuntimeError as e:\n",
    "            logger.warning(f\"{self.__class__.__name__}.start(): RuntimeError raised for loop ({loop}): {e}\")\n",
    "            logger.warning(f\"{self.__class__.__name__}.start(): calling nest_asyncio.apply()\")\n",
    "            nest_asyncio.apply(loop)\n",
    "\n",
    "            retval = loop.run_until_complete(self._start())\n",
    "            logger.info(f\"{self.__class__}.start(): returning {retval}\")\n",
    "            self.started = True\n",
    "            return retval\n",
    "        \n",
    "    finally:\n",
    "        logger.info(f\"{self.__class__.__name__}.start(): exited.\")\n",
    "\n",
    "@patch # type: ignore\n",
    "def stop(self: LocalKafkaBroker) -> None:\n",
    "    \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           None\n",
    "    \"\"\"\n",
    "    logger.info(f\"{self.__class__.__name__}.stop(): entering...\")\n",
    "    try:\n",
    "        if not self.started:\n",
    "            raise RuntimeError(\"LocalKafkaBroker not started yet, please call LocalKafkaBroker.start() before!\")\n",
    "            \n",
    "        loop = asyncio.get_event_loop()\n",
    "        self.started = False\n",
    "        return loop.run_until_complete(self._stop())\n",
    "    finally:\n",
    "        logger.info(f\"{self.__class__.__name__}.stop(): exited.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5f295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: LocalKafkaBroker.start(): entering...\n",
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "[INFO] __main__: Zookeeper started, sleeping for 5 seconds...\n",
      "[INFO] __main__: Starting Kafka broker...\n",
      "[INFO] __main__: Kafka broker started, sleeping for 5 seconds...\n",
      "[INFO] __main__: Local Kafka broker up and running on 127.0.0.1:9789\n",
      "[INFO] __main__: LocalKafkaBroker.start(): returning 127.0.0.1:9789\n",
      "[INFO] __main__: LocalKafkaBroker.start(): exited.\n",
      "Hello world!\n",
      "[INFO] __main__: LocalKafkaBroker.stop(): entering...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 28586...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 28586 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 28220...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 28220 terminated.\n",
      "[INFO] __main__: LocalKafkaBroker.stop(): exited.\n",
      "**************************************************ZOOKEEPER LOGS++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[2023-02-16 13:08:43,604] INFO Reading configuration from: /tmp/tmp9sdfm342/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:43,609] INFO clientPortAddress is 0.0.0.0:9998 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:43,610] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:43,610] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:43,610] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:43,612] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2023-02-16 13:08:43,612] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2023-02-16 13:08:43,612] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2023-02-16 13:08:43,612] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)\n",
      "[2023-02-16 13:08:43,613] INFO Log4j 1.2 jmx support not found; jmx disabled. (org.apache.zookeeper.jmx.ManagedUtil)\n",
      "[2023-02-16 13:08:43,614] INFO Reading configuration from: /tmp/tmp9sdfm342/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:43,614] INFO clientPortAddress is 0.0.0.0:9998 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:43,614] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:43,614] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:43,614] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-02-16 13:08:43,615] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)\n",
      "[2023-02-16 13:08:43,625] INFO ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@1649b0e6 (org.apache.zookeeper.server.ServerMetrics)\n",
      "[2023-02-16 13:08:43,628] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2023-02-16 13:08:43,637] INFO  (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,637] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,637] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,637] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,637] INFO    / /    / _ \\   / _ \\  | |/ /  / _ \\  / _ \\ | '_ \\   / _ \\ | '__| (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,637] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,637] INFO  /_____|  \\___/   \\___/  |_|\\_\\  \\___|  \\___| | .__/   \\___| |_| (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,637] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,638] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,638] INFO  (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,639] INFO Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,639] INFO Server environment:host.name=tvrtko-fastkafka-devel (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,639] INFO Server environment:java.version=11.0.18 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,639] INFO Server environment:java.vendor=Eclipse Adoptium (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,639] INFO Server environment:java.home=/home/tvrtko/.jdk/jdk-11.0.18+10 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,639] INFO Server environment:java.class.path=/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/activation-1.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/argparse4j-0.7.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/audience-annotations-0.5.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-cli-1.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.12.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.8.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-basic-auth-extension-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-json-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-client-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-runtime-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-transforms-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-api-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-locator-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-utils-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-core-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-databind-2.13.4.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-dataformat-csv-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-datatype-jdk8-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-base-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-json-provider-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-jaxb-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-scala_2.13-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.activation-api-1.2.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.inject-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javassist-3.27.0-GA.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.servlet-api-3.1.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jaxb-api-2.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-client-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-common-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-core-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-hk2-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-server-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-client-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-continuation-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-http-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-io-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-security-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-server-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlet-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlets-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-ajax-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jline-3.21.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jopt-simple-5.0.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jose4j-0.7.9.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-clients-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-log4j-appender-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-metadata-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-raft-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-server-common-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-shell-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-examples-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-scala_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-test-utils-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-tools-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/lz4-java-1.8.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/maven-artifact-3.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-4.1.12.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-buffer-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-codec-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-handler-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-resolver-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-classes-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-unix-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/paranamer-2.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/plexus-utils-3.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reflections-0.9.12.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reload4j-1.2.19.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/rocksdbjni-7.1.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-collection-compat_2.13-2.6.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-library-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-logging_2.13-3.9.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-reflect-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-api-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-reload4j-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/snappy-java-1.1.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/swagger-annotations-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/trogdor-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-jute-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zstd-jni-1.5.2-1.jar (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO Server environment:os.version=5.15.0-58-generic (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO Server environment:user.name=tvrtko (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO Server environment:user.home=/home/tvrtko (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO Server environment:user.dir=/work/fastkafka/nbs (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO Server environment:os.memory.free=490MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO zookeeper.enableEagerACLCheck = false (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO zookeeper.digest.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO zookeeper.closeSessionTxn.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO zookeeper.flushDelay=0 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,640] INFO zookeeper.maxWriteQueuePollTime=0 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,641] INFO zookeeper.maxBatchSize=1000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,641] INFO zookeeper.intBufferStartingSizeBytes = 1024 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,642] INFO Weighed connection throttling is disabled (org.apache.zookeeper.server.BlueThrottle)\n",
      "[2023-02-16 13:08:43,643] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,643] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,644] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)\n",
      "[2023-02-16 13:08:43,644] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)\n",
      "[2023-02-16 13:08:43,645] INFO zookeeper.pathStats.slotCapacity = 60 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:43,645] INFO zookeeper.pathStats.slotDuration = 15 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:43,645] INFO zookeeper.pathStats.maxDepth = 6 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:43,645] INFO zookeeper.pathStats.initialDelay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:43,645] INFO zookeeper.pathStats.delay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:43,645] INFO zookeeper.pathStats.enabled = false (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-02-16 13:08:43,647] INFO The max bytes for all large requests are set to 104857600 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,647] INFO The large request threshold is set to -1 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,648] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 clientPortListenBacklog -1 datadir /tmp/tmp9sdfm342/zookeeper/version-2 snapdir /tmp/tmp9sdfm342/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,654] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)\n",
      "[2023-02-16 13:08:43,655] WARN maxCnxns is not configured, using default value 0. (org.apache.zookeeper.server.ServerCnxnFactory)\n",
      "[2023-02-16 13:08:43,657] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 8 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)\n",
      "[2023-02-16 13:08:43,660] INFO binding to port 0.0.0.0/0.0.0.0:9998 (org.apache.zookeeper.server.NIOServerCnxnFactory)\n",
      "[2023-02-16 13:08:43,681] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)\n",
      "[2023-02-16 13:08:43,681] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)\n",
      "[2023-02-16 13:08:43,682] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[2023-02-16 13:08:43,682] INFO zookeeper.commitLogCount=500 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[2023-02-16 13:08:43,687] INFO zookeeper.snapshot.compression.method = CHECKED (org.apache.zookeeper.server.persistence.SnapStream)\n",
      "[2023-02-16 13:08:43,687] INFO Snapshotting: 0x0 to /tmp/tmp9sdfm342/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2023-02-16 13:08:43,690] INFO Snapshot loaded in 7 ms, highest zxid is 0x0, digest is 1371985504 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[2023-02-16 13:08:43,690] INFO Snapshotting: 0x0 to /tmp/tmp9sdfm342/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2023-02-16 13:08:43,691] INFO Snapshot taken in 0 ms (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-02-16 13:08:43,700] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)\n",
      "[2023-02-16 13:08:43,700] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)\n",
      "[2023-02-16 13:08:43,716] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)\n",
      "[2023-02-16 13:08:43,717] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)\n",
      "[2023-02-16 13:08:49,970] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)\n",
      "\n",
      "**************************************************KAFKA LOGS++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[2023-02-16 13:08:49,521] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\n",
      "[2023-02-16 13:08:49,832] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\n",
      "[2023-02-16 13:08:49,913] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\n",
      "[2023-02-16 13:08:49,914] INFO starting (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:49,915] INFO Connecting to zookeeper on localhost:9998 (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:49,928] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:9998. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-02-16 13:08:49,932] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,933] INFO Client environment:host.name=tvrtko-fastkafka-devel (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,933] INFO Client environment:java.version=11.0.18 (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,933] INFO Client environment:java.vendor=Eclipse Adoptium (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,933] INFO Client environment:java.home=/home/tvrtko/.jdk/jdk-11.0.18+10 (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,933] INFO Client environment:java.class.path=/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/activation-1.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/argparse4j-0.7.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/audience-annotations-0.5.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-cli-1.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.12.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.8.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-basic-auth-extension-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-json-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-client-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-runtime-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-transforms-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-api-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-locator-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-utils-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-core-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-databind-2.13.4.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-dataformat-csv-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-datatype-jdk8-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-base-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-json-provider-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-jaxb-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-scala_2.13-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.activation-api-1.2.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.inject-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javassist-3.27.0-GA.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.servlet-api-3.1.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jaxb-api-2.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-client-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-common-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-core-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-hk2-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-server-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-client-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-continuation-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-http-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-io-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-security-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-server-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlet-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlets-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-ajax-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jline-3.21.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jopt-simple-5.0.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jose4j-0.7.9.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-clients-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-log4j-appender-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-metadata-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-raft-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-server-common-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-shell-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-examples-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-scala_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-test-utils-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-tools-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/lz4-java-1.8.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/maven-artifact-3.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-4.1.12.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-buffer-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-codec-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-handler-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-resolver-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-classes-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-unix-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/paranamer-2.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/plexus-utils-3.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reflections-0.9.12.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reload4j-1.2.19.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/rocksdbjni-7.1.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-collection-compat_2.13-2.6.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-library-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-logging_2.13-3.9.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-reflect-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-api-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-reload4j-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/snappy-java-1.1.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/swagger-annotations-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/trogdor-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-jute-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zstd-jni-1.5.2-1.jar (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,937] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,937] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,937] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,937] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,937] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,937] INFO Client environment:os.version=5.15.0-58-generic (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,937] INFO Client environment:user.name=tvrtko (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,938] INFO Client environment:user.home=/home/tvrtko (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,938] INFO Client environment:user.dir=/work/fastkafka/nbs (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,938] INFO Client environment:os.memory.free=1009MB (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,938] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,938] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,940] INFO Initiating client connection, connectString=localhost:9998 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@30b19518 (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:49,944] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)\n",
      "[2023-02-16 13:08:49,949] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-02-16 13:08:49,951] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-02-16 13:08:49,955] INFO Opening socket connection to server localhost/127.0.0.1:9998. (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-02-16 13:08:49,961] INFO Socket connection established, initiating session, client: /127.0.0.1:36404, server: localhost/127.0.0.1:9998 (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-02-16 13:08:49,980] INFO Session establishment complete on server localhost/127.0.0.1:9998, session id = 0x10010d1a7100000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-02-16 13:08:49,983] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-02-16 13:08:50,218] INFO Cluster ID = Q8rFS0UrRwSXvvh2CJOwdw (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:50,220] WARN No meta.properties file under dir /tmp/tmp9sdfm342/kafka_logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)\n",
      "[2023-02-16 13:08:50,261] INFO KafkaConfig values: \n",
      "\tadvertised.listeners = null\n",
      "\talter.config.policy.class.name = null\n",
      "\talter.log.dirs.replication.quota.window.num = 11\n",
      "\talter.log.dirs.replication.quota.window.size.seconds = 1\n",
      "\tauthorizer.class.name = \n",
      "\tauto.create.topics.enable = true\n",
      "\tauto.leader.rebalance.enable = true\n",
      "\tbackground.threads = 10\n",
      "\tbroker.heartbeat.interval.ms = 2000\n",
      "\tbroker.id = 0\n",
      "\tbroker.id.generation.enable = true\n",
      "\tbroker.rack = null\n",
      "\tbroker.session.timeout.ms = 9000\n",
      "\tclient.quota.callback.class = null\n",
      "\tcompression.type = producer\n",
      "\tconnection.failed.authentication.delay.ms = 100\n",
      "\tconnections.max.idle.ms = 600000\n",
      "\tconnections.max.reauth.ms = 0\n",
      "\tcontrol.plane.listener.name = null\n",
      "\tcontrolled.shutdown.enable = true\n",
      "\tcontrolled.shutdown.max.retries = 3\n",
      "\tcontrolled.shutdown.retry.backoff.ms = 5000\n",
      "\tcontroller.listener.names = null\n",
      "\tcontroller.quorum.append.linger.ms = 25\n",
      "\tcontroller.quorum.election.backoff.max.ms = 1000\n",
      "\tcontroller.quorum.election.timeout.ms = 1000\n",
      "\tcontroller.quorum.fetch.timeout.ms = 2000\n",
      "\tcontroller.quorum.request.timeout.ms = 2000\n",
      "\tcontroller.quorum.retry.backoff.ms = 20\n",
      "\tcontroller.quorum.voters = []\n",
      "\tcontroller.quota.window.num = 11\n",
      "\tcontroller.quota.window.size.seconds = 1\n",
      "\tcontroller.socket.timeout.ms = 30000\n",
      "\tcreate.topic.policy.class.name = null\n",
      "\tdefault.replication.factor = 1\n",
      "\tdelegation.token.expiry.check.interval.ms = 3600000\n",
      "\tdelegation.token.expiry.time.ms = 86400000\n",
      "\tdelegation.token.master.key = null\n",
      "\tdelegation.token.max.lifetime.ms = 604800000\n",
      "\tdelegation.token.secret.key = null\n",
      "\tdelete.records.purgatory.purge.interval.requests = 1\n",
      "\tdelete.topic.enable = true\n",
      "\tearly.start.listeners = null\n",
      "\tfetch.max.bytes = 57671680\n",
      "\tfetch.purgatory.purge.interval.requests = 1000\n",
      "\tgroup.initial.rebalance.delay.ms = 0\n",
      "\tgroup.max.session.timeout.ms = 1800000\n",
      "\tgroup.max.size = 2147483647\n",
      "\tgroup.min.session.timeout.ms = 6000\n",
      "\tinitial.broker.registration.timeout.ms = 60000\n",
      "\tinter.broker.listener.name = null\n",
      "\tinter.broker.protocol.version = 3.3-IV3\n",
      "\tkafka.metrics.polling.interval.secs = 10\n",
      "\tkafka.metrics.reporters = []\n",
      "\tleader.imbalance.check.interval.seconds = 300\n",
      "\tleader.imbalance.per.broker.percentage = 10\n",
      "\tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n",
      "\tlisteners = PLAINTEXT://:9789\n",
      "\tlog.cleaner.backoff.ms = 15000\n",
      "\tlog.cleaner.dedupe.buffer.size = 134217728\n",
      "\tlog.cleaner.delete.retention.ms = 86400000\n",
      "\tlog.cleaner.enable = true\n",
      "\tlog.cleaner.io.buffer.load.factor = 0.9\n",
      "\tlog.cleaner.io.buffer.size = 524288\n",
      "\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n",
      "\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n",
      "\tlog.cleaner.min.cleanable.ratio = 0.5\n",
      "\tlog.cleaner.min.compaction.lag.ms = 0\n",
      "\tlog.cleaner.threads = 1\n",
      "\tlog.cleanup.policy = [delete]\n",
      "\tlog.dir = /tmp/kafka-logs\n",
      "\tlog.dirs = /tmp/tmp9sdfm342/kafka_logs\n",
      "\tlog.flush.interval.messages = 10000\n",
      "\tlog.flush.interval.ms = 1000\n",
      "\tlog.flush.offset.checkpoint.interval.ms = 60000\n",
      "\tlog.flush.scheduler.interval.ms = 9223372036854775807\n",
      "\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n",
      "\tlog.index.interval.bytes = 4096\n",
      "\tlog.index.size.max.bytes = 10485760\n",
      "\tlog.message.downconversion.enable = true\n",
      "\tlog.message.format.version = 3.0-IV1\n",
      "\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n",
      "\tlog.message.timestamp.type = CreateTime\n",
      "\tlog.preallocate = false\n",
      "\tlog.retention.bytes = 1073741824\n",
      "\tlog.retention.check.interval.ms = 300000\n",
      "\tlog.retention.hours = 168\n",
      "\tlog.retention.minutes = null\n",
      "\tlog.retention.ms = null\n",
      "\tlog.roll.hours = 168\n",
      "\tlog.roll.jitter.hours = 0\n",
      "\tlog.roll.jitter.ms = null\n",
      "\tlog.roll.ms = null\n",
      "\tlog.segment.bytes = 1073741824\n",
      "\tlog.segment.delete.delay.ms = 60000\n",
      "\tmax.connection.creation.rate = 2147483647\n",
      "\tmax.connections = 2147483647\n",
      "\tmax.connections.per.ip = 2147483647\n",
      "\tmax.connections.per.ip.overrides = \n",
      "\tmax.incremental.fetch.session.cache.slots = 1000\n",
      "\tmessage.max.bytes = 1048588\n",
      "\tmetadata.log.dir = null\n",
      "\tmetadata.log.max.record.bytes.between.snapshots = 20971520\n",
      "\tmetadata.log.segment.bytes = 1073741824\n",
      "\tmetadata.log.segment.min.bytes = 8388608\n",
      "\tmetadata.log.segment.ms = 604800000\n",
      "\tmetadata.max.idle.interval.ms = 500\n",
      "\tmetadata.max.retention.bytes = -1\n",
      "\tmetadata.max.retention.ms = 604800000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tmin.insync.replicas = 1\n",
      "\tnode.id = 0\n",
      "\tnum.io.threads = 8\n",
      "\tnum.network.threads = 3\n",
      "\tnum.partitions = 1\n",
      "\tnum.recovery.threads.per.data.dir = 1\n",
      "\tnum.replica.alter.log.dirs.threads = null\n",
      "\tnum.replica.fetchers = 1\n",
      "\toffset.metadata.max.bytes = 4096\n",
      "\toffsets.commit.required.acks = -1\n",
      "\toffsets.commit.timeout.ms = 5000\n",
      "\toffsets.load.buffer.size = 5242880\n",
      "\toffsets.retention.check.interval.ms = 600000\n",
      "\toffsets.retention.minutes = 10080\n",
      "\toffsets.topic.compression.codec = 0\n",
      "\toffsets.topic.num.partitions = 50\n",
      "\toffsets.topic.replication.factor = 1\n",
      "\toffsets.topic.segment.bytes = 104857600\n",
      "\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n",
      "\tpassword.encoder.iterations = 4096\n",
      "\tpassword.encoder.key.length = 128\n",
      "\tpassword.encoder.keyfactory.algorithm = null\n",
      "\tpassword.encoder.old.secret = null\n",
      "\tpassword.encoder.secret = null\n",
      "\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n",
      "\tprocess.roles = []\n",
      "\tproducer.purgatory.purge.interval.requests = 1000\n",
      "\tqueued.max.request.bytes = -1\n",
      "\tqueued.max.requests = 500\n",
      "\tquota.window.num = 11\n",
      "\tquota.window.size.seconds = 1\n",
      "\tremote.log.index.file.cache.total.size.bytes = 1073741824\n",
      "\tremote.log.manager.task.interval.ms = 30000\n",
      "\tremote.log.manager.task.retry.backoff.max.ms = 30000\n",
      "\tremote.log.manager.task.retry.backoff.ms = 500\n",
      "\tremote.log.manager.task.retry.jitter = 0.2\n",
      "\tremote.log.manager.thread.pool.size = 10\n",
      "\tremote.log.metadata.manager.class.name = null\n",
      "\tremote.log.metadata.manager.class.path = null\n",
      "\tremote.log.metadata.manager.impl.prefix = null\n",
      "\tremote.log.metadata.manager.listener.name = null\n",
      "\tremote.log.reader.max.pending.tasks = 100\n",
      "\tremote.log.reader.threads = 10\n",
      "\tremote.log.storage.manager.class.name = null\n",
      "\tremote.log.storage.manager.class.path = null\n",
      "\tremote.log.storage.manager.impl.prefix = null\n",
      "\tremote.log.storage.system.enable = false\n",
      "\treplica.fetch.backoff.ms = 1000\n",
      "\treplica.fetch.max.bytes = 1048576\n",
      "\treplica.fetch.min.bytes = 1\n",
      "\treplica.fetch.response.max.bytes = 10485760\n",
      "\treplica.fetch.wait.max.ms = 500\n",
      "\treplica.high.watermark.checkpoint.interval.ms = 5000\n",
      "\treplica.lag.time.max.ms = 30000\n",
      "\treplica.selector.class = null\n",
      "\treplica.socket.receive.buffer.bytes = 65536\n",
      "\treplica.socket.timeout.ms = 30000\n",
      "\treplication.quota.window.num = 11\n",
      "\treplication.quota.window.size.seconds = 1\n",
      "\trequest.timeout.ms = 30000\n",
      "\treserved.broker.max.id = 1000\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.enabled.mechanisms = [GSSAPI]\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism.controller.protocol = GSSAPI\n",
      "\tsasl.mechanism.inter.broker.protocol = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsasl.server.callback.handler.class = null\n",
      "\tsasl.server.max.receive.size = 524288\n",
      "\tsecurity.inter.broker.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tsocket.listen.backlog.size = 50\n",
      "\tsocket.receive.buffer.bytes = 102400\n",
      "\tsocket.request.max.bytes = 104857600\n",
      "\tsocket.send.buffer.bytes = 102400\n",
      "\tssl.cipher.suites = []\n",
      "\tssl.client.auth = none\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.principal.mapping.rules = DEFAULT\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n",
      "\ttransaction.max.timeout.ms = 900000\n",
      "\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n",
      "\ttransaction.state.log.load.buffer.size = 5242880\n",
      "\ttransaction.state.log.min.isr = 1\n",
      "\ttransaction.state.log.num.partitions = 50\n",
      "\ttransaction.state.log.replication.factor = 1\n",
      "\ttransaction.state.log.segment.bytes = 104857600\n",
      "\ttransactional.id.expiration.ms = 604800000\n",
      "\tunclean.leader.election.enable = false\n",
      "\tzookeeper.clientCnxnSocket = null\n",
      "\tzookeeper.connect = localhost:9998\n",
      "\tzookeeper.connection.timeout.ms = 18000\n",
      "\tzookeeper.max.in.flight.requests = 10\n",
      "\tzookeeper.session.timeout.ms = 18000\n",
      "\tzookeeper.set.acl = false\n",
      "\tzookeeper.ssl.cipher.suites = null\n",
      "\tzookeeper.ssl.client.enable = false\n",
      "\tzookeeper.ssl.crl.enable = false\n",
      "\tzookeeper.ssl.enabled.protocols = null\n",
      "\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n",
      "\tzookeeper.ssl.keystore.location = null\n",
      "\tzookeeper.ssl.keystore.password = null\n",
      "\tzookeeper.ssl.keystore.type = null\n",
      "\tzookeeper.ssl.ocsp.enable = false\n",
      "\tzookeeper.ssl.protocol = TLSv1.2\n",
      "\tzookeeper.ssl.truststore.location = null\n",
      "\tzookeeper.ssl.truststore.password = null\n",
      "\tzookeeper.ssl.truststore.type = null\n",
      " (kafka.server.KafkaConfig)\n",
      "[2023-02-16 13:08:50,294] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:50,296] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:50,297] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:50,297] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:50,312] INFO Log directory /tmp/tmp9sdfm342/kafka_logs not found, creating it. (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:50,328] INFO Loading logs from log dirs ArraySeq(/tmp/tmp9sdfm342/kafka_logs) (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:50,331] INFO Attempting recovery for all logs in /tmp/tmp9sdfm342/kafka_logs since no clean shutdown file was found (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:50,341] INFO Loaded 0 logs in 13ms. (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:50,342] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:50,344] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:50,446] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-02-16 13:08:50,455] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)\n",
      "[2023-02-16 13:08:50,490] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:50,801] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n",
      "[2023-02-16 13:08:50,804] INFO Awaiting socket connections on 0.0.0.0:9789. (kafka.network.DataPlaneAcceptor)\n",
      "[2023-02-16 13:08:50,828] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n",
      "[2023-02-16 13:08:50,834] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:50,851] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:50,853] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:50,855] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:50,859] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:50,871] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-02-16 13:08:50,896] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)\n",
      "[2023-02-16 13:08:50,921] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1676552930906,1676552930906,1,0,0,72076086674980864,228,0,25\n",
      " (kafka.zk.KafkaZkClient)\n",
      "[2023-02-16 13:08:50,921] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://tvrtko-fastkafka-devel:9789, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)\n",
      "[2023-02-16 13:08:50,980] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:50,987] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:50,989] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:50,996] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)\n",
      "[2023-02-16 13:08:51,004] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-02-16 13:08:51,014] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)\n",
      "[2023-02-16 13:08:51,014] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-02-16 13:08:51,033] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-02-16 13:08:51,039] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-02-16 13:08:51,039] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-02-16 13:08:51,059] INFO [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0). (kafka.server.metadata.ZkMetadataCache)\n",
      "[2023-02-16 13:08:51,087] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:51,147] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-02-16 13:08:51,157] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing. (kafka.network.SocketServer)\n",
      "[2023-02-16 13:08:51,176] INFO Kafka version: 3.3.2 (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-02-16 13:08:51,176] INFO Kafka commitId: b66af662e61082cb (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-02-16 13:08:51,176] INFO Kafka startTimeMs: 1676552931171 (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-02-16 13:08:51,182] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:51,236] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node tvrtko-fastkafka-devel:9789 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:51,305] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node tvrtko-fastkafka-devel:9789 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:53,902] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)\n",
      "[2023-02-16 13:08:53,909] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:53,913] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:53,952] INFO [KafkaServer id=0] Controlled shutdown request returned successfully after 23ms (kafka.server.KafkaServer)\n",
      "[2023-02-16 13:08:53,956] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-02-16 13:08:53,959] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-02-16 13:08:53,961] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-02-16 13:08:53,962] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer)\n",
      "[2023-02-16 13:08:53,968] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer)\n",
      "[2023-02-16 13:08:53,969] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)\n",
      "[2023-02-16 13:08:53,970] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)\n",
      "[2023-02-16 13:08:53,972] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,974] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,974] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,975] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)\n",
      "[2023-02-16 13:08:53,976] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,977] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,977] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,978] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-02-16 13:08:53,979] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)\n",
      "[2023-02-16 13:08:53,979] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-02-16 13:08:53,980] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-02-16 13:08:53,980] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-02-16 13:08:53,980] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-02-16 13:08:53,981] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-02-16 13:08:53,981] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,982] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,982] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,982] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,983] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,983] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,984] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-02-16 13:08:53,984] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)\n",
      "[2023-02-16 13:08:53,984] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-02-16 13:08:53,985] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-02-16 13:08:53,985] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-02-16 13:08:53,985] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)\n",
      "[2023-02-16 13:08:53,986] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)\n",
      "[2023-02-16 13:08:53,986] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)\n",
      "[2023-02-16 13:08:53,986] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)\n",
      "[2023-02-16 13:08:53,986] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,987] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,987] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,988] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,988] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,988] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,988] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,989] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,989] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,989] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,989] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,989] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-02-16 13:08:53,994] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)\n",
      "[2023-02-16 13:08:53,995] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:53,997] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:53,997] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:53,999] INFO Broker to controller channel manager for alterPartition shutdown (kafka.server.BrokerToControllerChannelManagerImpl)\n",
      "[2023-02-16 13:08:53,999] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:54,000] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:54,000] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-02-16 13:08:54,000] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl)\n",
      "[2023-02-16 13:08:54,001] INFO Shutting down. (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:54,025] INFO Shutdown complete. (kafka.log.LogManager)\n",
      "[2023-02-16 13:08:54,030] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-02-16 13:08:54,031] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-02-16 13:08:54,031] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-02-16 13:08:54,032] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-02-16 13:08:54,036] WARN An exception was thrown while closing send thread for session 0x10010d1a7100000. (org.apache.zookeeper.ClientCnxn)\n",
      "EndOfStreamException: Unable to read additional data from server sessionid 0x10010d1a7100000, likely server has closed socket\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77)\n",
      "\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)\n",
      "\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)\n",
      "[2023-02-16 13:08:54,140] INFO Session: 0x10010d1a7100000 closed (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-02-16 13:08:54,140] INFO EventThread shut down for session: 0x10010d1a7100000 (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-02-16 13:08:54,142] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-02-16 13:08:54,143] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:54,147] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:54,147] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:54,148] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:54,148] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:54,148] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:54,149] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:54,149] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:54,149] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:54,150] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:54,150] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:54,150] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-02-16 13:08:54,153] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer)\n",
      "[2023-02-16 13:08:54,192] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer)\n",
      "[2023-02-16 13:08:54,192] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-02-16 13:08:54,192] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-02-16 13:08:54,192] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-02-16 13:08:54,194] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)\n",
      "[2023-02-16 13:08:54,194] INFO App info kafka.server for 0 unregistered (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-02-16 13:08:54,195] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broker = LocalKafkaBroker(zookeeper_port=9998, listener_port=9789)\n",
    "with broker:\n",
    "    print(\"Hello world!\")\n",
    "\n",
    "print(\"*\"*50 + \"ZOOKEEPER LOGS\" + \"+\"*50)\n",
    "zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "print(zookeeper_output.decode(\"UTF-8\"))\n",
    "\n",
    "\n",
    "print(\"*\"*50 + \"KAFKA LOGS\" + \"+\"*50)\n",
    "kafka_output, _ = await broker.kafka_task.communicate()\n",
    "print(kafka_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: LocalKafkaBroker.start(): entering...\n",
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "[INFO] __main__: Zookeeper started, sleeping for 5 seconds...\n",
      "[INFO] __main__: Starting Kafka broker...\n",
      "[INFO] __main__: Kafka broker started, sleeping for 5 seconds...\n",
      "[INFO] __main__: Local Kafka broker up and running on 127.0.0.1:9789\n",
      "[INFO] __main__: LocalKafkaBroker.start(): returning 127.0.0.1:9789\n",
      "[INFO] __main__: LocalKafkaBroker.start(): exited.\n",
      "127.0.0.1:9789\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac62370da084586a5cc8831d3b5394a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generating messages:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'test_data'})\n",
      "[WARNING] aiokafka.cluster: Topic test_data is not available during auto-create initialization\n",
      "[WARNING] aiokafka.cluster: Topic test_data is not available during auto-create initialization\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'test_data': 1}. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b0513b5d7a4339afa90df4422a7378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consuming from 'test_data':   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080b079852284f9f9353e5de52934a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "producing to 'test_data':   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: LocalKafkaBroker.stop(): entering...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 29382...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 29382 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 29017...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 29017 terminated.\n",
      "[INFO] __main__: LocalKafkaBroker.stop(): exited.\n"
     ]
    }
   ],
   "source": [
    "with LocalKafkaBroker(zookeeper_port=9998, listener_port=9789) as bootstrap_servers:\n",
    "    print(bootstrap_servers)\n",
    "    assert bootstrap_servers == \"127.0.0.1:9789\"\n",
    "    \n",
    "    msgs = [\n",
    "        dict(user_id=i, feature_1=[(i / 1_000) ** 2], feature_2=[i % 177])\n",
    "        for i in trange(100_000, desc=\"generating messages\")\n",
    "    ]\n",
    "\n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        tg.soonify(consumes_messages)(\n",
    "            msgs_count=len(msgs), topic=\"test_data\", bootstrap_servers=bootstrap_servers\n",
    "        )\n",
    "\n",
    "        await anyio.sleep(2)\n",
    "\n",
    "        tg.soonify(produce_messages)(\n",
    "            msgs=msgs, topic=\"test_data\", bootstrap_servers=bootstrap_servers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d7ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "[INFO] __main__: Zookeeper started, sleeping for 5 seconds...\n",
      "[INFO] __main__: Starting Kafka broker...\n",
      "[INFO] __main__: Kafka broker started, sleeping for 5 seconds...\n",
      "[INFO] __main__: Local Kafka broker up and running on 127.0.0.1:9789\n",
      "127.0.0.1:9789\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1c81d309b9412bb6b7d131e8fdd53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generating messages:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'test_data'})\n",
      "[WARNING] aiokafka.cluster: Topic test_data is not available during auto-create initialization\n",
      "[WARNING] aiokafka.cluster: Topic test_data is not available during auto-create initialization\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'test_data': 1}. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0199aa24768b437a9a695d29475953a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consuming from 'test_data':   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2196be5308d443aaa02e6023264e0b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "producing to 'test_data':   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 30181...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 30181 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 29816...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 29816 terminated.\n"
     ]
    }
   ],
   "source": [
    "async with LocalKafkaBroker(zookeeper_port=9998, listener_port=9789) as bootstrap_servers:\n",
    "    print(bootstrap_servers)\n",
    "    assert bootstrap_servers == \"127.0.0.1:9789\"\n",
    "    \n",
    "    msgs = [\n",
    "        dict(user_id=i, feature_1=[(i / 1_000) ** 2], feature_2=[i % 177])\n",
    "        for i in trange(100_000, desc=\"generating messages\")\n",
    "    ]\n",
    "\n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        tg.soonify(consumes_messages)(\n",
    "            msgs_count=len(msgs), topic=\"test_data\", bootstrap_servers=bootstrap_servers\n",
    "        )\n",
    "\n",
    "        await anyio.sleep(2)\n",
    "\n",
    "        tg.soonify(produce_messages)(\n",
    "            msgs=msgs, topic=\"test_data\", bootstrap_servers=bootstrap_servers\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
