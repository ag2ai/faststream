{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc959176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import asyncio\n",
    "import contextlib\n",
    "import hashlib\n",
    "import os\n",
    "import random\n",
    "import shlex\n",
    "import subprocess  # nosec: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n",
    "import time\n",
    "import unittest\n",
    "from contextlib import asynccontextmanager, contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Any, Callable, Dict, Generator, List, Optional, Tuple\n",
    "\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "from fast_kafka_api._components.logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# allows async calls in notebooks\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2eb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3eee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__, level=20)\n",
    "logger.debug(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6902fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "kafka_server_url = (\n",
    "    os.environ[\"KAFKA_HOSTNAME\"] if \"KAFKA_HOSTNAME\" in os.environ else \"localhost\"\n",
    ")\n",
    "kafka_server_port = os.environ[\"KAFKA_PORT\"] if \"KAFKA_PORT\" in os.environ else \"9092\"\n",
    "\n",
    "kafka_config = {\n",
    "    \"bootstrap.servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n",
    "    # \"group.id\": f\"{kafka_server_url}:{kafka_server_port}_group\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf46a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def true_after(seconds: float):\n",
    "    \"\"\"Function returning True after a given number of seconds\"\"\"\n",
    "    t = datetime.now()\n",
    "\n",
    "    def _true_after(seconds=seconds, t=t):\n",
    "        return (datetime.now() - t) > timedelta(seconds=seconds)\n",
    "\n",
    "    return _true_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac939ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = true_after(1.1)\n",
    "assert not f()\n",
    "time.sleep(1)\n",
    "assert not f()\n",
    "time.sleep(0.1)\n",
    "assert f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "## TODO: Check if replication num is <= of number of brokers\n",
    "## TODO: Add tests for:\n",
    "#             - Replication factor (less than and greater than number of brokers)\n",
    "#             - Num partitions\n",
    "\n",
    "\n",
    "def create_missing_topics(\n",
    "    admin: AdminClient,\n",
    "    topic_names: List[str],\n",
    "    *,\n",
    "    num_partitions: Optional[int] = None,\n",
    "    replication_factor: Optional[int] = None,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    if not replication_factor:\n",
    "        replication_factor = len(admin.list_topics().brokers)\n",
    "    if not num_partitions:\n",
    "        num_partitions = replication_factor\n",
    "    existing_topics = list(admin.list_topics().topics.keys())\n",
    "    logger.debug(\n",
    "        f\"create_missing_topics({topic_names}): existing_topics={existing_topics}, num_partitions={num_partitions}, replication_factor={replication_factor}\"\n",
    "    )\n",
    "    new_topics = [\n",
    "        NewTopic(\n",
    "            topic,\n",
    "            num_partitions=num_partitions,\n",
    "            replication_factor=replication_factor,\n",
    "            **kwargs,\n",
    "        )\n",
    "        for topic in topic_names\n",
    "        if topic not in existing_topics\n",
    "    ]\n",
    "    if len(new_topics):\n",
    "        logger.info(f\"create_missing_topics({topic_names}): new_topics = {new_topics}\")\n",
    "        fs = admin.create_topics(new_topics)\n",
    "        while not set(topic_names).issubset(set(admin.list_topics().topics.keys())):\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-01-06 08:26:45.855 [INFO] __main__: create_missing_topics(['A', 'B', 'C']): new_topics = [NewTopic(topic=A,num_partitions=3), NewTopic(topic=B,num_partitions=3), NewTopic(topic=C,num_partitions=3)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if topics are created\n",
    "\n",
    "kafka_admin = AdminClient(kafka_config)\n",
    "topics = [\"A\", \"B\", \"C\"]\n",
    "create_missing_topics(kafka_admin, topics)\n",
    "\n",
    "existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "assert set([\"A\", \"B\", \"C\"]) <= existing_topics\n",
    "\n",
    "# Cleanup\n",
    "[\n",
    "    await asyncio.wrap_future(topic, loop=None)\n",
    "    for topic in kafka_admin.delete_topics(topics=topics).values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed690d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def create_testing_topic(\n",
    "    kafka_config: Dict[str, Any], topic_prefix: str, seed: Optional[int] = None\n",
    ") -> Generator[str, None, None]:\n",
    "    # create random topic name\n",
    "    random.seed(seed)\n",
    "    # [B311:blacklist] Standard pseudo-random generators are not suitable for security/cryptographic purposes.\n",
    "    suffix = str(random.randint(0, 10**10))  # nosec\n",
    "\n",
    "    topic = topic_prefix + suffix.zfill(3)\n",
    "\n",
    "    # delete topic if it already exists\n",
    "    admin = AdminClient(kafka_config)\n",
    "    existing_topics = admin.list_topics().topics.keys()\n",
    "    if topic in existing_topics:\n",
    "        logger.warning(f\"topic {topic} exists, deleting it...\")\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        results = {k: f.result() for k, f in fs.items()}\n",
    "        while topic in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)\n",
    "    try:\n",
    "        # create topic if needed\n",
    "        create_missing_topics(admin, [topic])\n",
    "        while topic not in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)\n",
    "        yield topic\n",
    "\n",
    "    finally:\n",
    "        pass\n",
    "        # cleanup if needed again\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        while topic in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a1bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-01-06 08:26:46.924 [INFO] __main__: create_missing_topics(['my_topic_9167024629']): new_topics = [NewTopic(topic=my_topic_9167024629,num_partitions=3)]\n"
     ]
    }
   ],
   "source": [
    "kafka_admin = AdminClient(kafka_config)\n",
    "\n",
    "with create_testing_topic(kafka_config, \"my_topic_\", 1) as topic:\n",
    "    # Check if topic is created and exists in topic list\n",
    "    existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "    assert topic in existing_topics\n",
    "\n",
    "# Check if topic is deleted after exiting context\n",
    "existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "assert topic not in existing_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def create_and_fill_testing_topic(\n",
    "    msgs: List[bytes], kafka_config: Dict[str, str] = kafka_config, *, seed: int\n",
    ") -> Generator[str, None, None]:\n",
    "\n",
    "    with create_testing_topic(kafka_config, \"my_topic_\", seed=seed) as topic:\n",
    "\n",
    "        producer = AIOKafkaProducer(bootstrap_servers=kafka_config[\"bootstrap.servers\"])\n",
    "        logger.info(f\"Producer {producer} created.\")\n",
    "\n",
    "        await producer.start()\n",
    "        logger.info(f\"Producer {producer} started.\")\n",
    "        try:\n",
    "            fx = [\n",
    "                producer.send(\n",
    "                    topic,\n",
    "                    msg,\n",
    "                    key=f\"{i % 17}\".encode(\"utf-8\"),\n",
    "                )\n",
    "                for i, msg in enumerate(msgs)\n",
    "            ]\n",
    "            await producer.flush()\n",
    "            sent_msgs = [await f for f in fx]\n",
    "            msg_statuses = [await s for s in sent_msgs]\n",
    "            logger.info(f\"Sent messages: len(sent_msgs)={len(sent_msgs)}\")\n",
    "\n",
    "            yield topic\n",
    "        finally:\n",
    "            await producer.stop()\n",
    "            logger.info(f\"Producer {producer} stoped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2424896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-01-06 08:26:48.961 [INFO] __main__: create_missing_topics(['my_topic_9167024629']): new_topics = [NewTopic(topic=my_topic_9167024629,num_partitions=3)]\n",
      "23-01-06 08:26:49.964 [INFO] __main__: Producer <aiokafka.producer.producer.AIOKafkaProducer object> created.\n",
      "23-01-06 08:26:49.972 [INFO] __main__: Producer <aiokafka.producer.producer.AIOKafkaProducer object> started.\n",
      "23-01-06 08:26:49.982 [INFO] __main__: Sent messages: len(sent_msgs)=317\n",
      "23-01-06 08:26:49.983 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_9167024629'})\n",
      "23-01-06 08:26:49.983 [INFO] __main__: Consumer <aiokafka.consumer.consumer.AIOKafkaConsumer object> created.\n",
      "23-01-06 08:26:49.988 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_9167024629': 3}. \n",
      "23-01-06 08:26:49.990 [INFO] __main__: Consumer <aiokafka.consumer.consumer.AIOKafkaConsumer object> started.\n",
      "Total messages received: 317\n",
      "23-01-06 08:26:55.038 [INFO] __main__: Consumer <aiokafka.consumer.consumer.AIOKafkaConsumer object> stopped.\n",
      "23-01-06 08:26:55.039 [INFO] __main__: Producer <aiokafka.producer.producer.AIOKafkaProducer object> stoped.\n"
     ]
    }
   ],
   "source": [
    "msgs_sent = 317\n",
    "msgs = [f\"Hello world {i:05d}\".encode(\"utf-8\") for i in range(msgs_sent)]\n",
    "\n",
    "async with create_and_fill_testing_topic(msgs, seed=1) as topic:\n",
    "    consumer = AIOKafkaConsumer(\n",
    "        topic,\n",
    "        bootstrap_servers=kafka_config[\"bootstrap.servers\"],\n",
    "        auto_offset_reset=\"earliest\",\n",
    "        max_poll_records=100,\n",
    "    )\n",
    "    logger.info(f\"Consumer {consumer} created.\")\n",
    "    await consumer.start()\n",
    "    logger.info(f\"Consumer {consumer} started.\")\n",
    "    is_shutting_down_f = true_after(5)\n",
    "    msgs_received = 0\n",
    "    try:\n",
    "        while True:\n",
    "            msgs = await consumer.getmany(timeout_ms=100)\n",
    "            for k, v in msgs.items():\n",
    "                msgs_received = msgs_received + len(v)\n",
    "            if is_shutting_down_f():\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        assert msgs_received == msgs_sent\n",
    "        print(f\"Total messages received: {msgs_received}\")\n",
    "        await consumer.stop()\n",
    "        logger.info(f\"Consumer {consumer} stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Send repeatedly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f69103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def nb_safe_seed(s: str) -> Callable[[int], int]:\n",
    "    \"\"\"Gets a unique seed function for a notebook\n",
    "\n",
    "    Params:\n",
    "        s: name of the notebook used to initialize the seed function\n",
    "\n",
    "    Returns:\n",
    "        A unique seed function\n",
    "    \"\"\"\n",
    "    init_seed = int(hashlib.sha256(s.encode(\"utf-8\")).hexdigest(), 16) % (10**8)\n",
    "\n",
    "    def _get_seed(x: int = 0, *, init_seed: int = init_seed) -> int:\n",
    "        return init_seed + x\n",
    "\n",
    "    return _get_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = nb_safe_seed(\"999_test_utils\")\n",
    "\n",
    "assert seed() == seed(0)\n",
    "assert seed() + 1 == seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mock_AIOKafkaProducer_send():\n",
    "    \"\"\"Mocks **send** method of **AIOKafkaProducer**\"\"\"\n",
    "    with unittest.mock.patch(\"__main__.AIOKafkaProducer.send\") as mock:\n",
    "\n",
    "        async def _f():\n",
    "            pass\n",
    "\n",
    "        mock.return_value = asyncio.create_task(_f())\n",
    "\n",
    "        yield mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def change_dir(d: str):\n",
    "    curdir = os.getcwd()\n",
    "    os.chdir(d)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1eac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tests\n",
    "with TemporaryDirectory() as d:\n",
    "    original_wd = os.getcwd()\n",
    "    assert original_wd != d\n",
    "    with change_dir(d):\n",
    "        assert os.getcwd() == d\n",
    "    assert os.getcwd() == original_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a031f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def run_script_and_cancel(\n",
    "    *, script: str, script_file: str, cmd: str, cancel_after: int\n",
    ") -> bytes:\n",
    "    with TemporaryDirectory() as d:\n",
    "        consumer_script = Path(d) / script_file\n",
    "\n",
    "        with open(consumer_script, \"a+\") as file:\n",
    "            file.write(script)\n",
    "\n",
    "        # os.chdir(d)\n",
    "        with change_dir(d):\n",
    "            proc = subprocess.Popen(  # nosec: [B603:subprocess_without_shell_equals_true] subprocess call - check for execution of untrusted input.\n",
    "                shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
    "            )\n",
    "            time.sleep(cancel_after)\n",
    "            proc.terminate()\n",
    "            proc.wait()\n",
    "\n",
    "        return proc.stdout.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f359e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
