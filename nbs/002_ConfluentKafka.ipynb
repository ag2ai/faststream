{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf503c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f863bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import List, Dict, Any, Optional, Callable, Tuple, Generator\n",
    "from os import environ\n",
    "import string\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import asyncio\n",
    "from asyncio import BaseEventLoop\n",
    "from inspect import iscoroutinefunction\n",
    "\n",
    "import numpy as np\n",
    "import confluent_kafka\n",
    "from confluent_kafka import KafkaException, Consumer, Producer, Message, KafkaError\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import time\n",
    "from threading import Thread\n",
    "from asyncer import syncify\n",
    "\n",
    "import fast_kafka_api.logger\n",
    "\n",
    "fast_kafka_api.logger.should_supress_timestamps = True\n",
    "\n",
    "from fast_kafka_api.logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258fac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "logger = get_logger(__name__, level=0)\n",
    "logger.debug(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pytest\n",
    "\n",
    "import nest_asyncio\n",
    "from rich.pretty import pprint\n",
    "\n",
    "# from fast_kafka_api.kafka.service.servers import create_missing_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "# allows async calls in notebooks\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e887b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def create_missing_topics(\n",
    "    admin: AdminClient,\n",
    "    topic_names: List[str],\n",
    "    *,\n",
    "    num_partitions: Optional[int] = None,\n",
    "    replication_factor: Optional[int] = None,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    if not replication_factor:\n",
    "        replication_factor = len(admin.list_topics().brokers)\n",
    "    if not num_partitions:\n",
    "        num_partitions = replication_factor\n",
    "    existing_topics = list(admin.list_topics().topics.keys())\n",
    "    logger.debug(\n",
    "        f\"create_missing_topics({topic_names}): existing_topics={existing_topics}, num_partitions={num_partitions}, replication_factor={replication_factor}\"\n",
    "    )\n",
    "    new_topics = [\n",
    "        NewTopic(\n",
    "            topic,\n",
    "            num_partitions=num_partitions,\n",
    "            replication_factor=replication_factor,\n",
    "            **kwargs,\n",
    "        )\n",
    "        for topic in topic_names\n",
    "        if topic not in existing_topics\n",
    "    ]\n",
    "    if len(new_topics):\n",
    "        logger.info(f\"create_missing_topics({topic_names}): new_topics = {new_topics}\")\n",
    "        #         nlsep = \"\\n - \"\n",
    "        #         logger.info(f\"create_missing_topics({topic_names}): creating topics:{nlsep}{nlsep.join([str(t) for t in new_topics])}\")\n",
    "        fs = admin.create_topics(new_topics)\n",
    "        results = {k: f.result() for k, f in fs.items()}\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f86682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] __main__: create_missing_topics(['A', 'B', 'C']): existing_topics=['my_topic_error', 'something_random_2', 'my_topic_1', '__consumer_offsets', 'B', 'C', 'training_data', 'prediction_status', 'realitime_data', 'training_request', 'A', 'something_random', 'training_status', 'model_metrics', 'training_model_status', 'prediction', 'prediction_request', 'training_data_status', 'my_topic_4', 'my_topic_2', 'error', 'my_topic_3', 'something_random_3'], num_partitions=1, replication_factor=1\n",
      "Existing topics:\n",
      " - A\n",
      " - B\n",
      " - C\n",
      " - __consumer_offsets\n",
      " - error\n",
      " - model_metrics\n",
      " - my_topic_1\n",
      " - my_topic_2\n",
      " - my_topic_3\n",
      " - my_topic_4\n",
      " - my_topic_error\n",
      " - prediction\n",
      " - prediction_request\n",
      " - prediction_status\n",
      " - realitime_data\n",
      " - something_random\n",
      " - something_random_2\n",
      " - something_random_3\n",
      " - training_data\n",
      " - training_data_status\n",
      " - training_model_status\n",
      " - training_request\n",
      " - training_status\n"
     ]
    }
   ],
   "source": [
    "kafka_server_url = environ[\"KAFKA_HOSTNAME\"]\n",
    "kafka_server_port = environ[\"KAFKA_PORT\"]\n",
    "\n",
    "kafka_config = {\n",
    "    \"bootstrap.servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n",
    "    \"group.id\": f\"{kafka_server_url}:{kafka_server_port}_group\",  # ToDo: Figure out msg deletion from kafka after consuming once\n",
    "    \"auto.offset.reset\": \"earliest\",\n",
    "}\n",
    "\n",
    "kafka_admin = AdminClient(kafka_config)\n",
    "\n",
    "create_missing_topics(kafka_admin, [\"A\", \"B\", \"C\"])\n",
    "existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "print(\"Existing topics:\\n - \" + \"\\n - \".join(sorted(existing_topics)))\n",
    "assert set([\"A\", \"B\", \"C\"]) <= existing_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing topics:\n",
      " - B\n",
      " - C\n",
      " - __consumer_offsets\n",
      " - error\n",
      " - model_metrics\n",
      " - my_topic_1\n",
      " - my_topic_2\n",
      " - my_topic_3\n",
      " - my_topic_4\n",
      " - my_topic_error\n",
      " - prediction\n",
      " - prediction_request\n",
      " - prediction_status\n",
      " - realitime_data\n",
      " - something_random\n",
      " - something_random_2\n",
      " - something_random_3\n",
      " - training_data\n",
      " - training_data_status\n",
      " - training_model_status\n",
      " - training_request\n",
      " - training_status\n"
     ]
    }
   ],
   "source": [
    "fs = kafka_admin.delete_topics(topics=[\"A\"])\n",
    "results = {k: f.result() for k, f in fs.items()}\n",
    "existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "print(\"Existing topics:\\n - \" + \"\\n - \".join(sorted(existing_topics)))\n",
    "assert \"A\" not in existing_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _consume_all_messages(c: Consumer, timeout=0.1, no_retries: int = 25):\n",
    "    while True:\n",
    "        for i in range(no_retries):\n",
    "            msg = c.poll(timeout=timeout)\n",
    "            if msg:\n",
    "                if msg.error():\n",
    "                    logger.warning(f\"Error while consuming message: {msg.error()}\")\n",
    "                break\n",
    "        break\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def create_testing_topic(\n",
    "    kafka_config: Dict[str, Any], topic_prefix: str, seed: int\n",
    ") -> Generator[Tuple[str, Consumer, Producer], None, None]:\n",
    "    # create random topic name\n",
    "    rng = np.random.default_rng(seed)\n",
    "    topic = topic_prefix + \"\".join(rng.choice(list(string.ascii_lowercase), size=10))\n",
    "\n",
    "    # delete topic if it already exists\n",
    "    admin = AdminClient(kafka_config)\n",
    "    existing_topics = admin.list_topics().topics.keys()\n",
    "    if topic in existing_topics:\n",
    "        logger.warning(f\"topic {topic} exists, deleting it...\")\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        results = {k: f.result() for k, f in fs.items()}\n",
    "        time.sleep(1)\n",
    "\n",
    "    try:\n",
    "        # create topic if needed\n",
    "        create_missing_topics(admin, [topic])\n",
    "\n",
    "        # create consumer and producer for the topic\n",
    "        c = Consumer(kafka_config)\n",
    "        c.subscribe([topic])\n",
    "        p = Producer(kafka_config)\n",
    "\n",
    "        yield topic, c, p\n",
    "\n",
    "    finally:\n",
    "        pass\n",
    "        # cleanup if needed again\n",
    "        #         _consume_all_messages(c)\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        results = {k: f.result() for k, f in fs.items()}\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    print(\"*\" * 120)\n",
    "    with create_testing_topic(kafka_config, \"my_topic_\", 723453) as (topic, c, p):\n",
    "        print(f\"{topic}\")\n",
    "\n",
    "    existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "    assert topic not in existing_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c619f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class AIOProducer:\n",
    "    \"\"\"Async producer\n",
    "\n",
    "    Adapted companion code of the blog post \"Integrating Kafka With Python Asyncio Web Applications\"\n",
    "    https://www.confluent.io/blog/kafka-python-asyncio-integration/\n",
    "\n",
    "    https://github.com/confluentinc/confluent-kafka-python/blob/master/examples/asyncio_example.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, config: Dict[str, Any], loop: Optional[BaseEventLoop] = None\n",
    "    ) -> None:\n",
    "        self._loop = loop or asyncio.get_event_loop()\n",
    "        self._producer = Producer(config)\n",
    "        self._cancelled = False\n",
    "        self._poll_thread = Thread(target=self._poll_loop)\n",
    "        self._poll_thread.start()\n",
    "\n",
    "    def _poll_loop(self) -> None:\n",
    "        while not self._cancelled:\n",
    "            self._producer.poll(0.1)\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Shutdowns the pooling thread pool\"\"\"\n",
    "        self._cancelled = True\n",
    "        self._poll_thread.join()\n",
    "\n",
    "    def produce(\n",
    "        self,\n",
    "        topic: str,\n",
    "        value: bytes,\n",
    "        on_delivery: Optional[Callable[[KafkaError, Message], Any]] = None,\n",
    "    ) -> \"asyncio.Future[Any]\":\n",
    "        \"\"\"An awaitable produce method\n",
    "\n",
    "        Params:\n",
    "            topic: name of the topic\n",
    "            value: encoded message\n",
    "            on_delivery: callback function to be called on delivery from a separate thread\n",
    "\n",
    "        Returns:\n",
    "            Awaitable future\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if a coroutine passed as on_delivery\n",
    "        \"\"\"\n",
    "        if on_delivery and iscoroutinefunction(on_delivery):\n",
    "            raise ValueError(\"can only call synchronous code for now\")\n",
    "\n",
    "        result = self._loop.create_future()\n",
    "\n",
    "        def ack(\n",
    "            err: KafkaError,\n",
    "            msg: Message,\n",
    "            self: \"AIOProducer\" = self,\n",
    "            result: \"asyncio.Future[Any]\" = result,\n",
    "            on_delivery: Optional[Callable[[KafkaError, Message], Any]] = on_delivery,\n",
    "        ) -> None:\n",
    "            if err:\n",
    "                self._loop.call_soon_threadsafe(\n",
    "                    result.set_exception, KafkaException(err)\n",
    "                )\n",
    "            else:\n",
    "                self._loop.call_soon_threadsafe(result.set_result, msg)\n",
    "\n",
    "            if on_delivery:\n",
    "                self._loop.call_soon_threadsafe(on_delivery, err, msg)\n",
    "\n",
    "        self._producer.produce(topic, value, on_delivery=ack)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server_url = environ[\"KAFKA_HOSTNAME\"]\n",
    "kafka_server_port = environ[\"KAFKA_PORT\"]\n",
    "\n",
    "config = {\n",
    "    \"bootstrap.servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n",
    "    \"group.id\": f\"{kafka_server_url}:{kafka_server_port}_group\",  # ToDo: Figure out msg deletion from kafka after consuming once\n",
    "    \"auto.offset.reset\": \"earliest\",\n",
    "}\n",
    "\n",
    "# kafka_admin = AdminClient(config)\n",
    "# create_missing_topics(admin=kafka_admin, topic_names=[\"test_topic_aioproducer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_delivery(*args, **kwargs):\n",
    "    logger.info(f\"Hello world!, args={args}, kwargs={kwargs}\")\n",
    "    time.sleep(1)\n",
    "    logger.info(f\"Goodbye world!, args={args}, kwargs={kwargs}\")\n",
    "\n",
    "\n",
    "async def async_on_delivery(*args, **kwargs):\n",
    "    logger.info(f\"Async hello world!, args={args}, kwargs={kwargs}\")\n",
    "\n",
    "\n",
    "async def test_me(topic, config=config):\n",
    "    aioproducer = AIOProducer(config)\n",
    "\n",
    "    fx = [\n",
    "        aioproducer.produce(topic, \"hello\", on_delivery=on_delivery) for _ in range(5)\n",
    "    ]\n",
    "\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        aioproducer.produce(topic, \"hello\", on_delivery=async_on_delivery)\n",
    "    assert str(e.value) == \"can only call synchronous code for now\"\n",
    "\n",
    "    # await fx\n",
    "    pprint([await f for f in fx])\n",
    "\n",
    "    aioproducer.close()\n",
    "\n",
    "\n",
    "with create_testing_topic(kafka_config, \"test_topic_aioproducer_\", 0) as (topic, c, p):\n",
    "    asyncio.run(test_me(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd020f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
