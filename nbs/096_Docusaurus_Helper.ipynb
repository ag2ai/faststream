{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f22162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _docusaurus_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a8972",
   "metadata": {},
   "source": [
    "# Docusaurus Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4b3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harishm/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "import types\n",
    "from inspect import Signature, getmembers, isclass, isfunction, signature\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "from typing import *\n",
    "\n",
    "import yaml\n",
    "\n",
    "from docstring_parser import parse\n",
    "from docstring_parser.common import DocstringParam, DocstringRaises, DocstringReturns\n",
    "from nbdev.config import get_config\n",
    "from nbdev_mkdocs.mkdocs import (\n",
    "    _add_all_submodules,\n",
    "    _get_api_summary,\n",
    "    _import_all_members,\n",
    "    _import_functions_and_classes,\n",
    "    _import_submodules,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import pytest\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69297aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _format_docstring_sections(\n",
    "    items: Union[List[DocstringParam], List[DocstringReturns], List[DocstringRaises]],\n",
    "    keyword: str,\n",
    ") -> str:\n",
    "    \"\"\"Format a list of docstring sections\n",
    "\n",
    "    Args:\n",
    "        items: A list of DocstringParam objects\n",
    "        keyword: The type of section to format (e.g. 'Parameters', 'Returns', 'Exceptions')\n",
    "\n",
    "    Returns:\n",
    "        The formatted docstring.\n",
    "    \"\"\"\n",
    "    formatted_docstring = \"\"\n",
    "    if len(items) > 0:\n",
    "        formatted_docstring += f\"**{keyword}**:\\n\"\n",
    "        for item in items:\n",
    "            if keyword == \"Parameters\":\n",
    "                formatted_docstring += f\"- `{item.arg_name}`: {item.description}\\n\"  # type: ignore\n",
    "            elif keyword == \"Exceptions\":\n",
    "                formatted_docstring += f\"- `{item.type_name}`: {item.description}\\n\"\n",
    "            else:\n",
    "                formatted_docstring += f\"- {item.description}\\n\"\n",
    "        formatted_docstring = f\"{formatted_docstring}\\n\"\n",
    "    return formatted_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c6c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Parameters**:\n",
      "- `name`: name of the person\n",
      "- `age`: age of the person\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fixture = parse(\n",
    "    \"\"\"\n",
    "This is a docstring for a sample function.\n",
    "\n",
    "It can contain multiple lines and can include *markdown* syntax.\n",
    "\n",
    "Args:\n",
    "    name: name of the person\n",
    "    age: age of the person\n",
    "    \n",
    "Returns:\n",
    "    A formatted string\n",
    "\n",
    "Raises:\n",
    "    ValueError: If name is not a string\n",
    "    TypeError: If name is not a string\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "actual = _format_docstring_sections(fixture.params, \"Parameters\")\n",
    "expected = \"\"\"**Parameters**:\n",
    "- `name`: name of the person\n",
    "- `age`: age of the person\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79edf8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Returns**:\n",
      "- A formatted string\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actual = _format_docstring_sections(fixture.many_returns, \"Returns\")\n",
    "expected = \"\"\"**Returns**:\n",
    "- A formatted string\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c682271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Exceptions**:\n",
      "- `ValueError`: If name is not a string\n",
      "- `TypeError`: If name is not a string\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actual = _format_docstring_sections(fixture.raises, \"Exceptions\")\n",
    "expected = \"\"\"**Exceptions**:\n",
    "- `ValueError`: If name is not a string\n",
    "- `TypeError`: If name is not a string\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ccbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _docstring_to_markdown(docstring: str) -> str:\n",
    "    \"\"\"Converts a docstring to a markdown-formatted string.\n",
    "\n",
    "    Args:\n",
    "        docstring: The docstring to convert.\n",
    "\n",
    "    Returns:\n",
    "        The markdown-formatted docstring.\n",
    "    \"\"\"\n",
    "    parsed_docstring = parse(docstring)\n",
    "    formatted_docstring = f\"{parsed_docstring.short_description}\\n\\n\"\n",
    "    formatted_docstring += (\n",
    "        f\"{parsed_docstring.long_description}\\n\\n\"\n",
    "        if parsed_docstring.long_description\n",
    "        else \"\"\n",
    "    )\n",
    "    formatted_docstring += _format_docstring_sections(\n",
    "        parsed_docstring.params, \"Parameters\"\n",
    "    )\n",
    "    formatted_docstring += _format_docstring_sections(\n",
    "        parsed_docstring.many_returns, \"Returns\"\n",
    "    )\n",
    "    formatted_docstring += _format_docstring_sections(\n",
    "        parsed_docstring.raises, \"Exceptions\"\n",
    "    )\n",
    "\n",
    "    return formatted_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a docstring for a sample function.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"This is a docstring for a sample function.\"\"\"\n",
    "\n",
    "expected = \"\"\"This is a docstring for a sample function.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "actual = _docstring_to_markdown(fixture)\n",
    "print(actual)\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd702f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a docstring for a sample function.\n",
      "\n",
      "**Parameters**:\n",
      "- `name`: name of the person\n",
      "- `age`: age of the person\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"This is a docstring for a sample function.\n",
    "\n",
    "Args:\n",
    "    name: name of the person\n",
    "    age: age of the person\n",
    "\"\"\"\n",
    "\n",
    "expected = \"\"\"This is a docstring for a sample function.\n",
    "\n",
    "**Parameters**:\n",
    "- `name`: name of the person\n",
    "- `age`: age of the person\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "actual = _docstring_to_markdown(fixture)\n",
    "print(actual)\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff970bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a docstring for a sample function.\n",
      "\n",
      "It can contain multiple lines and can include *markdown* syntax.\n",
      "\n",
      "**Parameters**:\n",
      "- `name`: name of the person\n",
      "- `age`: age of the person\n",
      "\n",
      "**Returns**:\n",
      "- A formatted string\n",
      "\n",
      "**Exceptions**:\n",
      "- `ValueError`: If name is not a string\n",
      "- `TypeError`: If name is not a string\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"\n",
    "This is a docstring for a sample function.\n",
    "\n",
    "It can contain multiple lines and can include *markdown* syntax.\n",
    "\n",
    "Args:\n",
    "    name: name of the person\n",
    "    age: age of the person\n",
    "    \n",
    "Returns:\n",
    "    A formatted string\n",
    "\n",
    "Raises:\n",
    "    ValueError: If name is not a string\n",
    "    TypeError: If name is not a string\n",
    "\"\"\"\n",
    "\n",
    "expected = \"\"\"This is a docstring for a sample function.\n",
    "\n",
    "It can contain multiple lines and can include *markdown* syntax.\n",
    "\n",
    "**Parameters**:\n",
    "- `name`: name of the person\n",
    "- `age`: age of the person\n",
    "\n",
    "**Returns**:\n",
    "- A formatted string\n",
    "\n",
    "**Exceptions**:\n",
    "- `ValueError`: If name is not a string\n",
    "- `TypeError`: If name is not a string\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "actual = _docstring_to_markdown(fixture)\n",
    "print(actual)\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fbe401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_submodules(module_name: str) -> List[str]:\n",
    "    \"\"\"Get a list of all submodules contained within the module.\n",
    "\n",
    "    Args:\n",
    "        module_name: The name of the module to retrieve submodules from\n",
    "\n",
    "    Returns:\n",
    "        A list of submodule names within the module\n",
    "    \"\"\"\n",
    "    members = _import_all_members(module_name)\n",
    "    members_with_submodules = _add_all_submodules(members)\n",
    "    members_with_submodules_str: List[str] = [\n",
    "        x[:-1] if x.endswith(\".\") else x for x in members_with_submodules\n",
    "    ]\n",
    "    return members_with_submodules_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8c561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fastkafka',\n",
       " 'fastkafka.FastKafka',\n",
       " 'fastkafka.KafkaEvent',\n",
       " 'fastkafka.testing',\n",
       " 'fastkafka.testing.ApacheKafkaBroker',\n",
       " 'fastkafka.testing.LocalRedpandaBroker',\n",
       " 'fastkafka.testing.Tester']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_name = \"fastkafka\"\n",
    "members_with_submodules = _get_submodules(module_name)\n",
    "members_with_submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec44e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _load_submodules(\n",
    "    module_name: str, members_with_submodules: List[str]\n",
    ") -> List[Union[types.FunctionType, Type[Any]]]:\n",
    "    \"\"\"Load the given submodules from the module.\n",
    "\n",
    "    Args:\n",
    "        module_name: The name of the module whose submodules to load\n",
    "        members_with_submodules: A list of submodule names to load\n",
    "\n",
    "    Returns:\n",
    "        A list of imported submodule objects.\n",
    "    \"\"\"\n",
    "    submodules = _import_submodules(module_name)\n",
    "    members: List[Tuple[str, Union[types.FunctionType, Type[Any]]]] = list(\n",
    "        itertools.chain(*[_import_functions_and_classes(m) for m in submodules])\n",
    "    )\n",
    "    names = [\n",
    "        y\n",
    "        for x, y in members\n",
    "        if f\"{y.__module__}.{y.__name__}\" in members_with_submodules\n",
    "    ]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3f5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[fastkafka.FastKafka,\n",
       " fastkafka.KafkaEvent,\n",
       " fastkafka.testing.ApacheKafkaBroker,\n",
       " fastkafka.testing.LocalRedpandaBroker,\n",
       " fastkafka.testing.Tester]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_name = \"fastkafka\"\n",
    "members_with_submodules = _get_submodules(module_name)\n",
    "symbols = _load_submodules(module_name, members_with_submodules)\n",
    "symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bcc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _convert_union_to_optional(annotation_str: str) -> str:\n",
    "    \"\"\"Convert the 'Union[Type1, Type2, ..., NoneType]' to 'Optional[Type1, Type2, ...]' in the given annotation string\n",
    "\n",
    "    Args:\n",
    "        annotation_str: The type annotation string to convert.\n",
    "\n",
    "    Returns:\n",
    "        The converted type annotation string.\n",
    "    \"\"\"\n",
    "    pattern = r\"Union\\[(.*)?,\\s*NoneType\\s*\\]\"\n",
    "    match = re.search(pattern, annotation_str)\n",
    "    if match:\n",
    "        union_type = match.group(1)\n",
    "        optional_type = f\"Optional[{union_type}]\"\n",
    "        return re.sub(pattern, optional_type, annotation_str)\n",
    "    else:\n",
    "        return annotation_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1faa795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg_1: Optional[int] = 80\n",
      "arg_1: Optional[Dict[str, str]]\n",
      "arg_1: Union[Dict[str, str], str]\n",
      "arg_1: str\n",
      "arg_1: bool = False\n",
      "prefix: str = 'to_'\n"
     ]
    }
   ],
   "source": [
    "fixtures = [\n",
    "    {\n",
    "        \"input\": \"arg_1: Union[int, NoneType] = 80\",\n",
    "        \"expected\": \"arg_1: Optional[int] = 80\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"arg_1: Union[Dict[str, str], NoneType]\",\n",
    "        \"expected\": \"arg_1: Optional[Dict[str, str]]\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"arg_1: Union[Dict[str, str], str]\",\n",
    "        \"expected\": \"arg_1: Union[Dict[str, str], str]\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"arg_1: str\",\n",
    "        \"expected\": \"arg_1: str\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"arg_1: bool = False\",\n",
    "        \"expected\": \"arg_1: bool = False\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"prefix: str = 'to_'\",\n",
    "        \"expected\": \"prefix: str = 'to_'\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for fixture in fixtures:\n",
    "    actual = _convert_union_to_optional(fixture[\"input\"])\n",
    "    print(actual)\n",
    "    assert actual == fixture[\"expected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a307153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_arg_list_with_signature(_signature: Signature) -> str:\n",
    "    \"\"\"Converts a function's signature into a string representation of its argument list.\n",
    "\n",
    "    Args:\n",
    "        _signature (signature): The signature object for the function to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: A string representation of the function's argument list.\n",
    "    \"\"\"\n",
    "    arg_list = []\n",
    "    for param in _signature.parameters.values():\n",
    "        arg_list.append(_convert_union_to_optional(str(param)))\n",
    "\n",
    "    return \", \".join(arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de04bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg_1: str, arg_2, arg_3: Union[Dict[str, str], str], arg_4: Optional[int] = 80\n"
     ]
    }
   ],
   "source": [
    "def fixture_function(\n",
    "    arg_1: str, arg_2, arg_3: Union[Dict[str, str], str], arg_4: Optional[int] = 80\n",
    ") -> str:\n",
    "    pass\n",
    "\n",
    "\n",
    "_signature = signature(fixture_function)\n",
    "\n",
    "expected = (\n",
    "    \"arg_1: str, arg_2, arg_3: Union[Dict[str, str], str], arg_4: Optional[int] = 80\"\n",
    ")\n",
    "actual = _get_arg_list_with_signature(_signature)\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101243e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg_1: str, arg_2\n"
     ]
    }
   ],
   "source": [
    "def fixture_function(arg_1: str, arg_2) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "_signature = signature(fixture_function)\n",
    "\n",
    "expected = \"arg_1: str, arg_2\"\n",
    "actual = _get_arg_list_with_signature(_signature)\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46867996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_symbol_definition(symbol: Union[types.FunctionType, Type[Any]]) -> str:\n",
    "    \"\"\"Return the definition of a given symbol.\n",
    "\n",
    "    Args:\n",
    "        symbol: A function or method object to get the definition for.\n",
    "\n",
    "    Returns:\n",
    "        A string representing the function definition\n",
    "    \"\"\"\n",
    "    _signature = signature(symbol)\n",
    "    arg_list = _get_arg_list_with_signature(_signature)\n",
    "    ret_val = \"\"\n",
    "\n",
    "    if isfunction(symbol):\n",
    "        ret_val = f\"### `{symbol.__name__}`\"  + f\" {{#{symbol.__name__.strip('_')}}}\\n\\n\"\n",
    "        ret_val = ret_val + f\"`def {symbol.__name__}({arg_list})\"\n",
    "        if _signature.return_annotation and \"inspect._empty\" not in str(\n",
    "            _signature.return_annotation\n",
    "        ):\n",
    "            if isinstance(_signature.return_annotation, type):\n",
    "                ret_val = ret_val + f\" -> {_signature.return_annotation.__name__}`\\n\"\n",
    "            else:\n",
    "                ret_val = ret_val + f\" -> {_signature.return_annotation}`\\n\"\n",
    "\n",
    "        else:\n",
    "            ret_val = ret_val + \" -> None`\\n\"\n",
    "\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d523b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### `fixture_function` {#fixture_function}\n",
      "\n",
      "`def fixture_function(arg_1: str) -> typing.Callable[[pydantic.main.BaseModel], typing.Optional[typing.Awaitable[NoneType]]]`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TestCallable = Callable[[BaseModel], Union[Awaitable[None], None]]\n",
    "\n",
    "\n",
    "def fixture_function(arg_1: str) -> TestCallable:\n",
    "    pass\n",
    "\n",
    "\n",
    "actual = _get_symbol_definition(fixture_function)\n",
    "print(actual)\n",
    "assert (\n",
    "    \"`def fixture_function(arg_1: str) -> typing.Callable[[pydantic.main.BaseModel]\"\n",
    "    in actual\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f68554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### `fixture_function` {#fixture_function}\n",
      "\n",
      "`def fixture_function(arg_1: str, arg_2) -> None`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fixture_function(arg_1: str, arg_2) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "actual = _get_symbol_definition(fixture_function)\n",
    "expected = \"### `fixture_function` {#fixture_function}\\n\\n`def fixture_function(arg_1: str, arg_2) -> None`\\n\"\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab8ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### `fixture_function` {#fixture_function}\n",
      "\n",
      "`def fixture_function(arg_1: str, arg_2) -> int`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fixture_function(arg_1: str, arg_2) -> int:\n",
    "    pass\n",
    "\n",
    "\n",
    "actual = _get_symbol_definition(fixture_function)\n",
    "expected = \"### `fixture_function` {#fixture_function}\\n\\n`def fixture_function(arg_1: str, arg_2) -> int`\\n\"\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3340df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### `fixture_function` {#fixture_function}\n",
      "\n",
      "`def fixture_function(arg_1: str, arg_2) -> Tester`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fixture_function(arg_1: str, arg_2) -> \"Tester\":\n",
    "    pass\n",
    "\n",
    "\n",
    "actual = _get_symbol_definition(fixture_function)\n",
    "expected = \"### `fixture_function` {#fixture_function}\\n\\n`def fixture_function(arg_1: str, arg_2) -> Tester`\\n\"\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab5994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### `__fixture_function__` {#fixture_function}\n",
      "\n",
      "`def __fixture_function__(arg_1: str, arg_2) -> Tester`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def __fixture_function__(arg_1: str, arg_2) -> \"Tester\":\n",
    "    pass\n",
    "\n",
    "\n",
    "actual = _get_symbol_definition(__fixture_function__)\n",
    "expected = \"### `__fixture_function__` {#fixture_function}\\n\\n`def __fixture_function__(arg_1: str, arg_2) -> Tester`\\n\"\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e1deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### `fixture_function` {#fixture_function}\n",
      "\n",
      "`def fixture_function(arg_1: int, arg_2: str = 'default_string', arg_3: Dict[str, int] = {}, arg_4: Optional[float] = None, arg_5: Tuple[int, str, float] = (1, 'string', 2.0), arg_6: List[Union[int, str]] = [1, 'string'], arg_7: Set[int] = {1, 2, 3}, arg_8: Union[int, str] = 'string') -> None`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fixture_function(\n",
    "    arg_1: int,\n",
    "    arg_2: str = \"default_string\",\n",
    "    arg_3: Dict[str, int] = {},\n",
    "    arg_4: Optional[float] = None,\n",
    "    arg_5: Tuple[int, str, float] = (1, \"string\", 2.0),\n",
    "    arg_6: List[Union[int, str]] = [1, \"string\"],\n",
    "    arg_7: Set[int] = {1, 2, 3},\n",
    "    arg_8: Union[int, str] = \"string\",\n",
    ") -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "actual = _get_symbol_definition(fixture_function)\n",
    "expected = \"### `fixture_function` {#fixture_function}\\n\\n`def fixture_function(arg_1: int, arg_2: str = 'default_string', arg_3: Dict[str, int] = {}, arg_4: Optional[float] = None, arg_5: Tuple[int, str, float] = (1, 'string', 2.0), arg_6: List[Union[int, str]] = [1, 'string'], arg_7: Set[int] = {1, 2, 3}, arg_8: Union[int, str] = 'string') -> None`\\n\"\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e81719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_formatted_docstring_for_symbol(\n",
    "    symbol: Union[types.FunctionType, Type[Any]]\n",
    ") -> str:\n",
    "    \"\"\"Recursively parses and get formatted docstring of a symbol.\n",
    "\n",
    "    Args:\n",
    "        symbol: A Python class or function object to parse the docstring for.\n",
    "\n",
    "    Returns:\n",
    "        A formatted docstring of the symbol and its members.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def traverse(symbol: Union[types.FunctionType, Type[Any]], contents: str) -> str:\n",
    "        \"\"\"Recursively traverse the members of a symbol and append their docstrings to the provided contents string.\n",
    "\n",
    "        Args:\n",
    "            symbol: A Python class or function object to parse the docstring for.\n",
    "            contents: The current formatted docstrings.\n",
    "\n",
    "        Returns:\n",
    "            The updated formatted docstrings.\n",
    "\n",
    "        \"\"\"\n",
    "        for x, y in getmembers(symbol):\n",
    "            if not x.startswith(\"_\") or x.endswith(\"__\"):\n",
    "                if isfunction(y) and y.__doc__ is not None:\n",
    "                    contents += f\"{_get_symbol_definition(y)}\\n{_docstring_to_markdown(y.__doc__)}\"\n",
    "                elif isclass(y) and not x.startswith(\"__\") and y.__doc__ is not None:\n",
    "                    contents += f\"{_get_symbol_definition(y)}\\n{_docstring_to_markdown(y.__doc__)}\"\n",
    "                    contents = traverse(y, contents)\n",
    "        return contents\n",
    "\n",
    "    contents = (\n",
    "        f\"{_get_symbol_definition(symbol)}\\n{_docstring_to_markdown(symbol.__doc__)}\"\n",
    "        if symbol.__doc__ is not None\n",
    "        else \"\"\n",
    "    )\n",
    "    if isclass(symbol):\n",
    "        contents = traverse(symbol, contents)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ae51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### `fixture_function` {#fixture_function}\n",
      "\n",
      "`def fixture_function(arg_1: str, arg_2: Union[List[str], str], arg_3: Optional[int], arg_4: Optional[str] = None) -> str`\n",
      "\n",
      "This is a one line description for the function\n",
      "\n",
      "**Parameters**:\n",
      "- `arg_1`: Argument 1\n",
      "- `arg_2`: Argument 2\n",
      "- `arg_3`: Argument 3\n",
      "- `arg_4`: Argument 4\n",
      "\n",
      "**Returns**:\n",
      "- The concatinated string\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fixture_function(\n",
    "    arg_1: str,\n",
    "    arg_2: Union[List[str], str],\n",
    "    arg_3: Optional[int],\n",
    "    arg_4: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"This is a one line description for the function\n",
    "\n",
    "    Args:\n",
    "        arg_1: Argument 1\n",
    "        arg_2: Argument 2\n",
    "        arg_3: Argument 3\n",
    "        arg_4: Argument 4\n",
    "\n",
    "    Returns:\n",
    "        The concatinated string\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "expected = \"\"\"### `fixture_function` {#fixture_function}\\n\\n`def fixture_function(arg_1: str, arg_2: Union[List[str], str], arg_3: Optional[int], arg_4: Optional[str] = None) -> str`\n",
    "\n",
    "This is a one line description for the function\n",
    "\n",
    "**Parameters**:\n",
    "- `arg_1`: Argument 1\n",
    "- `arg_2`: Argument 2\n",
    "- `arg_3`: Argument 3\n",
    "- `arg_4`: Argument 4\n",
    "\n",
    "**Returns**:\n",
    "- The concatinated string\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "actual = _get_formatted_docstring_for_symbol(fixture_function)\n",
    "print(actual)\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa4c21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is a docstring for the class\n",
      "\n",
      "### `__init__` {#init}\n",
      "\n",
      "`def __init__(self, brand: str, model: str, type: str) -> None`\n",
      "\n",
      "Constructor\n",
      "\n",
      "**Parameters**:\n",
      "- `brand`: Name of the brand\n",
      "- `model`: Name of the model\n",
      "- `type`: Model type\n",
      "\n",
      "### `drive` {#drive}\n",
      "\n",
      "`def drive(self) -> None`\n",
      "\n",
      "Drive\n",
      "\n",
      "### `fuel_up` {#fuel_up}\n",
      "\n",
      "`def fuel_up(self) -> None`\n",
      "\n",
      "Fuel up\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Vehicle:\n",
    "    \"\"\"This is a docstring for the class\"\"\"\n",
    "\n",
    "    def __init__(self, brand: str, model: str, type: str):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            brand: Name of the brand\n",
    "            model: Name of the model\n",
    "            type: Model type\n",
    "        \"\"\"\n",
    "        self.brand = brand\n",
    "        self.model = model\n",
    "        self.type = type\n",
    "        self.gas_tank_size = 14\n",
    "        self.fuel_level = 0\n",
    "\n",
    "    def fuel_up(self):\n",
    "        \"\"\"Fuel up\"\"\"\n",
    "        self.fuel_level = self.gas_tank_size\n",
    "        print(\"Gas tank is now full.\")\n",
    "\n",
    "    def drive(self):\n",
    "        \"\"\"Drive\"\"\"\n",
    "        print(f\"The {self.model} is now driving.\")\n",
    "\n",
    "\n",
    "expected = \"\"\"\n",
    "This is a docstring for the class\n",
    "\n",
    "### `__init__` {#init}\n",
    "\n",
    "`def __init__(self, brand: str, model: str, type: str) -> None`\n",
    "\n",
    "Constructor\n",
    "\n",
    "**Parameters**:\n",
    "- `brand`: Name of the brand\n",
    "- `model`: Name of the model\n",
    "- `type`: Model type\n",
    "\n",
    "### `drive` {#drive}\n",
    "\n",
    "`def drive(self) -> None`\n",
    "\n",
    "Drive\n",
    "\n",
    "### `fuel_up` {#fuel_up}\n",
    "\n",
    "`def fuel_up(self) -> None`\n",
    "\n",
    "Fuel up\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "actual = _get_formatted_docstring_for_symbol(Vehicle)\n",
    "print(actual)\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd400ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outer Class\n",
      "\n",
      "\n",
      "Inner Class\n",
      "\n",
      "### `inner_display` {#inner_display}\n",
      "\n",
      "`def inner_display(self, msg) -> None`\n",
      "\n",
      "Inner display\n",
      "\n",
      "\n",
      "Inner2 Class\n",
      "\n",
      "### `inner_display_2` {#inner_display_2}\n",
      "\n",
      "`def inner_display_2(self, msg) -> None`\n",
      "\n",
      "Inner display_2\n",
      "\n",
      "### `__init__` {#init}\n",
      "\n",
      "`def __init__(self) -> None`\n",
      "\n",
      "Outer class constructor\n",
      "\n",
      "### `reveal` {#reveal}\n",
      "\n",
      "`def reveal(self) -> None`\n",
      "\n",
      "Reveal function\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Outer:\n",
    "    \"\"\"Outer Class\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Outer class constructor\"\"\"\n",
    "        ## instantiating the 'Inner' class\n",
    "        self.inner = self.Inner()\n",
    "\n",
    "    def reveal(self):\n",
    "        \"\"\"Reveal function\"\"\"\n",
    "        ## calling the 'Inner' class function display\n",
    "        self.inner.inner_display(\"Calling Inner class function from Outer class\")\n",
    "\n",
    "    class Inner:\n",
    "        \"\"\"Inner Class\"\"\"\n",
    "\n",
    "        def inner_display(self, msg):\n",
    "            \"\"\"Inner display\"\"\"\n",
    "            print(msg)\n",
    "\n",
    "    class Inner2:\n",
    "        \"\"\"Inner2 Class\"\"\"\n",
    "\n",
    "        def inner_display_2(self, msg):\n",
    "            \"\"\"Inner display_2\"\"\"\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "expected = \"\"\"\n",
    "Outer Class\n",
    "\n",
    "\n",
    "Inner Class\n",
    "\n",
    "### `inner_display` {#inner_display}\n",
    "\n",
    "`def inner_display(self, msg) -> None`\n",
    "\n",
    "Inner display\n",
    "\n",
    "\n",
    "Inner2 Class\n",
    "\n",
    "### `inner_display_2` {#inner_display_2}\n",
    "\n",
    "`def inner_display_2(self, msg) -> None`\n",
    "\n",
    "Inner display_2\n",
    "\n",
    "### `__init__` {#init}\n",
    "\n",
    "`def __init__(self) -> None`\n",
    "\n",
    "Outer class constructor\n",
    "\n",
    "### `reveal` {#reveal}\n",
    "\n",
    "`def reveal(self) -> None`\n",
    "\n",
    "Reveal function\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "actual = _get_formatted_docstring_for_symbol(Outer)\n",
    "print(actual)\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae47ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _convert_html_style_attribute_to_jsx(contents: str) -> str:\n",
    "    \"\"\"Converts the inline style attributes in an HTML string to JSX compatible format.\n",
    "\n",
    "    Args:\n",
    "        contents: A string containing an HTML document or fragment.\n",
    "\n",
    "    Returns:\n",
    "        A string with inline style attributes converted to JSX compatible format.\n",
    "    \"\"\"\n",
    "    style_regex = re.compile(r'style=\"(.+?)\"')\n",
    "    style_matches = style_regex.findall(contents)\n",
    "\n",
    "    for style_match in style_matches:\n",
    "        style_dict = {}\n",
    "        styles = style_match.split(\";\")\n",
    "        for style in styles:\n",
    "            key_value = style.split(\":\")\n",
    "            if len(key_value) == 2:\n",
    "                key = re.sub(\n",
    "                    r\"-(.)\", lambda m: m.group(1).upper(), key_value[0].strip()\n",
    "                )\n",
    "                value = key_value[1].strip().replace(\"'\", '\"')\n",
    "                style_dict[key] = value\n",
    "        replacement = \"style={{\"\n",
    "        for key, value in style_dict.items():\n",
    "            replacement += f\"{key}: '{value}', \"\n",
    "        replacement = replacement[:-2] + \"}}\"\n",
    "        contents = contents.replace(f'style=\"{style_match}\"', replacement)\n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa9e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "<a\n",
      "href=\"https://github.com/airtai/fastkafka/blob/main/fastkafka/_components/test_dependencies.py#L28\"\n",
      "target=\"_blank\" style={{float: 'right', fontSize: 'smaller'}}>source</a> some text goes here <a\n",
      "href=\"https://github.com/airtai/fastkafka/blob/main/fastkafka/_components/test_dependencies.py#L28\"\n",
      "target=\"_blank\" style={{float: 'right', fontSize: 'smaller'}}>source</a>\n",
      "************************************************************************************************************************\n",
      "<span style={{color: 'red'}}>Test</span>\n",
      "************************************************************************************************************************\n",
      "<div style={{backgroundColor: 'blue', border: '1px solid black'}}>Test</div>\n",
      "************************************************************************************************************************\n",
      "<span style={{fontSize: '1.2rem'}}>Test</span>\n",
      "************************************************************************************************************************\n",
      "<pre style={{whiteSpace: 'pre', overflowX: 'auto', lineHeight: 'normal', fontFamily: 'Menlo,\"DejaVu Sans Mono\",consolas,\"Courier New\",monospace'}}></pre>\n"
     ]
    }
   ],
   "source": [
    "fixtures = [\n",
    "    {\n",
    "        \"input\": \"\"\"<a\n",
    "href=\"https://github.com/airtai/fastkafka/blob/main/fastkafka/_components/test_dependencies.py#L28\"\n",
    "target=\"_blank\" style=\"float:right; font-size:smaller\">source</a> some text goes here <a\n",
    "href=\"https://github.com/airtai/fastkafka/blob/main/fastkafka/_components/test_dependencies.py#L28\"\n",
    "target=\"_blank\" style=\"float:right; font-size:smaller\">source</a>\"\"\",\n",
    "        \"expected\": \"\"\"<a\n",
    "href=\"https://github.com/airtai/fastkafka/blob/main/fastkafka/_components/test_dependencies.py#L28\"\n",
    "target=\"_blank\" style={{float: 'right', fontSize: 'smaller'}}>source</a> some text goes here <a\n",
    "href=\"https://github.com/airtai/fastkafka/blob/main/fastkafka/_components/test_dependencies.py#L28\"\n",
    "target=\"_blank\" style={{float: 'right', fontSize: 'smaller'}}>source</a>\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": '<span style=\"color: red;\">Test</span>',\n",
    "        \"expected\": \"<span style={{color: 'red'}}>Test</span>\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": '<div style=\"background-color: blue; border: 1px solid black;\">Test</div>',\n",
    "        \"expected\": \"<div style={{backgroundColor: 'blue', border: '1px solid black'}}>Test</div>\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": '<span style=\"font-size: 1.2rem;\">Test</span>',\n",
    "        \"expected\": \"<span style={{fontSize: '1.2rem'}}>Test</span>\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\"\"\",\n",
    "        \"expected\": \"\"\"<pre style={{whiteSpace: 'pre', overflowX: 'auto', lineHeight: 'normal', fontFamily: 'Menlo,\"DejaVu Sans Mono\",consolas,\"Courier New\",monospace'}}></pre>\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for fixture in fixtures:\n",
    "    actual = _convert_html_style_attribute_to_jsx(fixture[\"input\"])\n",
    "    print(\"*\" * 120)\n",
    "    print(actual)\n",
    "    assert actual == fixture[\"expected\"], fixture[\"expected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fa05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_all_markdown_files_path(docs_path: Path) -> List[Path]:\n",
    "    \"\"\"Get all Markdown files in a directory and its subdirectories.\n",
    "\n",
    "    Args:\n",
    "        directory: The path to the directory to search in.\n",
    "\n",
    "    Returns:\n",
    "        A list of paths to all Markdown files found in the directory and its subdirectories.\n",
    "    \"\"\"\n",
    "    markdown_files = [file_path for file_path in docs_path.glob(\"**/*.md\")]\n",
    "    return markdown_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc72d20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('/var/folders/6n/3rjds7v52cd83wqkd565db0h0000gn/T/tmpteg12omy/docusaurus/docs/file.md'), Path('/var/folders/6n/3rjds7v52cd83wqkd565db0h0000gn/T/tmpteg12omy/docusaurus/docs/blog/file.md'), Path('/var/folders/6n/3rjds7v52cd83wqkd565db0h0000gn/T/tmpteg12omy/docusaurus/docs/api/file.md'), Path('/var/folders/6n/3rjds7v52cd83wqkd565db0h0000gn/T/tmpteg12omy/docusaurus/docs/api/fastKafka/file.md')]\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    module_name = \"fastkafka\"\n",
    "\n",
    "    docs_path = Path(d) / \"docusaurus\" / \"docs\"\n",
    "    docs_path.mkdir(parents=True)\n",
    "\n",
    "    api_path = docs_path / \"api\"\n",
    "    api_path.mkdir(parents=True)\n",
    "\n",
    "    blog_path = docs_path / \"blog\"\n",
    "    blog_path.mkdir(parents=True)\n",
    "\n",
    "    nested_api_path = api_path / \"fastKafka\"\n",
    "    nested_api_path.mkdir(parents=True)\n",
    "\n",
    "    for p in [docs_path, api_path, blog_path, nested_api_path]:\n",
    "        with open((p / \"file.md\"), \"w\") as f:\n",
    "            f.write(\"sample text\")\n",
    "\n",
    "    actual = _get_all_markdown_files_path(docs_path)\n",
    "    expected = [\n",
    "        Path(docs_path) / \"file.md\",\n",
    "        Path(api_path) / \"file.md\",\n",
    "        Path(nested_api_path) / \"file.md\",\n",
    "        Path(blog_path) / \"file.md\",\n",
    "    ]\n",
    "\n",
    "    print(actual)\n",
    "    assert sorted(actual) == sorted(expected), expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc75616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _fix_special_symbols_in_html(contents: str) -> str:\n",
    "    contents = contents.replace(\"”\", '\"')\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd675ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://colab.research.google.com/github/airtai/fastkafka/blob/main/nbs/guides/Guide_00_FastKafka_Demo.ipynb\" target=\"_blank\">\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"<a href=\"https://colab.research.google.com/github/airtai/fastkafka/blob/main/nbs/guides/Guide_00_FastKafka_Demo.ipynb\" target=”_blank”>\"\"\"\n",
    "expected = \"\"\"<a href=\"https://colab.research.google.com/github/airtai/fastkafka/blob/main/nbs/guides/Guide_00_FastKafka_Demo.ipynb\" target=\"_blank\">\"\"\"\n",
    "\n",
    "actual = _fix_special_symbols_in_html(fixture)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68923a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def _add_file_extension_to_link(url: str) -> str:\n",
    "    \"\"\"Add file extension to the last segment of a URL\n",
    "\n",
    "    Args:\n",
    "        url: A URL string.\n",
    "\n",
    "    Returns:\n",
    "        A string of the updated URL with a file extension added to the last segment of the URL.\n",
    "    \"\"\"\n",
    "    segments = url.split(\"/#\")[0].split(\"/\")[-2:]\n",
    "    return url.replace(f\"/{segments[1]}\", f\"/{segments[1]}.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c33927e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://airtai.github.io/fastkafka/api/fastkafka/FastKafka.md/#fastkafka.FastKafka\n"
     ]
    }
   ],
   "source": [
    "fixture = \"https://airtai.github.io/fastkafka/api/fastkafka/FastKafka/#fastkafka.FastKafka\"\n",
    "expected = \"https://airtai.github.io/fastkafka/api/fastkafka/FastKafka.md/#fastkafka.FastKafka\"\n",
    "\n",
    "actual = _add_file_extension_to_link(fixture)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afbe422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://airtai.github.io/fastkafka/api/fastkafka/testing/ApacheKafkaBroker.md/#fastkafka.testing.ApacheKafkaBroker\n"
     ]
    }
   ],
   "source": [
    "fixture = \"https://airtai.github.io/fastkafka/api/fastkafka/testing/ApacheKafkaBroker/#fastkafka.testing.ApacheKafkaBroker\"\n",
    "expected = \"https://airtai.github.io/fastkafka/api/fastkafka/testing/ApacheKafkaBroker.md/#fastkafka.testing.ApacheKafkaBroker\"\n",
    "\n",
    "actual = _add_file_extension_to_link(fixture)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5540873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/airtai/sample_fastkafka_with_redpanda.md\n"
     ]
    }
   ],
   "source": [
    "fixture = \"https://github.com/airtai/sample_fastkafka_with_redpanda\"\n",
    "expected = \"https://github.com/airtai/sample_fastkafka_with_redpanda.md\"\n",
    "\n",
    "actual = _add_file_extension_to_link(fixture)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d131ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _fix_symbol_links(contents: str, dir_prefix: str, doc_host: str, doc_baseurl: str) -> str:\n",
    "    \"\"\"Fix symbol links in Markdown content.\n",
    "\n",
    "    Args:\n",
    "        contents: The Markdown content to search for symbol links.\n",
    "        dir_prefix: Directory prefix to append in the relative URL.\n",
    "        doc_host: The host URL for the documentation site.\n",
    "        doc_baseurl: The base URL for the documentation site.\n",
    "\n",
    "    Returns:\n",
    "        str: The Markdown content with updated symbol links.\n",
    "    \"\"\"\n",
    "    prefix = re.escape(urljoin(doc_host + \"/\", doc_baseurl))\n",
    "    pattern = re.compile(rf\"\\[(.*?)\\]\\(({prefix}[^)]+)\\)\")\n",
    "    matches = pattern.findall(contents)\n",
    "    for match in matches:\n",
    "        old_url = match[1]\n",
    "        new_url = _add_file_extension_to_link(old_url).replace(\n",
    "            \"/api/\", \"/docs/api/\"\n",
    "        )\n",
    "        dir_prefix = \"./\" if dir_prefix == \"\" else dir_prefix\n",
    "        relative_url = dir_prefix + new_url.split(\"/docs/\")[1]\n",
    "        contents = contents.replace(old_url, relative_url)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfbe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the above example,\n",
      "[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
      "[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
      "app is named as `kafka_app`\n",
      "[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"In the above example,\n",
    "[`FastKafka`](https://airtai.github.io/fastkafka/api/fastkafka/FastKafka/#fastkafka.FastKafka)\n",
    "[`FastKafka`](https://airtai.github.io/fastkafka/0.5.0/api/fastkafka/FastKafka/#fastkafka.FastKafka)\n",
    "app is named as `kafka_app`\n",
    "[`FastKafka`](https://airtai.github.io/fastkafka/0.5.0rc0/api/fastkafka/FastKafka/#fastkafka.FastKafka)\"\"\"\n",
    "\n",
    "expected = \"\"\"In the above example,\n",
    "[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
    "[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
    "app is named as `kafka_app`\n",
    "[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\"\"\"\n",
    "\n",
    "dir_prefix = \"../\"\n",
    "doc_host=\"https://airtai.github.io\"\n",
    "doc_baseurl=\"/fastkafka\"\n",
    "actual = _fix_symbol_links(fixture, dir_prefix, doc_host, doc_baseurl)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54241bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the above example,\n",
      "[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
      "[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
      "app is named as `kafka_app`\n",
      "[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"In the above example,\n",
    "[`FastKafka`](https://airtai.github.io/fastkafka/api/fastkafka/FastKafka/#fastkafka.FastKafka)\n",
    "[`FastKafka`](https://airtai.github.io/fastkafka/0.5.0/api/fastkafka/FastKafka/#fastkafka.FastKafka)\n",
    "app is named as `kafka_app`\n",
    "[`FastKafka`](https://airtai.github.io/fastkafka/0.5.0rc0/api/fastkafka/FastKafka/#fastkafka.FastKafka)\"\"\"\n",
    "\n",
    "expected = \"\"\"In the above example,\n",
    "[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
    "[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
    "app is named as `kafka_app`\n",
    "[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\"\"\"\n",
    "\n",
    "dir_prefix = \"\"\n",
    "doc_host=\"https://airtai.github.io\"\n",
    "doc_baseurl=\"/fastkafka\"\n",
    "actual = _fix_symbol_links(fixture, dir_prefix, doc_host, doc_baseurl)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a51204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The service can be tested using the\n",
      "[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
      "[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
      "[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
      "instance and we can start the Kafka\n",
      "broker locally using the\n",
      "[`ApacheKafkaBroker`](./api/fastkafka/testing/ApacheKafkaBroker.md/#fastkafka.testing.ApacheKafkaBroker).\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"The service can be tested using the\n",
    "[`Tester`](https://airtai.github.io/fastkafka/api/fastkafka/testing/Tester/#fastkafka.testing.Tester)\n",
    "[`Tester`](https://airtai.github.io/fastkafka/0.5.0/api/fastkafka/testing/Tester/#fastkafka.testing.Tester)\n",
    "[`Tester`](https://airtai.github.io/fastkafka/dev/api/fastkafka/testing/Tester/#fastkafka.testing.Tester)\n",
    "instance and we can start the Kafka\n",
    "broker locally using the\n",
    "[`ApacheKafkaBroker`](https://airtai.github.io/fastkafka/api/fastkafka/testing/ApacheKafkaBroker/#fastkafka.testing.ApacheKafkaBroker).\"\"\"\n",
    "\n",
    "expected = \"\"\"The service can be tested using the\n",
    "[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
    "[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
    "[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
    "instance and we can start the Kafka\n",
    "broker locally using the\n",
    "[`ApacheKafkaBroker`](./api/fastkafka/testing/ApacheKafkaBroker.md/#fastkafka.testing.ApacheKafkaBroker).\"\"\"\n",
    "\n",
    "dir_prefix = \"\"\n",
    "doc_host=\"https://airtai.github.io\"\n",
    "doc_baseurl=\"/fastkafka\"\n",
    "actual = _fix_symbol_links(fixture, dir_prefix, doc_host, doc_baseurl)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e866b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not a link to a symbol: https://www.google.com\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"This is not a link to a symbol: https://www.google.com\"\"\"\n",
    "\n",
    "expected = \"\"\"This is not a link to a symbol: https://www.google.com\"\"\"\n",
    "\n",
    "dir_prefix = \"\"\n",
    "doc_host=\"https://airtai.github.io\"\n",
    "doc_baseurl=\"/fastkafka\"\n",
    "actual = _fix_symbol_links(fixture, dir_prefix, doc_host, doc_baseurl)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33bf47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample fastkafka-based library that uses Redpanda for testing, based\n",
      "on this guide, can be found\n",
      "[here](https://github.com/airtai/sample_fastkafka_with_redpanda)\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"A sample fastkafka-based library that uses Redpanda for testing, based\n",
    "on this guide, can be found\n",
    "[here](https://github.com/airtai/sample_fastkafka_with_redpanda)\"\"\"\n",
    "\n",
    "expected = \"\"\"A sample fastkafka-based library that uses Redpanda for testing, based\n",
    "on this guide, can be found\n",
    "[here](https://github.com/airtai/sample_fastkafka_with_redpanda)\"\"\"\n",
    "\n",
    "dir_prefix = \"\"\n",
    "doc_host=\"https://airtai.github.io\"\n",
    "doc_baseurl=\"/fastkafka\"\n",
    "actual = _fix_symbol_links(fixture, dir_prefix, doc_host, doc_baseurl)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617b2f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To learn more about Redpanda, please visit their\n",
      "[website](https://redpanda.com/) or checkout this [blog\n",
      "post](https://redpanda.com/blog/redpanda-vs-kafka-performance-benchmark)\n",
      "comparing Redpanda and Kafka’s performance benchmarks.\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"To learn more about Redpanda, please visit their\n",
    "[website](https://redpanda.com/) or checkout this [blog\n",
    "post](https://redpanda.com/blog/redpanda-vs-kafka-performance-benchmark)\n",
    "comparing Redpanda and Kafka’s performance benchmarks.\"\"\"\n",
    "\n",
    "expected = \"\"\"To learn more about Redpanda, please visit their\n",
    "[website](https://redpanda.com/) or checkout this [blog\n",
    "post](https://redpanda.com/blog/redpanda-vs-kafka-performance-benchmark)\n",
    "comparing Redpanda and Kafka’s performance benchmarks.\"\"\"\n",
    "\n",
    "dir_prefix = \"\"\n",
    "doc_host=\"https://airtai.github.io\"\n",
    "doc_baseurl=\"/fastkafka\"\n",
    "actual = _fix_symbol_links(fixture, dir_prefix, doc_host, doc_baseurl)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c76c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def _get_relative_url_prefix(docs_path: Path, sub_path: Path) -> str:\n",
    "    \"\"\"Returns a relative url prefix from a sub path to a docs path.\n",
    "\n",
    "    Args:\n",
    "        docs_path (Path): The docs directory path.\n",
    "        sub_path (Path): The sub directory path.\n",
    "\n",
    "    Returns:\n",
    "        str: A string representing the relative path from the sub path to the docs path.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the sub path is not a descendant of the docs path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        relative_path = sub_path.relative_to(docs_path)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"{sub_path} is not a descendant of {docs_path}\")\n",
    "    \n",
    "    return \"../\" * (len(relative_path.parts) - 1) if len(relative_path.parts) > 1 else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a4524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../\n",
      "../../\n"
     ]
    }
   ],
   "source": [
    "docs_path = Path('docusaurus/docs')\n",
    "\n",
    "sub_path = Path('docusaurus/docs/index.md')\n",
    "actual = _get_relative_url_prefix(docs_path, sub_path) \n",
    "print(actual)\n",
    "assert actual == \"\"\n",
    "\n",
    "sub_path = Path('docusaurus/docs/guides/Guide_31_Using_redpanda_to_test_fastkafka.md')\n",
    "actual = _get_relative_url_prefix(docs_path, sub_path)\n",
    "print(actual)\n",
    "assert actual == \"../\"\n",
    "\n",
    "sub_path = Path('docusaurus/docs/guides/tutorial/fastkafka.md')\n",
    "actual = _get_relative_url_prefix(docs_path, sub_path)\n",
    "print(actual)\n",
    "assert actual == \"../../\"\n",
    "\n",
    "with pytest.raises(ValueError) as e:\n",
    "    sub_path = Path('mkdocs/docs/guides/tutorial/fastkafka.md')\n",
    "    _get_relative_url_prefix(docs_path, sub_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def fix_invalid_syntax_in_markdown(docs_path: str) -> None:\n",
    "    \"\"\"Fix invalid HTML syntax in markdown files and converts inline style attributes to JSX-compatible format.\n",
    "\n",
    "    Args:\n",
    "        docs_path: The path to the root directory to search for markdown files.\n",
    "    \"\"\"\n",
    "    cfg = get_config()\n",
    "    doc_host = cfg[\"doc_host\"]\n",
    "    doc_baseurl = cfg[\"doc_baseurl\"]\n",
    "    \n",
    "    markdown_files = _get_all_markdown_files_path(Path(docs_path))\n",
    "    for file in markdown_files:\n",
    "        relative_url_prefix = _get_relative_url_prefix(Path(docs_path), file)\n",
    "        contents = Path(file).read_text()\n",
    "\n",
    "        contents = _convert_html_style_attribute_to_jsx(contents)\n",
    "        contents = _fix_special_symbols_in_html(contents)\n",
    "        contents = _fix_symbol_links(contents, relative_url_prefix, doc_host, doc_baseurl)\n",
    "\n",
    "        file.write_text(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9e23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "source some text goes here Test and one more tag Test\n",
      "[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
      "[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
      "[here](https://github.com/airtai/sample_fastkafka_with_redpanda)\n",
      "\n",
      "************************************************************************************************************************\n",
      "source some text goes here Test and one more tag Test\n",
      "[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
      "[`Tester`](../api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
      "[here](https://github.com/airtai/sample_fastkafka_with_redpanda)\n",
      "\n",
      "************************************************************************************************************************\n",
      "source some text goes here Test and one more tag Test\n",
      "[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
      "[`Tester`](../api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
      "[here](https://github.com/airtai/sample_fastkafka_with_redpanda)\n",
      "\n",
      "************************************************************************************************************************\n",
      "source some text goes here Test and one more tag Test\n",
      "[`FastKafka`](../../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
      "[`Tester`](../../api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
      "[here](https://github.com/airtai/sample_fastkafka_with_redpanda)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    module_name = \"fastkafka\"\n",
    "\n",
    "    docs_path = Path(d) / \"docusaurus\" / \"docs\"\n",
    "    docs_path.mkdir(parents=True)\n",
    "\n",
    "    api_path = docs_path / \"api\"\n",
    "    api_path.mkdir(parents=True)\n",
    "\n",
    "    blog_path = docs_path / \"blog\"\n",
    "    blog_path.mkdir(parents=True)\n",
    "\n",
    "    nested_api_path = api_path / \"fastKafka\"\n",
    "    nested_api_path.mkdir(parents=True)\n",
    "\n",
    "    for p in [docs_path, api_path, blog_path, nested_api_path]:\n",
    "        with open((p / \"file.md\"), \"w\") as f:\n",
    "            f.write(\n",
    "                \"\"\"source some text goes here Test and one more tag Test\n",
    "[`FastKafka`](https://airtai.github.io/fastkafka/api/fastkafka/FastKafka/#fastkafka.FastKafka)\n",
    "[`Tester`](https://airtai.github.io/fastkafka/api/fastkafka/testing/Tester/#fastkafka.testing.Tester)\n",
    "[here](https://github.com/airtai/sample_fastkafka_with_redpanda)\n",
    "\"\"\"\n",
    "            )\n",
    "\n",
    "    fix_invalid_syntax_in_markdown(str(docs_path))\n",
    "    expected = [\n",
    "        \"\"\"source some text goes here Test and one more tag Test\n",
    "[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
    "[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
    "[here](https://github.com/airtai/sample_fastkafka_with_redpanda)\n",
    "\"\"\",\n",
    "        \"\"\"source some text goes here Test and one more tag Test\n",
    "[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
    "[`Tester`](../api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
    "[here](https://github.com/airtai/sample_fastkafka_with_redpanda)\n",
    "\"\"\",\n",
    "        \"\"\"source some text goes here Test and one more tag Test\n",
    "[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
    "[`Tester`](../api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
    "[here](https://github.com/airtai/sample_fastkafka_with_redpanda)\n",
    "\"\"\",\n",
    "        \"\"\"source some text goes here Test and one more tag Test\n",
    "[`FastKafka`](../../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)\n",
    "[`Tester`](../../api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)\n",
    "[here](https://github.com/airtai/sample_fastkafka_with_redpanda)\n",
    "\"\"\",\n",
    "    ]\n",
    "\n",
    "    for i, p in enumerate([docs_path, api_path, blog_path, nested_api_path]):\n",
    "        with open((p / \"file.md\"), \"r\") as f:\n",
    "            actual = f.read()\n",
    "            print(\"*\" * 120)\n",
    "            print(actual)\n",
    "            assert actual == expected[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def generate_markdown_docs(module_name: str, docs_path: str) -> None:\n",
    "    \"\"\"Generates Markdown documentation files for the symbols in the given module and save them to the given directory.\n",
    "\n",
    "    Args:\n",
    "        module_name: The name of the module to generate documentation for.\n",
    "        docs_path: The path to the directory where the documentation files will be saved.\n",
    "    \"\"\"\n",
    "    members_with_submodules = _get_submodules(module_name)\n",
    "    symbols = _load_submodules(module_name, members_with_submodules)\n",
    "\n",
    "    for symbol in symbols:\n",
    "        content = f\"## `{symbol.__module__}.{symbol.__name__}` {{#{symbol.__module__}.{symbol.__name__}}}\\n\\n\"\n",
    "        content += _get_formatted_docstring_for_symbol(symbol)\n",
    "        target_file_path = (\n",
    "            \"/\".join(f\"{symbol.__module__}.{symbol.__name__}\".split(\".\")) + \".md\"\n",
    "        )\n",
    "\n",
    "        with open((Path(docs_path) / \"api\" / target_file_path), \"w\") as f:\n",
    "            f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51a31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial content in 'fastkafka/FastKafka.md'\n",
      "Initial content in 'fastkafka/KafkaEvent.md'\n",
      "Initial content in 'fastkafka/testing/ApacheKafkaBroker.md'\n",
      "Initial content in 'fastkafka/testing/LocalRedpandaBroker.md'\n",
      "Initial content in 'fastkafka/testing/Tester.md'\n",
      "****************************************************************************************************\n",
      "## `fastkafka.FastKafka` {#fastkafka.FastKafka}\n",
      "\n",
      "### `__init__` {#init}\n",
      "\n",
      "`def __init__(self, title: Optional[str] = None, description: Optional[str] = None, version: Optional[str] = None, contact: Optional[Dict[str, str]] = None, kafka_brokers: Dict[str, Any], root_path: Optional[pathlib.Path, str] = None, lifespan: Optional[Callable[[ForwardRef('FastKafka')], AsyncContextManager[NoneType]]] = None, loop=None, client_id=None, metadata_max_age_ms=300000, request_timeout_ms=40000, api_version='auto', acks=<object object>, key_serializer=None, value_serializer=None, compression_type=None, max_batch_size=16384, partitioner=<kafka.partitioner.default.DefaultPartitioner object>, max_request_size=1048576, linger_ms=0, send_backoff_ms=100, retry_backoff_ms=100, security_protocol='PLAINTEXT', ssl_context=None, connections_max_idle_ms=540000, enable_idempotence=False, transactional_id=None, transaction_timeout_ms=60000, sasl_mechanism='PLAIN', sasl_plain_password=None, sasl_plain_username=None, sasl_kerberos_service_name='kafka', sasl_kerberos_domain_name=None, sasl_oauth_token_provider=None, group_id=None, key_deserializer=None, value_deserializer=None, fetch_max_wait_ms=500, fetch_max_bytes=52428800, fetch_min_bytes=1, max_partition_fetch_bytes=1048576, auto_offset_reset='latest', enable_auto_commit=True, auto_commit_interval_ms=5000, check_crcs=True, partition_assignment_strategy=(<class 'kafka.coordinator.assignors.roundrobin.RoundRobinPartitionAssignor'>,), max_poll_interval_ms=300000, rebalance_timeout_ms=None, session_timeout_ms=10000, heartbeat_interval_ms=3000, consumer_timeout_ms=200, max_poll_records=None, exclude_internal_topics=True, isolation_level='read_uncommitted') -> None`\n",
      "\n",
      "Creates FastKafka application\n",
      "\n",
      "**Parameters**:\n",
      "- `title`: optional title for the documentation. If None,\n",
      "the title will be set to empty string\n",
      "- `description`: optional description for the documentation. If\n",
      "None, the description will be set to empty string\n",
      "- `version`: optional version for the documentation. If None,\n",
      "the version will be set to empty string\n",
      "- `contact`: optional contact for the documentation. If None, the\n",
      "contact will be set to placeholder values:\n",
      "name='Author' url=HttpUrl('https://www.google.com', ) email='noreply@gmail.com'\n",
      "- `kafka_brokers`: dictionary describing kafka brokers used for\n",
      "generating documentation\n",
      "- `root_path`: path to where documentation will be created\n",
      "- `lifespan`: asynccontextmanager that is used for setting lifespan hooks.\n",
      "__aenter__ is called before app start and __aexit__ after app stop.\n",
      "The lifespan is called whe application is started as async context\n",
      "manager, e.g.:`async with kafka_app...`\n",
      "- `client_id`: a name for this client. This string is passed in\n",
      "each request to servers and can be used to identify specific\n",
      "server-side log entries that correspond to this client.\n",
      "Default: ``aiokafka-producer-#`` (appended with a unique number\n",
      "per instance)\n",
      "- `key_serializer`: used to convert user-supplied keys to bytes\n",
      "If not :data:`None`, called as ``f(key),`` should return\n",
      ":class:`bytes`.\n",
      "Default: :data:`None`.\n",
      "- `value_serializer`: used to convert user-supplied message\n",
      "values to :class:`bytes`. If not :data:`None`, called as\n",
      "``f(value)``, should return :class:`bytes`.\n",
      "Default: :data:`None`.\n",
      "- `acks`: one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
      "the producer requires the leader to have received before considering a\n",
      "request complete. This controls the durability of records that are\n",
      "sent. The following settings are common:\n",
      "\n",
      "* ``0``: Producer will not wait for any acknowledgment from the server\n",
      "  at all. The message will immediately be added to the socket\n",
      "  buffer and considered sent. No guarantee can be made that the\n",
      "  server has received the record in this case, and the retries\n",
      "  configuration will not take effect (as the client won't\n",
      "  generally know of any failures). The offset given back for each\n",
      "  record will always be set to -1.\n",
      "* ``1``: The broker leader will write the record to its local log but\n",
      "  will respond without awaiting full acknowledgement from all\n",
      "  followers. In this case should the leader fail immediately\n",
      "  after acknowledging the record but before the followers have\n",
      "  replicated it then the record will be lost.\n",
      "* ``all``: The broker leader will wait for the full set of in-sync\n",
      "  replicas to acknowledge the record. This guarantees that the\n",
      "  record will not be lost as long as at least one in-sync replica\n",
      "  remains alive. This is the strongest available guarantee.\n",
      "\n",
      "If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
      ":data:`True` defaults to ``acks=all``\n",
      "- `compression_type`: The compression type for all data generated by\n",
      "the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
      "or :data:`None`.\n",
      "Compression is of full batches of data, so the efficacy of batching\n",
      "will also impact the compression ratio (more batching means better\n",
      "compression). Default: :data:`None`.\n",
      "- `max_batch_size`: Maximum size of buffered data per partition.\n",
      "After this amount :meth:`send` coroutine will block until batch is\n",
      "drained.\n",
      "Default: 16384\n",
      "- `linger_ms`: The producer groups together any records that arrive\n",
      "in between request transmissions into a single batched request.\n",
      "Normally this occurs only under load when records arrive faster\n",
      "than they can be sent out. However in some circumstances the client\n",
      "may want to reduce the number of requests even under moderate load.\n",
      "This setting accomplishes this by adding a small amount of\n",
      "artificial delay; that is, if first request is processed faster,\n",
      "than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
      "Default: 0 (i.e. no delay).\n",
      "- `partitioner`: Callable used to determine which partition\n",
      "each message is assigned to. Called (after key serialization):\n",
      "``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
      "The default partitioner implementation hashes each non-None key\n",
      "using the same murmur2 algorithm as the Java client so that\n",
      "messages with the same key are assigned to the same partition.\n",
      "When a key is :data:`None`, the message is delivered to a random partition\n",
      "(filtered to partitions with available leaders only, if possible).\n",
      "- `max_request_size`: The maximum size of a request. This is also\n",
      "effectively a cap on the maximum record size. Note that the server\n",
      "has its own cap on record size which may be different from this.\n",
      "This setting will limit the number of record batches the producer\n",
      "will send in a single request to avoid sending huge requests.\n",
      "Default: 1048576.\n",
      "- `metadata_max_age_ms`: The period of time in milliseconds after\n",
      "which we force a refresh of metadata even if we haven't seen any\n",
      "partition leadership changes to proactively discover any new\n",
      "brokers or partitions. Default: 300000\n",
      "- `request_timeout_ms`: Produce request timeout in milliseconds.\n",
      "As it's sent as part of\n",
      ":class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
      "call), maximum waiting time can be up to ``2 *\n",
      "request_timeout_ms``.\n",
      "Default: 40000.\n",
      "- `retry_backoff_ms`: Milliseconds to backoff when retrying on\n",
      "errors. Default: 100.\n",
      "- `api_version`: specify which kafka API version to use.\n",
      "If set to ``auto``, will attempt to infer the broker version by\n",
      "probing various APIs. Default: ``auto``\n",
      "- `security_protocol`: Protocol used to communicate with brokers.\n",
      "Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
      "Default: ``PLAINTEXT``.\n",
      "- `ssl_context`: pre-configured :class:`~ssl.SSLContext`\n",
      "for wrapping socket connections. Directly passed into asyncio's\n",
      ":meth:`~asyncio.loop.create_connection`. For more\n",
      "information see :ref:`ssl_auth`.\n",
      "Default: :data:`None`\n",
      "- `connections_max_idle_ms`: Close idle connections after the number\n",
      "of milliseconds specified by this config. Specifying :data:`None` will\n",
      "disable idle checks. Default: 540000 (9 minutes).\n",
      "- `enable_idempotence`: When set to :data:`True`, the producer will\n",
      "ensure that exactly one copy of each message is written in the\n",
      "stream. If :data:`False`, producer retries due to broker failures,\n",
      "etc., may write duplicates of the retried message in the stream.\n",
      "Note that enabling idempotence acks to set to ``all``. If it is not\n",
      "explicitly set by the user it will be chosen. If incompatible\n",
      "values are set, a :exc:`ValueError` will be thrown.\n",
      "New in version 0.5.0.\n",
      "- `sasl_mechanism`: Authentication mechanism when security_protocol\n",
      "is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
      "are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
      "``OAUTHBEARER``.\n",
      "Default: ``PLAIN``\n",
      "- `sasl_plain_username`: username for SASL ``PLAIN`` authentication.\n",
      "Default: :data:`None`\n",
      "- `sasl_plain_password`: password for SASL ``PLAIN`` authentication.\n",
      "Default: :data:`None`\n",
      "- `sasl_oauth_token_provider (`: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
      "OAuthBearer token provider instance. (See\n",
      ":mod:`kafka.oauth.abstract`).\n",
      "Default: :data:`None`\n",
      "- `*topics`: optional list of topics to subscribe to. If not set,\n",
      "call :meth:`.subscribe` or :meth:`.assign` before consuming records.\n",
      "Passing topics directly is same as calling :meth:`.subscribe` API.\n",
      "- `group_id`: name of the consumer group to join for dynamic\n",
      "partition assignment (if enabled), and to use for fetching and\n",
      "committing offsets. If None, auto-partition assignment (via\n",
      "group coordinator) and offset commits are disabled.\n",
      "Default: None\n",
      "- `key_deserializer`: Any callable that takes a\n",
      "raw message key and returns a deserialized key.\n",
      "- `value_deserializer`: Any callable that takes a\n",
      "raw message value and returns a deserialized value.\n",
      "- `fetch_min_bytes`: Minimum amount of data the server should\n",
      "return for a fetch request, otherwise wait up to\n",
      "`fetch_max_wait_ms` for more data to accumulate. Default: 1.\n",
      "- `fetch_max_bytes`: The maximum amount of data the server should\n",
      "return for a fetch request. This is not an absolute maximum, if\n",
      "the first message in the first non-empty partition of the fetch\n",
      "is larger than this value, the message will still be returned\n",
      "to ensure that the consumer can make progress. NOTE: consumer\n",
      "performs fetches to multiple brokers in parallel so memory\n",
      "usage will depend on the number of brokers containing\n",
      "partitions for the topic.\n",
      "Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).\n",
      "- `fetch_max_wait_ms`: The maximum amount of time in milliseconds\n",
      "the server will block before answering the fetch request if\n",
      "there isn't sufficient data to immediately satisfy the\n",
      "requirement given by fetch_min_bytes. Default: 500.\n",
      "- `max_partition_fetch_bytes`: The maximum amount of data\n",
      "per-partition the server will return. The maximum total memory\n",
      "used for a request ``= #partitions * max_partition_fetch_bytes``.\n",
      "This size must be at least as large as the maximum message size\n",
      "the server allows or else it is possible for the producer to\n",
      "send messages larger than the consumer can fetch. If that\n",
      "happens, the consumer can get stuck trying to fetch a large\n",
      "message on a certain partition. Default: 1048576.\n",
      "- `max_poll_records`: The maximum number of records returned in a\n",
      "single call to :meth:`.getmany`. Defaults ``None``, no limit.\n",
      "- `auto_offset_reset`: A policy for resetting offsets on\n",
      ":exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest\n",
      "available message, ``latest`` will move to the most recent, and\n",
      "``none`` will raise an exception so you can handle this case.\n",
      "Default: ``latest``.\n",
      "- `enable_auto_commit`: If true the consumer's offset will be\n",
      "periodically committed in the background. Default: True.\n",
      "- `auto_commit_interval_ms`: milliseconds between automatic\n",
      "offset commits, if enable_auto_commit is True. Default: 5000.\n",
      "- `check_crcs`: Automatically check the CRC32 of the records\n",
      "consumed. This ensures no on-the-wire or on-disk corruption to\n",
      "the messages occurred. This check adds some overhead, so it may\n",
      "be disabled in cases seeking extreme performance. Default: True\n",
      "- `partition_assignment_strategy`: List of objects to use to\n",
      "distribute partition ownership amongst consumer instances when\n",
      "group management is used. This preference is implicit in the order\n",
      "of the strategies in the list. When assignment strategy changes:\n",
      "to support a change to the assignment strategy, new versions must\n",
      "enable support both for the old assignment strategy and the new\n",
      "one. The coordinator will choose the old assignment strategy until\n",
      "all members have been updated. Then it will choose the new\n",
      "strategy. Default: [:class:`.RoundRobinPartitionAssignor`]\n",
      "- `max_poll_interval_ms`: Maximum allowed time between calls to\n",
      "consume messages (e.g., :meth:`.getmany`). If this interval\n",
      "is exceeded the consumer is considered failed and the group will\n",
      "rebalance in order to reassign the partitions to another consumer\n",
      "group member. If API methods block waiting for messages, that time\n",
      "does not count against this timeout. See `KIP-62`_ for more\n",
      "information. Default 300000\n",
      "- `rebalance_timeout_ms`: The maximum time server will wait for this\n",
      "consumer to rejoin the group in a case of rebalance. In Java client\n",
      "this behaviour is bound to `max.poll.interval.ms` configuration,\n",
      "but as ``aiokafka`` will rejoin the group in the background, we\n",
      "decouple this setting to allow finer tuning by users that use\n",
      ":class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults\n",
      "to ``session_timeout_ms``\n",
      "- `session_timeout_ms`: Client group session and failure detection\n",
      "timeout. The consumer sends periodic heartbeats\n",
      "(`heartbeat.interval.ms`) to indicate its liveness to the broker.\n",
      "If no hearts are received by the broker for a group member within\n",
      "the session timeout, the broker will remove the consumer from the\n",
      "group and trigger a rebalance. The allowed range is configured with\n",
      "the **broker** configuration properties\n",
      "`group.min.session.timeout.ms` and `group.max.session.timeout.ms`.\n",
      "Default: 10000\n",
      "- `heartbeat_interval_ms`: The expected time in milliseconds\n",
      "between heartbeats to the consumer coordinator when using\n",
      "Kafka's group management feature. Heartbeats are used to ensure\n",
      "that the consumer's session stays active and to facilitate\n",
      "rebalancing when new consumers join or leave the group. The\n",
      "value must be set lower than `session_timeout_ms`, but typically\n",
      "should be set no higher than 1/3 of that value. It can be\n",
      "adjusted even lower to control the expected time for normal\n",
      "rebalances. Default: 3000\n",
      "- `consumer_timeout_ms`: maximum wait timeout for background fetching\n",
      "routine. Mostly defines how fast the system will see rebalance and\n",
      "request new data for new partitions. Default: 200\n",
      "- `exclude_internal_topics`: Whether records from internal topics\n",
      "(such as offsets) should be exposed to the consumer. If set to True\n",
      "the only way to receive records from an internal topic is\n",
      "subscribing to it. Requires 0.10+ Default: True\n",
      "- `isolation_level`: Controls how to read messages written\n",
      "transactionally.\n",
      "\n",
      "If set to ``read_committed``, :meth:`.getmany` will only return\n",
      "transactional messages which have been committed.\n",
      "If set to ``read_uncommitted`` (the default), :meth:`.getmany` will\n",
      "return all messages, even transactional messages which have been\n",
      "aborted.\n",
      "\n",
      "Non-transactional messages will be returned unconditionally in\n",
      "either mode.\n",
      "\n",
      "Messages will always be returned in offset order. Hence, in\n",
      "`read_committed` mode, :meth:`.getmany` will only return\n",
      "messages up to the last stable offset (LSO), which is the one less\n",
      "than the offset of the first open transaction. In particular any\n",
      "messages appearing after messages belonging to ongoing transactions\n",
      "will be withheld until the relevant transaction has been completed.\n",
      "As a result, `read_committed` consumers will not be able to read up\n",
      "to the high watermark when there are in flight transactions.\n",
      "Further, when in `read_committed` the seek_to_end method will\n",
      "return the LSO. See method docs below. Default: ``read_uncommitted``\n",
      "- `sasl_oauth_token_provider`: OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).\n",
      "Default: None\n",
      "\n",
      "### `benchmark` {#benchmark}\n",
      "\n",
      "`def benchmark(self: fastkafka.FastKafka, interval: Union[int, datetime.timedelta] = 1, sliding_window_size: Optional[int] = None) -> typing.Callable[[typing.Callable[[~I], typing.Optional[~O]]], typing.Callable[[~I], typing.Optional[~O]]]`\n",
      "\n",
      "Decorator to benchmark produces/consumes functions\n",
      "\n",
      "**Parameters**:\n",
      "- `interval`: Period to use to calculate throughput. If value is of type int,\n",
      "then it will be used as seconds. If value is of type timedelta,\n",
      "then it will be used as it is. default: 1 - one second\n",
      "- `sliding_window_size`: The size of the sliding window to use to calculate\n",
      "average throughput. default: None - By default average throughput is\n",
      "not calculated\n",
      "\n",
      "### `consumes` {#consumes}\n",
      "\n",
      "`def consumes(self: fastkafka.FastKafka, topic: Optional[str] = None, decoder: Union[str, Callable[[bytes, pydantic.main.ModelMetaclass], Any]] = 'json', prefix: str = 'on_', loop=None, bootstrap_servers='localhost', client_id='aiokafka-0.8.0', group_id=None, key_deserializer=None, value_deserializer=None, fetch_max_wait_ms=500, fetch_max_bytes=52428800, fetch_min_bytes=1, max_partition_fetch_bytes=1048576, request_timeout_ms=40000, retry_backoff_ms=100, auto_offset_reset='latest', enable_auto_commit=True, auto_commit_interval_ms=5000, check_crcs=True, metadata_max_age_ms=300000, partition_assignment_strategy=(<class 'kafka.coordinator.assignors.roundrobin.RoundRobinPartitionAssignor'>,), max_poll_interval_ms=300000, rebalance_timeout_ms=None, session_timeout_ms=10000, heartbeat_interval_ms=3000, consumer_timeout_ms=200, max_poll_records=None, ssl_context=None, security_protocol='PLAINTEXT', api_version='auto', exclude_internal_topics=True, connections_max_idle_ms=540000, isolation_level='read_uncommitted', sasl_mechanism='PLAIN', sasl_plain_password=None, sasl_plain_username=None, sasl_kerberos_service_name='kafka', sasl_kerberos_domain_name=None, sasl_oauth_token_provider=None) -> typing.Callable[[typing.Callable[[pydantic.main.BaseModel], typing.Optional[typing.Awaitable[NoneType]]]], typing.Callable[[pydantic.main.BaseModel], typing.Optional[typing.Awaitable[NoneType]]]]`\n",
      "\n",
      "Decorator registering the callback called when a message is received in a topic.\n",
      "\n",
      "This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.\n",
      "\n",
      "**Parameters**:\n",
      "- `topic`: Kafka topic that the consumer will subscribe to and execute the\n",
      "decorated function when it receives a message from the topic,\n",
      "default: None. If the topic is not specified, topic name will be\n",
      "inferred from the decorated function name by stripping the defined prefix\n",
      "- `decoder`: Decoder to use to decode messages consumed from the topic,\n",
      "default: json - By default, it uses json decoder to decode\n",
      "bytes to json string and then it creates instance of pydantic\n",
      "BaseModel. It also accepts custom decoder function.\n",
      "- `prefix`: Prefix stripped from the decorated function to define a topic name\n",
      "if the topic argument is not passed, default: \"on_\". If the decorated\n",
      "function name is not prefixed with the defined prefix and topic argument\n",
      "is not passed, then this method will throw ValueError\n",
      "- `*topics`: optional list of topics to subscribe to. If not set,\n",
      "call :meth:`.subscribe` or :meth:`.assign` before consuming records.\n",
      "Passing topics directly is same as calling :meth:`.subscribe` API.\n",
      "- `bootstrap_servers`: a ``host[:port]`` string (or list of\n",
      "``host[:port]`` strings) that the consumer should contact to bootstrap\n",
      "initial cluster metadata.\n",
      "\n",
      "This does not have to be the full node list.\n",
      "It just needs to have at least one broker that will respond to a\n",
      "Metadata API Request. Default port is 9092. If no servers are\n",
      "specified, will default to ``localhost:9092``.\n",
      "- `client_id`: a name for this client. This string is passed in\n",
      "each request to servers and can be used to identify specific\n",
      "server-side log entries that correspond to this client. Also\n",
      "submitted to :class:`~.consumer.group_coordinator.GroupCoordinator`\n",
      "for logging with respect to consumer group administration. Default:\n",
      "``aiokafka-{version}``\n",
      "- `group_id`: name of the consumer group to join for dynamic\n",
      "partition assignment (if enabled), and to use for fetching and\n",
      "committing offsets. If None, auto-partition assignment (via\n",
      "group coordinator) and offset commits are disabled.\n",
      "Default: None\n",
      "- `key_deserializer`: Any callable that takes a\n",
      "raw message key and returns a deserialized key.\n",
      "- `value_deserializer`: Any callable that takes a\n",
      "raw message value and returns a deserialized value.\n",
      "- `fetch_min_bytes`: Minimum amount of data the server should\n",
      "return for a fetch request, otherwise wait up to\n",
      "`fetch_max_wait_ms` for more data to accumulate. Default: 1.\n",
      "- `fetch_max_bytes`: The maximum amount of data the server should\n",
      "return for a fetch request. This is not an absolute maximum, if\n",
      "the first message in the first non-empty partition of the fetch\n",
      "is larger than this value, the message will still be returned\n",
      "to ensure that the consumer can make progress. NOTE: consumer\n",
      "performs fetches to multiple brokers in parallel so memory\n",
      "usage will depend on the number of brokers containing\n",
      "partitions for the topic.\n",
      "Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).\n",
      "- `fetch_max_wait_ms`: The maximum amount of time in milliseconds\n",
      "the server will block before answering the fetch request if\n",
      "there isn't sufficient data to immediately satisfy the\n",
      "requirement given by fetch_min_bytes. Default: 500.\n",
      "- `max_partition_fetch_bytes`: The maximum amount of data\n",
      "per-partition the server will return. The maximum total memory\n",
      "used for a request ``= #partitions * max_partition_fetch_bytes``.\n",
      "This size must be at least as large as the maximum message size\n",
      "the server allows or else it is possible for the producer to\n",
      "send messages larger than the consumer can fetch. If that\n",
      "happens, the consumer can get stuck trying to fetch a large\n",
      "message on a certain partition. Default: 1048576.\n",
      "- `max_poll_records`: The maximum number of records returned in a\n",
      "single call to :meth:`.getmany`. Defaults ``None``, no limit.\n",
      "- `request_timeout_ms`: Client request timeout in milliseconds.\n",
      "Default: 40000.\n",
      "- `retry_backoff_ms`: Milliseconds to backoff when retrying on\n",
      "errors. Default: 100.\n",
      "- `auto_offset_reset`: A policy for resetting offsets on\n",
      ":exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest\n",
      "available message, ``latest`` will move to the most recent, and\n",
      "``none`` will raise an exception so you can handle this case.\n",
      "Default: ``latest``.\n",
      "- `enable_auto_commit`: If true the consumer's offset will be\n",
      "periodically committed in the background. Default: True.\n",
      "- `auto_commit_interval_ms`: milliseconds between automatic\n",
      "offset commits, if enable_auto_commit is True. Default: 5000.\n",
      "- `check_crcs`: Automatically check the CRC32 of the records\n",
      "consumed. This ensures no on-the-wire or on-disk corruption to\n",
      "the messages occurred. This check adds some overhead, so it may\n",
      "be disabled in cases seeking extreme performance. Default: True\n",
      "- `metadata_max_age_ms`: The period of time in milliseconds after\n",
      "which we force a refresh of metadata even if we haven't seen any\n",
      "partition leadership changes to proactively discover any new\n",
      "brokers or partitions. Default: 300000\n",
      "- `partition_assignment_strategy`: List of objects to use to\n",
      "distribute partition ownership amongst consumer instances when\n",
      "group management is used. This preference is implicit in the order\n",
      "of the strategies in the list. When assignment strategy changes:\n",
      "to support a change to the assignment strategy, new versions must\n",
      "enable support both for the old assignment strategy and the new\n",
      "one. The coordinator will choose the old assignment strategy until\n",
      "all members have been updated. Then it will choose the new\n",
      "strategy. Default: [:class:`.RoundRobinPartitionAssignor`]\n",
      "- `max_poll_interval_ms`: Maximum allowed time between calls to\n",
      "consume messages (e.g., :meth:`.getmany`). If this interval\n",
      "is exceeded the consumer is considered failed and the group will\n",
      "rebalance in order to reassign the partitions to another consumer\n",
      "group member. If API methods block waiting for messages, that time\n",
      "does not count against this timeout. See `KIP-62`_ for more\n",
      "information. Default 300000\n",
      "- `rebalance_timeout_ms`: The maximum time server will wait for this\n",
      "consumer to rejoin the group in a case of rebalance. In Java client\n",
      "this behaviour is bound to `max.poll.interval.ms` configuration,\n",
      "but as ``aiokafka`` will rejoin the group in the background, we\n",
      "decouple this setting to allow finer tuning by users that use\n",
      ":class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults\n",
      "to ``session_timeout_ms``\n",
      "- `session_timeout_ms`: Client group session and failure detection\n",
      "timeout. The consumer sends periodic heartbeats\n",
      "(`heartbeat.interval.ms`) to indicate its liveness to the broker.\n",
      "If no hearts are received by the broker for a group member within\n",
      "the session timeout, the broker will remove the consumer from the\n",
      "group and trigger a rebalance. The allowed range is configured with\n",
      "the **broker** configuration properties\n",
      "`group.min.session.timeout.ms` and `group.max.session.timeout.ms`.\n",
      "Default: 10000\n",
      "- `heartbeat_interval_ms`: The expected time in milliseconds\n",
      "between heartbeats to the consumer coordinator when using\n",
      "Kafka's group management feature. Heartbeats are used to ensure\n",
      "that the consumer's session stays active and to facilitate\n",
      "rebalancing when new consumers join or leave the group. The\n",
      "value must be set lower than `session_timeout_ms`, but typically\n",
      "should be set no higher than 1/3 of that value. It can be\n",
      "adjusted even lower to control the expected time for normal\n",
      "rebalances. Default: 3000\n",
      "- `consumer_timeout_ms`: maximum wait timeout for background fetching\n",
      "routine. Mostly defines how fast the system will see rebalance and\n",
      "request new data for new partitions. Default: 200\n",
      "- `api_version`: specify which kafka API version to use.\n",
      ":class:`AIOKafkaConsumer` supports Kafka API versions >=0.9 only.\n",
      "If set to ``auto``, will attempt to infer the broker version by\n",
      "probing various APIs. Default: ``auto``\n",
      "- `security_protocol`: Protocol used to communicate with brokers.\n",
      "Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
      "- `ssl_context`: pre-configured :class:`~ssl.SSLContext`\n",
      "for wrapping socket connections. Directly passed into asyncio's\n",
      ":meth:`~asyncio.loop.create_connection`. For more information see\n",
      ":ref:`ssl_auth`. Default: None.\n",
      "- `exclude_internal_topics`: Whether records from internal topics\n",
      "(such as offsets) should be exposed to the consumer. If set to True\n",
      "the only way to receive records from an internal topic is\n",
      "subscribing to it. Requires 0.10+ Default: True\n",
      "- `connections_max_idle_ms`: Close idle connections after the number\n",
      "of milliseconds specified by this config. Specifying `None` will\n",
      "disable idle checks. Default: 540000 (9 minutes).\n",
      "- `isolation_level`: Controls how to read messages written\n",
      "transactionally.\n",
      "\n",
      "If set to ``read_committed``, :meth:`.getmany` will only return\n",
      "transactional messages which have been committed.\n",
      "If set to ``read_uncommitted`` (the default), :meth:`.getmany` will\n",
      "return all messages, even transactional messages which have been\n",
      "aborted.\n",
      "\n",
      "Non-transactional messages will be returned unconditionally in\n",
      "either mode.\n",
      "\n",
      "Messages will always be returned in offset order. Hence, in\n",
      "`read_committed` mode, :meth:`.getmany` will only return\n",
      "messages up to the last stable offset (LSO), which is the one less\n",
      "than the offset of the first open transaction. In particular any\n",
      "messages appearing after messages belonging to ongoing transactions\n",
      "will be withheld until the relevant transaction has been completed.\n",
      "As a result, `read_committed` consumers will not be able to read up\n",
      "to the high watermark when there are in flight transactions.\n",
      "Further, when in `read_committed` the seek_to_end method will\n",
      "return the LSO. See method docs below. Default: ``read_uncommitted``\n",
      "- `sasl_mechanism`: Authentication mechanism when security_protocol\n",
      "is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values are:\n",
      "``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
      "``OAUTHBEARER``.\n",
      "Default: ``PLAIN``\n",
      "- `sasl_plain_username`: username for SASL ``PLAIN`` authentication.\n",
      "Default: None\n",
      "- `sasl_plain_password`: password for SASL ``PLAIN`` authentication.\n",
      "Default: None\n",
      "- `sasl_oauth_token_provider`: OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).\n",
      "Default: None\n",
      "\n",
      "**Returns**:\n",
      "- : A function returning the same function\n",
      "\n",
      "### `create_mocks` {#create_mocks}\n",
      "\n",
      "`def create_mocks(self: fastkafka.FastKafka) -> None`\n",
      "\n",
      "Creates self.mocks as a named tuple mapping a new function obtained by calling the original functions and a mock\n",
      "\n",
      "### `produces` {#produces}\n",
      "\n",
      "`def produces(self: fastkafka.FastKafka, topic: Optional[str] = None, encoder: Union[str, Callable[[pydantic.main.BaseModel], bytes]] = 'json', prefix: str = 'to_', loop=None, bootstrap_servers='localhost', client_id=None, metadata_max_age_ms=300000, request_timeout_ms=40000, api_version='auto', acks=<object object>, key_serializer=None, value_serializer=None, compression_type=None, max_batch_size=16384, partitioner=<kafka.partitioner.default.DefaultPartitioner object>, max_request_size=1048576, linger_ms=0, send_backoff_ms=100, retry_backoff_ms=100, security_protocol='PLAINTEXT', ssl_context=None, connections_max_idle_ms=540000, enable_idempotence=False, transactional_id=None, transaction_timeout_ms=60000, sasl_mechanism='PLAIN', sasl_plain_password=None, sasl_plain_username=None, sasl_kerberos_service_name='kafka', sasl_kerberos_domain_name=None, sasl_oauth_token_provider=None) -> typing.Callable[[typing.Union[typing.Callable[..., typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel]]], typing.Callable[..., typing.Awaitable[typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel]]]]]], typing.Union[typing.Callable[..., typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel]]], typing.Callable[..., typing.Awaitable[typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel]]]]]]`\n",
      "\n",
      "Decorator registering the callback called when delivery report for a produced message is received\n",
      "\n",
      "This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.\n",
      "\n",
      "**Parameters**:\n",
      "- `topic`: Kafka topic that the producer will send returned values from\n",
      "the decorated function to, default: None- If the topic is not\n",
      "specified, topic name will be inferred from the decorated function\n",
      "name by stripping the defined prefix.\n",
      "- `encoder`: Encoder to use to encode messages before sending it to topic,\n",
      "default: json - By default, it uses json encoder to convert\n",
      "pydantic basemodel to json string and then encodes the string to bytes\n",
      "using 'utf-8' encoding. It also accepts custom encoder function.\n",
      "- `prefix`: Prefix stripped from the decorated function to define a topic\n",
      "name if the topic argument is not passed, default: \"to_\". If the\n",
      "decorated function name is not prefixed with the defined prefix\n",
      "and topic argument is not passed, then this method will throw ValueError\n",
      "- `bootstrap_servers`: a ``host[:port]`` string or list of\n",
      "``host[:port]`` strings that the producer should contact to\n",
      "bootstrap initial cluster metadata. This does not have to be the\n",
      "full node list.  It just needs to have at least one broker that will\n",
      "respond to a Metadata API Request. Default port is 9092. If no\n",
      "servers are specified, will default to ``localhost:9092``.\n",
      "- `client_id`: a name for this client. This string is passed in\n",
      "each request to servers and can be used to identify specific\n",
      "server-side log entries that correspond to this client.\n",
      "Default: ``aiokafka-producer-#`` (appended with a unique number\n",
      "per instance)\n",
      "- `key_serializer`: used to convert user-supplied keys to bytes\n",
      "If not :data:`None`, called as ``f(key),`` should return\n",
      ":class:`bytes`.\n",
      "Default: :data:`None`.\n",
      "- `value_serializer`: used to convert user-supplied message\n",
      "values to :class:`bytes`. If not :data:`None`, called as\n",
      "``f(value)``, should return :class:`bytes`.\n",
      "Default: :data:`None`.\n",
      "- `acks`: one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
      "the producer requires the leader to have received before considering a\n",
      "request complete. This controls the durability of records that are\n",
      "sent. The following settings are common:\n",
      "\n",
      "* ``0``: Producer will not wait for any acknowledgment from the server\n",
      "  at all. The message will immediately be added to the socket\n",
      "  buffer and considered sent. No guarantee can be made that the\n",
      "  server has received the record in this case, and the retries\n",
      "  configuration will not take effect (as the client won't\n",
      "  generally know of any failures). The offset given back for each\n",
      "  record will always be set to -1.\n",
      "* ``1``: The broker leader will write the record to its local log but\n",
      "  will respond without awaiting full acknowledgement from all\n",
      "  followers. In this case should the leader fail immediately\n",
      "  after acknowledging the record but before the followers have\n",
      "  replicated it then the record will be lost.\n",
      "* ``all``: The broker leader will wait for the full set of in-sync\n",
      "  replicas to acknowledge the record. This guarantees that the\n",
      "  record will not be lost as long as at least one in-sync replica\n",
      "  remains alive. This is the strongest available guarantee.\n",
      "\n",
      "If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
      ":data:`True` defaults to ``acks=all``\n",
      "- `compression_type`: The compression type for all data generated by\n",
      "the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
      "or :data:`None`.\n",
      "Compression is of full batches of data, so the efficacy of batching\n",
      "will also impact the compression ratio (more batching means better\n",
      "compression). Default: :data:`None`.\n",
      "- `max_batch_size`: Maximum size of buffered data per partition.\n",
      "After this amount :meth:`send` coroutine will block until batch is\n",
      "drained.\n",
      "Default: 16384\n",
      "- `linger_ms`: The producer groups together any records that arrive\n",
      "in between request transmissions into a single batched request.\n",
      "Normally this occurs only under load when records arrive faster\n",
      "than they can be sent out. However in some circumstances the client\n",
      "may want to reduce the number of requests even under moderate load.\n",
      "This setting accomplishes this by adding a small amount of\n",
      "artificial delay; that is, if first request is processed faster,\n",
      "than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
      "Default: 0 (i.e. no delay).\n",
      "- `partitioner`: Callable used to determine which partition\n",
      "each message is assigned to. Called (after key serialization):\n",
      "``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
      "The default partitioner implementation hashes each non-None key\n",
      "using the same murmur2 algorithm as the Java client so that\n",
      "messages with the same key are assigned to the same partition.\n",
      "When a key is :data:`None`, the message is delivered to a random partition\n",
      "(filtered to partitions with available leaders only, if possible).\n",
      "- `max_request_size`: The maximum size of a request. This is also\n",
      "effectively a cap on the maximum record size. Note that the server\n",
      "has its own cap on record size which may be different from this.\n",
      "This setting will limit the number of record batches the producer\n",
      "will send in a single request to avoid sending huge requests.\n",
      "Default: 1048576.\n",
      "- `metadata_max_age_ms`: The period of time in milliseconds after\n",
      "which we force a refresh of metadata even if we haven't seen any\n",
      "partition leadership changes to proactively discover any new\n",
      "brokers or partitions. Default: 300000\n",
      "- `request_timeout_ms`: Produce request timeout in milliseconds.\n",
      "As it's sent as part of\n",
      ":class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
      "call), maximum waiting time can be up to ``2 *\n",
      "request_timeout_ms``.\n",
      "Default: 40000.\n",
      "- `retry_backoff_ms`: Milliseconds to backoff when retrying on\n",
      "errors. Default: 100.\n",
      "- `api_version`: specify which kafka API version to use.\n",
      "If set to ``auto``, will attempt to infer the broker version by\n",
      "probing various APIs. Default: ``auto``\n",
      "- `security_protocol`: Protocol used to communicate with brokers.\n",
      "Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
      "Default: ``PLAINTEXT``.\n",
      "- `ssl_context`: pre-configured :class:`~ssl.SSLContext`\n",
      "for wrapping socket connections. Directly passed into asyncio's\n",
      ":meth:`~asyncio.loop.create_connection`. For more\n",
      "information see :ref:`ssl_auth`.\n",
      "Default: :data:`None`\n",
      "- `connections_max_idle_ms`: Close idle connections after the number\n",
      "of milliseconds specified by this config. Specifying :data:`None` will\n",
      "disable idle checks. Default: 540000 (9 minutes).\n",
      "- `enable_idempotence`: When set to :data:`True`, the producer will\n",
      "ensure that exactly one copy of each message is written in the\n",
      "stream. If :data:`False`, producer retries due to broker failures,\n",
      "etc., may write duplicates of the retried message in the stream.\n",
      "Note that enabling idempotence acks to set to ``all``. If it is not\n",
      "explicitly set by the user it will be chosen. If incompatible\n",
      "values are set, a :exc:`ValueError` will be thrown.\n",
      "New in version 0.5.0.\n",
      "- `sasl_mechanism`: Authentication mechanism when security_protocol\n",
      "is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
      "are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
      "``OAUTHBEARER``.\n",
      "Default: ``PLAIN``\n",
      "- `sasl_plain_username`: username for SASL ``PLAIN`` authentication.\n",
      "Default: :data:`None`\n",
      "- `sasl_plain_password`: password for SASL ``PLAIN`` authentication.\n",
      "Default: :data:`None`\n",
      "- `sasl_oauth_token_provider (`: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
      "OAuthBearer token provider instance. (See\n",
      ":mod:`kafka.oauth.abstract`).\n",
      "Default: :data:`None`\n",
      "\n",
      "**Returns**:\n",
      "- : A function returning the same function\n",
      "\n",
      "**Exceptions**:\n",
      "- `ValueError`: when needed\n",
      "\n",
      "### `run_in_background` {#run_in_background}\n",
      "\n",
      "`def run_in_background(self: fastkafka.FastKafka) -> typing.Callable[[typing.Callable[..., typing.Coroutine[typing.Any, typing.Any, typing.Any]]], typing.Callable[..., typing.Coroutine[typing.Any, typing.Any, typing.Any]]]`\n",
      "\n",
      "Decorator to schedule a task to be run in the background.\n",
      "\n",
      "This decorator is used to schedule a task to be run in the background when the app's `_on_startup` event is triggered.\n",
      "\n",
      "**Returns**:\n",
      "- A decorator function that takes a background task as an input and stores it to be run in the backround.\n",
      "\n",
      "\n",
      "## `fastkafka.KafkaEvent` {#fastkafka.KafkaEvent}\n",
      "\n",
      "\n",
      "A generic class for representing Kafka events. Based on BaseSubmodel, bound to pydantic.BaseModel\n",
      "\n",
      "**Parameters**:\n",
      "- `message`: The message contained in the Kafka event, can be of type pydantic.BaseModel.\n",
      "- `key`: The optional key used to identify the Kafka event.\n",
      "\n",
      "\n",
      "## `fastkafka.testing.ApacheKafkaBroker` {#fastkafka.testing.ApacheKafkaBroker}\n",
      "\n",
      "\n",
      "ApacheKafkaBroker class, used for running unique kafka brokers in tests to prevent topic clashing.\n",
      "\n",
      "### `__init__` {#init}\n",
      "\n",
      "`def __init__(self, topics: Iterable[str] = [], retries: int = 3, apply_nest_asyncio: bool = False, zookeeper_port: int = 2181, listener_port: int = 9092) -> None`\n",
      "\n",
      "Initialises the ApacheKafkaBroker object\n",
      "\n",
      "**Parameters**:\n",
      "- `data_dir`: Path to the directory where the zookeepeer instance will save data\n",
      "- `zookeeper_port`: Port for clients (Kafka brokes) to connect\n",
      "- `listener_port`: Port on which the clients (producers and consumers) can connect\n",
      "\n",
      "### `start` {#start}\n",
      "\n",
      "`def start(self: fastkafka.testing.ApacheKafkaBroker) -> str`\n",
      "\n",
      "Starts a local kafka broker and zookeeper instance synchronously\n",
      "\n",
      "**Returns**:\n",
      "- Kafka broker bootstrap server address in string format: add:port\n",
      "\n",
      "### `stop` {#stop}\n",
      "\n",
      "`def stop(self: fastkafka.testing.ApacheKafkaBroker) -> None`\n",
      "\n",
      "Stops a local kafka broker and zookeeper instance synchronously\n",
      "\n",
      "**Returns**:\n",
      "- None\n",
      "\n",
      "\n",
      "## `fastkafka.testing.LocalRedpandaBroker` {#fastkafka.testing.LocalRedpandaBroker}\n",
      "\n",
      "\n",
      "LocalRedpandaBroker class, used for running unique redpanda brokers in tests to prevent topic clashing.\n",
      "\n",
      "### `__init__` {#init}\n",
      "\n",
      "`def __init__(self, topics: Iterable[str] = [], retries: int = 3, apply_nest_asyncio: bool = False, listener_port: int = 9092, tag: str = 'v23.1.2', seastar_core: int = 1, memory: str = '1G', mode: str = 'dev-container', default_log_level: str = 'debug', **kwargs: Dict[str, Any]) -> None`\n",
      "\n",
      "Initialises the LocalRedpandaBroker object\n",
      "\n",
      "**Parameters**:\n",
      "- `listener_port`: Port on which the clients (producers and consumers) can connect\n",
      "- `tag`: Tag of Redpanda image to use to start container\n",
      "- `seastar_core`: Core(s) to use byt Seastar (the framework Redpanda uses under the hood)\n",
      "- `memory`: The amount of memory to make available to Redpanda\n",
      "- `mode`: Mode to use to load configuration properties in container\n",
      "- `default_log_level`: Log levels to use for Redpanda\n",
      "\n",
      "### `get_service_config_string` {#get_service_config_string}\n",
      "\n",
      "`def get_service_config_string(self, service: str, data_dir: pathlib.Path) -> str`\n",
      "\n",
      "Generates a configuration for a service\n",
      "\n",
      "**Parameters**:\n",
      "- `data_dir`: Path to the directory where the zookeepeer instance will save data\n",
      "- `service`: \"redpanda\", defines which service to get config string for\n",
      "\n",
      "### `start` {#start}\n",
      "\n",
      "`def start(self: fastkafka.testing.LocalRedpandaBroker) -> str`\n",
      "\n",
      "Starts a local redpanda broker instance synchronously\n",
      "\n",
      "**Returns**:\n",
      "- Redpanda broker bootstrap server address in string format: add:port\n",
      "\n",
      "### `stop` {#stop}\n",
      "\n",
      "`def stop(self: fastkafka.testing.LocalRedpandaBroker) -> None`\n",
      "\n",
      "Stops a local redpanda broker instance synchronously\n",
      "\n",
      "**Returns**:\n",
      "- None\n",
      "\n",
      "\n",
      "## `fastkafka.testing.Tester` {#fastkafka.testing.Tester}\n",
      "\n",
      "### `__init__` {#init}\n",
      "\n",
      "`def __init__(self, app: Union[fastkafka.FastKafka, List[fastkafka.FastKafka]], broker: Optional[fastkafka.testing.ApacheKafkaBroker, fastkafka.testing.LocalRedpandaBroker, fastkafka._testing.in_memory_broker.InMemoryBroker] = None, topics: Iterable[str] = [], retries: int = 3, apply_nest_asyncio: bool = False, zookeeper_port: int = 2181, listener_port: int = 9092) -> None`\n",
      "\n",
      "Mirror-like object for testing a FastFafka application\n",
      "\n",
      "Can be used as context manager\n",
      "\n",
      "**Parameters**:\n",
      "- `data_dir`: Path to the directory where the zookeepeer instance will save data\n",
      "- `zookeeper_port`: Port for clients (Kafka brokes) to connect\n",
      "- `listener_port`: Port on which the clients (producers and consumers) can connect\n",
      "\n",
      "### `benchmark` {#benchmark}\n",
      "\n",
      "`def benchmark(self: fastkafka.FastKafka, interval: Union[int, datetime.timedelta] = 1, sliding_window_size: Optional[int] = None) -> typing.Callable[[typing.Callable[[~I], typing.Optional[~O]]], typing.Callable[[~I], typing.Optional[~O]]]`\n",
      "\n",
      "Decorator to benchmark produces/consumes functions\n",
      "\n",
      "**Parameters**:\n",
      "- `interval`: Period to use to calculate throughput. If value is of type int,\n",
      "then it will be used as seconds. If value is of type timedelta,\n",
      "then it will be used as it is. default: 1 - one second\n",
      "- `sliding_window_size`: The size of the sliding window to use to calculate\n",
      "average throughput. default: None - By default average throughput is\n",
      "not calculated\n",
      "\n",
      "### `consumes` {#consumes}\n",
      "\n",
      "`def consumes(self: fastkafka.FastKafka, topic: Optional[str] = None, decoder: Union[str, Callable[[bytes, pydantic.main.ModelMetaclass], Any]] = 'json', prefix: str = 'on_', loop=None, bootstrap_servers='localhost', client_id='aiokafka-0.8.0', group_id=None, key_deserializer=None, value_deserializer=None, fetch_max_wait_ms=500, fetch_max_bytes=52428800, fetch_min_bytes=1, max_partition_fetch_bytes=1048576, request_timeout_ms=40000, retry_backoff_ms=100, auto_offset_reset='latest', enable_auto_commit=True, auto_commit_interval_ms=5000, check_crcs=True, metadata_max_age_ms=300000, partition_assignment_strategy=(<class 'kafka.coordinator.assignors.roundrobin.RoundRobinPartitionAssignor'>,), max_poll_interval_ms=300000, rebalance_timeout_ms=None, session_timeout_ms=10000, heartbeat_interval_ms=3000, consumer_timeout_ms=200, max_poll_records=None, ssl_context=None, security_protocol='PLAINTEXT', api_version='auto', exclude_internal_topics=True, connections_max_idle_ms=540000, isolation_level='read_uncommitted', sasl_mechanism='PLAIN', sasl_plain_password=None, sasl_plain_username=None, sasl_kerberos_service_name='kafka', sasl_kerberos_domain_name=None, sasl_oauth_token_provider=None) -> typing.Callable[[typing.Callable[[pydantic.main.BaseModel], typing.Optional[typing.Awaitable[NoneType]]]], typing.Callable[[pydantic.main.BaseModel], typing.Optional[typing.Awaitable[NoneType]]]]`\n",
      "\n",
      "Decorator registering the callback called when a message is received in a topic.\n",
      "\n",
      "This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.\n",
      "\n",
      "**Parameters**:\n",
      "- `topic`: Kafka topic that the consumer will subscribe to and execute the\n",
      "decorated function when it receives a message from the topic,\n",
      "default: None. If the topic is not specified, topic name will be\n",
      "inferred from the decorated function name by stripping the defined prefix\n",
      "- `decoder`: Decoder to use to decode messages consumed from the topic,\n",
      "default: json - By default, it uses json decoder to decode\n",
      "bytes to json string and then it creates instance of pydantic\n",
      "BaseModel. It also accepts custom decoder function.\n",
      "- `prefix`: Prefix stripped from the decorated function to define a topic name\n",
      "if the topic argument is not passed, default: \"on_\". If the decorated\n",
      "function name is not prefixed with the defined prefix and topic argument\n",
      "is not passed, then this method will throw ValueError\n",
      "- `*topics`: optional list of topics to subscribe to. If not set,\n",
      "call :meth:`.subscribe` or :meth:`.assign` before consuming records.\n",
      "Passing topics directly is same as calling :meth:`.subscribe` API.\n",
      "- `bootstrap_servers`: a ``host[:port]`` string (or list of\n",
      "``host[:port]`` strings) that the consumer should contact to bootstrap\n",
      "initial cluster metadata.\n",
      "\n",
      "This does not have to be the full node list.\n",
      "It just needs to have at least one broker that will respond to a\n",
      "Metadata API Request. Default port is 9092. If no servers are\n",
      "specified, will default to ``localhost:9092``.\n",
      "- `client_id`: a name for this client. This string is passed in\n",
      "each request to servers and can be used to identify specific\n",
      "server-side log entries that correspond to this client. Also\n",
      "submitted to :class:`~.consumer.group_coordinator.GroupCoordinator`\n",
      "for logging with respect to consumer group administration. Default:\n",
      "``aiokafka-{version}``\n",
      "- `group_id`: name of the consumer group to join for dynamic\n",
      "partition assignment (if enabled), and to use for fetching and\n",
      "committing offsets. If None, auto-partition assignment (via\n",
      "group coordinator) and offset commits are disabled.\n",
      "Default: None\n",
      "- `key_deserializer`: Any callable that takes a\n",
      "raw message key and returns a deserialized key.\n",
      "- `value_deserializer`: Any callable that takes a\n",
      "raw message value and returns a deserialized value.\n",
      "- `fetch_min_bytes`: Minimum amount of data the server should\n",
      "return for a fetch request, otherwise wait up to\n",
      "`fetch_max_wait_ms` for more data to accumulate. Default: 1.\n",
      "- `fetch_max_bytes`: The maximum amount of data the server should\n",
      "return for a fetch request. This is not an absolute maximum, if\n",
      "the first message in the first non-empty partition of the fetch\n",
      "is larger than this value, the message will still be returned\n",
      "to ensure that the consumer can make progress. NOTE: consumer\n",
      "performs fetches to multiple brokers in parallel so memory\n",
      "usage will depend on the number of brokers containing\n",
      "partitions for the topic.\n",
      "Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).\n",
      "- `fetch_max_wait_ms`: The maximum amount of time in milliseconds\n",
      "the server will block before answering the fetch request if\n",
      "there isn't sufficient data to immediately satisfy the\n",
      "requirement given by fetch_min_bytes. Default: 500.\n",
      "- `max_partition_fetch_bytes`: The maximum amount of data\n",
      "per-partition the server will return. The maximum total memory\n",
      "used for a request ``= #partitions * max_partition_fetch_bytes``.\n",
      "This size must be at least as large as the maximum message size\n",
      "the server allows or else it is possible for the producer to\n",
      "send messages larger than the consumer can fetch. If that\n",
      "happens, the consumer can get stuck trying to fetch a large\n",
      "message on a certain partition. Default: 1048576.\n",
      "- `max_poll_records`: The maximum number of records returned in a\n",
      "single call to :meth:`.getmany`. Defaults ``None``, no limit.\n",
      "- `request_timeout_ms`: Client request timeout in milliseconds.\n",
      "Default: 40000.\n",
      "- `retry_backoff_ms`: Milliseconds to backoff when retrying on\n",
      "errors. Default: 100.\n",
      "- `auto_offset_reset`: A policy for resetting offsets on\n",
      ":exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest\n",
      "available message, ``latest`` will move to the most recent, and\n",
      "``none`` will raise an exception so you can handle this case.\n",
      "Default: ``latest``.\n",
      "- `enable_auto_commit`: If true the consumer's offset will be\n",
      "periodically committed in the background. Default: True.\n",
      "- `auto_commit_interval_ms`: milliseconds between automatic\n",
      "offset commits, if enable_auto_commit is True. Default: 5000.\n",
      "- `check_crcs`: Automatically check the CRC32 of the records\n",
      "consumed. This ensures no on-the-wire or on-disk corruption to\n",
      "the messages occurred. This check adds some overhead, so it may\n",
      "be disabled in cases seeking extreme performance. Default: True\n",
      "- `metadata_max_age_ms`: The period of time in milliseconds after\n",
      "which we force a refresh of metadata even if we haven't seen any\n",
      "partition leadership changes to proactively discover any new\n",
      "brokers or partitions. Default: 300000\n",
      "- `partition_assignment_strategy`: List of objects to use to\n",
      "distribute partition ownership amongst consumer instances when\n",
      "group management is used. This preference is implicit in the order\n",
      "of the strategies in the list. When assignment strategy changes:\n",
      "to support a change to the assignment strategy, new versions must\n",
      "enable support both for the old assignment strategy and the new\n",
      "one. The coordinator will choose the old assignment strategy until\n",
      "all members have been updated. Then it will choose the new\n",
      "strategy. Default: [:class:`.RoundRobinPartitionAssignor`]\n",
      "- `max_poll_interval_ms`: Maximum allowed time between calls to\n",
      "consume messages (e.g., :meth:`.getmany`). If this interval\n",
      "is exceeded the consumer is considered failed and the group will\n",
      "rebalance in order to reassign the partitions to another consumer\n",
      "group member. If API methods block waiting for messages, that time\n",
      "does not count against this timeout. See `KIP-62`_ for more\n",
      "information. Default 300000\n",
      "- `rebalance_timeout_ms`: The maximum time server will wait for this\n",
      "consumer to rejoin the group in a case of rebalance. In Java client\n",
      "this behaviour is bound to `max.poll.interval.ms` configuration,\n",
      "but as ``aiokafka`` will rejoin the group in the background, we\n",
      "decouple this setting to allow finer tuning by users that use\n",
      ":class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults\n",
      "to ``session_timeout_ms``\n",
      "- `session_timeout_ms`: Client group session and failure detection\n",
      "timeout. The consumer sends periodic heartbeats\n",
      "(`heartbeat.interval.ms`) to indicate its liveness to the broker.\n",
      "If no hearts are received by the broker for a group member within\n",
      "the session timeout, the broker will remove the consumer from the\n",
      "group and trigger a rebalance. The allowed range is configured with\n",
      "the **broker** configuration properties\n",
      "`group.min.session.timeout.ms` and `group.max.session.timeout.ms`.\n",
      "Default: 10000\n",
      "- `heartbeat_interval_ms`: The expected time in milliseconds\n",
      "between heartbeats to the consumer coordinator when using\n",
      "Kafka's group management feature. Heartbeats are used to ensure\n",
      "that the consumer's session stays active and to facilitate\n",
      "rebalancing when new consumers join or leave the group. The\n",
      "value must be set lower than `session_timeout_ms`, but typically\n",
      "should be set no higher than 1/3 of that value. It can be\n",
      "adjusted even lower to control the expected time for normal\n",
      "rebalances. Default: 3000\n",
      "- `consumer_timeout_ms`: maximum wait timeout for background fetching\n",
      "routine. Mostly defines how fast the system will see rebalance and\n",
      "request new data for new partitions. Default: 200\n",
      "- `api_version`: specify which kafka API version to use.\n",
      ":class:`AIOKafkaConsumer` supports Kafka API versions >=0.9 only.\n",
      "If set to ``auto``, will attempt to infer the broker version by\n",
      "probing various APIs. Default: ``auto``\n",
      "- `security_protocol`: Protocol used to communicate with brokers.\n",
      "Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
      "- `ssl_context`: pre-configured :class:`~ssl.SSLContext`\n",
      "for wrapping socket connections. Directly passed into asyncio's\n",
      ":meth:`~asyncio.loop.create_connection`. For more information see\n",
      ":ref:`ssl_auth`. Default: None.\n",
      "- `exclude_internal_topics`: Whether records from internal topics\n",
      "(such as offsets) should be exposed to the consumer. If set to True\n",
      "the only way to receive records from an internal topic is\n",
      "subscribing to it. Requires 0.10+ Default: True\n",
      "- `connections_max_idle_ms`: Close idle connections after the number\n",
      "of milliseconds specified by this config. Specifying `None` will\n",
      "disable idle checks. Default: 540000 (9 minutes).\n",
      "- `isolation_level`: Controls how to read messages written\n",
      "transactionally.\n",
      "\n",
      "If set to ``read_committed``, :meth:`.getmany` will only return\n",
      "transactional messages which have been committed.\n",
      "If set to ``read_uncommitted`` (the default), :meth:`.getmany` will\n",
      "return all messages, even transactional messages which have been\n",
      "aborted.\n",
      "\n",
      "Non-transactional messages will be returned unconditionally in\n",
      "either mode.\n",
      "\n",
      "Messages will always be returned in offset order. Hence, in\n",
      "`read_committed` mode, :meth:`.getmany` will only return\n",
      "messages up to the last stable offset (LSO), which is the one less\n",
      "than the offset of the first open transaction. In particular any\n",
      "messages appearing after messages belonging to ongoing transactions\n",
      "will be withheld until the relevant transaction has been completed.\n",
      "As a result, `read_committed` consumers will not be able to read up\n",
      "to the high watermark when there are in flight transactions.\n",
      "Further, when in `read_committed` the seek_to_end method will\n",
      "return the LSO. See method docs below. Default: ``read_uncommitted``\n",
      "- `sasl_mechanism`: Authentication mechanism when security_protocol\n",
      "is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values are:\n",
      "``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
      "``OAUTHBEARER``.\n",
      "Default: ``PLAIN``\n",
      "- `sasl_plain_username`: username for SASL ``PLAIN`` authentication.\n",
      "Default: None\n",
      "- `sasl_plain_password`: password for SASL ``PLAIN`` authentication.\n",
      "Default: None\n",
      "- `sasl_oauth_token_provider`: OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).\n",
      "Default: None\n",
      "\n",
      "**Returns**:\n",
      "- : A function returning the same function\n",
      "\n",
      "### `create_mocks` {#create_mocks}\n",
      "\n",
      "`def create_mocks(self: fastkafka.FastKafka) -> None`\n",
      "\n",
      "Creates self.mocks as a named tuple mapping a new function obtained by calling the original functions and a mock\n",
      "\n",
      "### `produces` {#produces}\n",
      "\n",
      "`def produces(self: fastkafka.FastKafka, topic: Optional[str] = None, encoder: Union[str, Callable[[pydantic.main.BaseModel], bytes]] = 'json', prefix: str = 'to_', loop=None, bootstrap_servers='localhost', client_id=None, metadata_max_age_ms=300000, request_timeout_ms=40000, api_version='auto', acks=<object object>, key_serializer=None, value_serializer=None, compression_type=None, max_batch_size=16384, partitioner=<kafka.partitioner.default.DefaultPartitioner object>, max_request_size=1048576, linger_ms=0, send_backoff_ms=100, retry_backoff_ms=100, security_protocol='PLAINTEXT', ssl_context=None, connections_max_idle_ms=540000, enable_idempotence=False, transactional_id=None, transaction_timeout_ms=60000, sasl_mechanism='PLAIN', sasl_plain_password=None, sasl_plain_username=None, sasl_kerberos_service_name='kafka', sasl_kerberos_domain_name=None, sasl_oauth_token_provider=None) -> typing.Callable[[typing.Union[typing.Callable[..., typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel]]], typing.Callable[..., typing.Awaitable[typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel]]]]]], typing.Union[typing.Callable[..., typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel]]], typing.Callable[..., typing.Awaitable[typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel]]]]]]`\n",
      "\n",
      "Decorator registering the callback called when delivery report for a produced message is received\n",
      "\n",
      "This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.\n",
      "\n",
      "**Parameters**:\n",
      "- `topic`: Kafka topic that the producer will send returned values from\n",
      "the decorated function to, default: None- If the topic is not\n",
      "specified, topic name will be inferred from the decorated function\n",
      "name by stripping the defined prefix.\n",
      "- `encoder`: Encoder to use to encode messages before sending it to topic,\n",
      "default: json - By default, it uses json encoder to convert\n",
      "pydantic basemodel to json string and then encodes the string to bytes\n",
      "using 'utf-8' encoding. It also accepts custom encoder function.\n",
      "- `prefix`: Prefix stripped from the decorated function to define a topic\n",
      "name if the topic argument is not passed, default: \"to_\". If the\n",
      "decorated function name is not prefixed with the defined prefix\n",
      "and topic argument is not passed, then this method will throw ValueError\n",
      "- `bootstrap_servers`: a ``host[:port]`` string or list of\n",
      "``host[:port]`` strings that the producer should contact to\n",
      "bootstrap initial cluster metadata. This does not have to be the\n",
      "full node list.  It just needs to have at least one broker that will\n",
      "respond to a Metadata API Request. Default port is 9092. If no\n",
      "servers are specified, will default to ``localhost:9092``.\n",
      "- `client_id`: a name for this client. This string is passed in\n",
      "each request to servers and can be used to identify specific\n",
      "server-side log entries that correspond to this client.\n",
      "Default: ``aiokafka-producer-#`` (appended with a unique number\n",
      "per instance)\n",
      "- `key_serializer`: used to convert user-supplied keys to bytes\n",
      "If not :data:`None`, called as ``f(key),`` should return\n",
      ":class:`bytes`.\n",
      "Default: :data:`None`.\n",
      "- `value_serializer`: used to convert user-supplied message\n",
      "values to :class:`bytes`. If not :data:`None`, called as\n",
      "``f(value)``, should return :class:`bytes`.\n",
      "Default: :data:`None`.\n",
      "- `acks`: one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
      "the producer requires the leader to have received before considering a\n",
      "request complete. This controls the durability of records that are\n",
      "sent. The following settings are common:\n",
      "\n",
      "* ``0``: Producer will not wait for any acknowledgment from the server\n",
      "  at all. The message will immediately be added to the socket\n",
      "  buffer and considered sent. No guarantee can be made that the\n",
      "  server has received the record in this case, and the retries\n",
      "  configuration will not take effect (as the client won't\n",
      "  generally know of any failures). The offset given back for each\n",
      "  record will always be set to -1.\n",
      "* ``1``: The broker leader will write the record to its local log but\n",
      "  will respond without awaiting full acknowledgement from all\n",
      "  followers. In this case should the leader fail immediately\n",
      "  after acknowledging the record but before the followers have\n",
      "  replicated it then the record will be lost.\n",
      "* ``all``: The broker leader will wait for the full set of in-sync\n",
      "  replicas to acknowledge the record. This guarantees that the\n",
      "  record will not be lost as long as at least one in-sync replica\n",
      "  remains alive. This is the strongest available guarantee.\n",
      "\n",
      "If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
      ":data:`True` defaults to ``acks=all``\n",
      "- `compression_type`: The compression type for all data generated by\n",
      "the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
      "or :data:`None`.\n",
      "Compression is of full batches of data, so the efficacy of batching\n",
      "will also impact the compression ratio (more batching means better\n",
      "compression). Default: :data:`None`.\n",
      "- `max_batch_size`: Maximum size of buffered data per partition.\n",
      "After this amount :meth:`send` coroutine will block until batch is\n",
      "drained.\n",
      "Default: 16384\n",
      "- `linger_ms`: The producer groups together any records that arrive\n",
      "in between request transmissions into a single batched request.\n",
      "Normally this occurs only under load when records arrive faster\n",
      "than they can be sent out. However in some circumstances the client\n",
      "may want to reduce the number of requests even under moderate load.\n",
      "This setting accomplishes this by adding a small amount of\n",
      "artificial delay; that is, if first request is processed faster,\n",
      "than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
      "Default: 0 (i.e. no delay).\n",
      "- `partitioner`: Callable used to determine which partition\n",
      "each message is assigned to. Called (after key serialization):\n",
      "``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
      "The default partitioner implementation hashes each non-None key\n",
      "using the same murmur2 algorithm as the Java client so that\n",
      "messages with the same key are assigned to the same partition.\n",
      "When a key is :data:`None`, the message is delivered to a random partition\n",
      "(filtered to partitions with available leaders only, if possible).\n",
      "- `max_request_size`: The maximum size of a request. This is also\n",
      "effectively a cap on the maximum record size. Note that the server\n",
      "has its own cap on record size which may be different from this.\n",
      "This setting will limit the number of record batches the producer\n",
      "will send in a single request to avoid sending huge requests.\n",
      "Default: 1048576.\n",
      "- `metadata_max_age_ms`: The period of time in milliseconds after\n",
      "which we force a refresh of metadata even if we haven't seen any\n",
      "partition leadership changes to proactively discover any new\n",
      "brokers or partitions. Default: 300000\n",
      "- `request_timeout_ms`: Produce request timeout in milliseconds.\n",
      "As it's sent as part of\n",
      ":class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
      "call), maximum waiting time can be up to ``2 *\n",
      "request_timeout_ms``.\n",
      "Default: 40000.\n",
      "- `retry_backoff_ms`: Milliseconds to backoff when retrying on\n",
      "errors. Default: 100.\n",
      "- `api_version`: specify which kafka API version to use.\n",
      "If set to ``auto``, will attempt to infer the broker version by\n",
      "probing various APIs. Default: ``auto``\n",
      "- `security_protocol`: Protocol used to communicate with brokers.\n",
      "Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
      "Default: ``PLAINTEXT``.\n",
      "- `ssl_context`: pre-configured :class:`~ssl.SSLContext`\n",
      "for wrapping socket connections. Directly passed into asyncio's\n",
      ":meth:`~asyncio.loop.create_connection`. For more\n",
      "information see :ref:`ssl_auth`.\n",
      "Default: :data:`None`\n",
      "- `connections_max_idle_ms`: Close idle connections after the number\n",
      "of milliseconds specified by this config. Specifying :data:`None` will\n",
      "disable idle checks. Default: 540000 (9 minutes).\n",
      "- `enable_idempotence`: When set to :data:`True`, the producer will\n",
      "ensure that exactly one copy of each message is written in the\n",
      "stream. If :data:`False`, producer retries due to broker failures,\n",
      "etc., may write duplicates of the retried message in the stream.\n",
      "Note that enabling idempotence acks to set to ``all``. If it is not\n",
      "explicitly set by the user it will be chosen. If incompatible\n",
      "values are set, a :exc:`ValueError` will be thrown.\n",
      "New in version 0.5.0.\n",
      "- `sasl_mechanism`: Authentication mechanism when security_protocol\n",
      "is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
      "are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
      "``OAUTHBEARER``.\n",
      "Default: ``PLAIN``\n",
      "- `sasl_plain_username`: username for SASL ``PLAIN`` authentication.\n",
      "Default: :data:`None`\n",
      "- `sasl_plain_password`: password for SASL ``PLAIN`` authentication.\n",
      "Default: :data:`None`\n",
      "- `sasl_oauth_token_provider (`: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
      "OAuthBearer token provider instance. (See\n",
      ":mod:`kafka.oauth.abstract`).\n",
      "Default: :data:`None`\n",
      "\n",
      "**Returns**:\n",
      "- : A function returning the same function\n",
      "\n",
      "**Exceptions**:\n",
      "- `ValueError`: when needed\n",
      "\n",
      "### `run_in_background` {#run_in_background}\n",
      "\n",
      "`def run_in_background(self: fastkafka.FastKafka) -> typing.Callable[[typing.Callable[..., typing.Coroutine[typing.Any, typing.Any, typing.Any]]], typing.Callable[..., typing.Coroutine[typing.Any, typing.Any, typing.Any]]]`\n",
      "\n",
      "Decorator to schedule a task to be run in the background.\n",
      "\n",
      "This decorator is used to schedule a task to be run in the background when the app's `_on_startup` event is triggered.\n",
      "\n",
      "**Returns**:\n",
      "- A decorator function that takes a background task as an input and stores it to be run in the backround.\n",
      "\n",
      "### `using_local_kafka` {#using_local_kafka}\n",
      "\n",
      "`def using_local_kafka(self, topics: Iterable[str] = [], retries: int = 3, apply_nest_asyncio: bool = False, zookeeper_port: int = 2181, listener_port: int = 9092) -> Tester`\n",
      "\n",
      "Starts local Kafka broker used by the Tester instance\n",
      "\n",
      "**Parameters**:\n",
      "- `data_dir`: Path to the directory where the zookeepeer instance will save data\n",
      "- `zookeeper_port`: Port for clients (Kafka brokes) to connect\n",
      "- `listener_port`: Port on which the clients (producers and consumers) can connect\n",
      "\n",
      "**Returns**:\n",
      "- An instance of tester with Kafka as broker\n",
      "\n",
      "### `using_local_redpanda` {#using_local_redpanda}\n",
      "\n",
      "`def using_local_redpanda(self, topics: Iterable[str] = [], retries: int = 3, apply_nest_asyncio: bool = False, listener_port: int = 9092, tag: str = 'v23.1.2', seastar_core: int = 1, memory: str = '1G', mode: str = 'dev-container', default_log_level: str = 'debug') -> Tester`\n",
      "\n",
      "Starts local Redpanda broker used by the Tester instance\n",
      "\n",
      "**Parameters**:\n",
      "- `listener_port`: Port on which the clients (producers and consumers) can connect\n",
      "- `tag`: Tag of Redpanda image to use to start container\n",
      "- `seastar_core`: Core(s) to use byt Seastar (the framework Redpanda uses under the hood)\n",
      "- `memory`: The amount of memory to make available to Redpanda\n",
      "- `mode`: Mode to use to load configuration properties in container\n",
      "- `default_log_level`: Log levels to use for Redpanda\n",
      "\n",
      "**Returns**:\n",
      "- An instance of tester with Redpanda as broker\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    module_name = \"fastkafka\"\n",
    "\n",
    "    docs_path = Path(d) / \"docusaurus\" / \"docs\"\n",
    "    docs_path.mkdir(parents=True)\n",
    "\n",
    "    api_path = docs_path / \"api\"\n",
    "    api_path.mkdir(parents=True)\n",
    "\n",
    "    members_with_submodules = _get_submodules(module_name)\n",
    "    symbols = _load_submodules(module_name, members_with_submodules)\n",
    "    for symbol in symbols:\n",
    "        target_file_path = (\n",
    "            \"/\".join(f\"{symbol.__module__}.{symbol.__name__}\".split(\".\")) + \".md\"\n",
    "        )\n",
    "        (api_path / \"/\".join(f\"{symbol.__module__}\".split(\".\"))).mkdir(\n",
    "            parents=True, exist_ok=True\n",
    "        )\n",
    "\n",
    "        with open((api_path / target_file_path), \"w\") as f:\n",
    "            f.write(f\"Initial content in '{target_file_path}'\")\n",
    "\n",
    "        with open((api_path / target_file_path), \"r\") as f:\n",
    "            contents = f.read()\n",
    "            print(contents)\n",
    "            assert f\"Initial content in '{target_file_path}'\" == contents, contents\n",
    "\n",
    "    generate_markdown_docs(module_name, str(docs_path))\n",
    "\n",
    "    print(\"*\" * 100)\n",
    "    for symbol in symbols:\n",
    "        target_file_path = (\n",
    "            \"/\".join(f\"{symbol.__module__}.{symbol.__name__}\".split(\".\")) + \".md\"\n",
    "        )\n",
    "        (api_path / \"/\".join(f\"{symbol.__module__}\".split(\".\"))).mkdir(\n",
    "            parents=True, exist_ok=True\n",
    "        )\n",
    "\n",
    "        with open((api_path / target_file_path), \"r\") as f:\n",
    "            contents = f.read()\n",
    "            print(contents)\n",
    "            assert f\"Initial content in '{target_file_path}'\" != contents, contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ae9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "SidebarT = Union[str, List[\"SidebarT\"], Dict[str, \"SidebarT\"]]\n",
    "\n",
    "def parse_contents(contents: List[SidebarT]) ->  List[SidebarT]:\n",
    "    new: List[SidebarT] = []\n",
    "    for content in contents:\n",
    "        if isinstance(content, str):\n",
    "            new.append(content.split(\".\")[0])\n",
    "        if isinstance(content, dict):\n",
    "            new.append(remove_section_contents(content)) # type: ignore\n",
    "    return new\n",
    "            \n",
    "def remove_section_contents(section: Dict[str, \"SidebarT\"]) -> Dict[str, List[SidebarT]]:\n",
    "    new_section: Dict[str, List[\"SidebarT\"]] = {}\n",
    "    new_section[section[\"section\"]] = parse_contents(section[\"contents\"]) # type: ignore\n",
    "    return new_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "sidebar = [\n",
    "    \"index.ipynb\",\n",
    "    {\n",
    "        \"section\": \"Guides\",\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"section\": \"Writing services\",\n",
    "                \"contents\": [\n",
    "                    \"guides/Guide_11_Consumes_Basics.ipynb\",\n",
    "                    \"guides/Guide_21_Produces_Basics.ipynb\",\n",
    "                    \"guides/Guide_22_Partition_Keys.ipynb\",\n",
    "                    \"guides/Guide_05_Lifespan_Handler.ipynb\",\n",
    "                    \"guides/Guide_07_Encoding_and_Decoding_Messages_with_FastKafka.ipynb\",\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "expected = [\n",
    "    \"index\",\n",
    "    {\n",
    "        \"Guides\": [\n",
    "            {\n",
    "                \"Writing services\": [\n",
    "                    \"guides/Guide_11_Consumes_Basics\",\n",
    "                    \"guides/Guide_21_Produces_Basics\",\n",
    "                    \"guides/Guide_22_Partition_Keys\",\n",
    "                    \"guides/Guide_05_Lifespan_Handler\",\n",
    "                    \"guides/Guide_07_Encoding_and_Decoding_Messages_with_FastKafka\",\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "assert parse_contents(sidebar) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f3728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def generate_sidebar(\n",
    "    nbs_sidebar: str = \"/work/fastkafka/nbs/sidebar.yml\",\n",
    "    target: str = \"./docusaurus/sidebars.js\",\n",
    ") -> None:\n",
    "    with open(nbs_sidebar, \"r\") as stream, open(target, \"w\") as target_stream:\n",
    "        try:\n",
    "            sidebar = yaml.safe_load(stream)\n",
    "            parsed_sidebar = parse_contents(sidebar[\"website\"][\"sidebar\"][\"contents\"])\n",
    "            sidebar_str = str(parsed_sidebar)[1:-1] + \",\"\n",
    "            target_stream.write(\n",
    "                \"\"\"module.exports = {\n",
    "tutorialSidebar: [\n",
    "    \"\"\"\n",
    "                + sidebar_str\n",
    "                + \"\"\"\n",
    "//         [require(\"./docs/reference/sidebar.json\")],\n",
    "    {\n",
    "        \"items\": [\n",
    "            \"api/fastkafka/FastKafka\",\n",
    "            \"api/fastkafka/KafkaEvent\",\n",
    "            {\n",
    "              \"items\": [\n",
    "                \"api/fastkafka/testing/ApacheKafkaBroker\",\n",
    "                \"api/fastkafka/testing/LocalRedpandaBroker\",\n",
    "                \"api/fastkafka/testing/Tester\"\n",
    "              ],\n",
    "              \"label\": \"testing\",\n",
    "              \"type\": \"category\"\n",
    "            },\n",
    "        ],\n",
    "        \"label\": \"API\",\n",
    "        \"type\": \"category\"\n",
    "    },\n",
    "    {\n",
    "        \"CLI\": ['cli/fastkafka', 'cli/run_fastkafka_server_process'],\n",
    "    },\n",
    "\n",
    "    \"CHANGELOG\",\n",
    "],\n",
    "};\"\"\"\n",
    "            )\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9592d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sidebar = \"\"\"website:\n",
    "  sidebar:\n",
    "    contents:\n",
    "      - index.ipynb\n",
    "      \n",
    "      - section: Guides\n",
    "        contents:\n",
    "        - section: Writing services\n",
    "          contents:\n",
    "          - guides/Guide_11_Consumes_Basics.ipynb\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as directory:\n",
    "    with open(directory + \"/sidebar.yml\", \"w\") as stream:\n",
    "        stream.write(sidebar)\n",
    "\n",
    "    generate_sidebar(nbs_sidebar=directory + \"/sidebar.yml\", target=directory + \"/test.js\")\n",
    "\n",
    "    with open(directory + \"/test.js\") as stream:\n",
    "        stream = stream.read()\n",
    "\n",
    "assert (\n",
    "    stream\n",
    "    == \"\"\"module.exports = {\n",
    "tutorialSidebar: [\n",
    "    'index', {'Guides': [{'Writing services': ['guides/Guide_11_Consumes_Basics']}]},\n",
    "//         [require(\"./docs/reference/sidebar.json\")],\n",
    "    {\n",
    "        \"items\": [\n",
    "            \"api/fastkafka/FastKafka\",\n",
    "            \"api/fastkafka/KafkaEvent\",\n",
    "            {\n",
    "              \"items\": [\n",
    "                \"api/fastkafka/testing/ApacheKafkaBroker\",\n",
    "                \"api/fastkafka/testing/LocalRedpandaBroker\",\n",
    "                \"api/fastkafka/testing/Tester\"\n",
    "              ],\n",
    "              \"label\": \"testing\",\n",
    "              \"type\": \"category\"\n",
    "            },\n",
    "        ],\n",
    "        \"label\": \"API\",\n",
    "        \"type\": \"category\"\n",
    "    },\n",
    "    {\n",
    "        \"CLI\": ['cli/fastkafka', 'cli/run_fastkafka_server_process'],\n",
    "    },\n",
    "\n",
    "    \"CHANGELOG\",\n",
    "],\n",
    "};\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
