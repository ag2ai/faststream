{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastKafka\n",
    "\n",
    "This notebook will demonstrate the capabilities and developed functionalities in FastKafka project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/airtai/fastkafka/blob/64-colab-based-tutorial/nbs/guides/00_FastKafka_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing fastkafka library\n",
    "\n",
    "To install fastkafka, run: `pip install fastkafka` in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: fastkafka==0.1.0rc1 in /usr/local/lib/python3.8/dist-packages (0.1.0rc1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (0.15)\n",
      "Requirement already satisfied: ipywidgets>=8.0 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (8.0.4)\n",
      "Requirement already satisfied: fastcore>=1.5.27 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (1.5.28)\n",
      "Requirement already satisfied: fastapi>=0.85.1 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (0.90.1)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (2.28.2)\n",
      "Requirement already satisfied: aiofiles>=22.1.0 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (23.1.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.5 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (0.0.5)\n",
      "Requirement already satisfied: ilock>=1.0.3 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (1.0.3)\n",
      "Requirement already satisfied: asyncer>=0.0.2 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (0.0.2)\n",
      "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (4.64.1)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (0.23.3)\n",
      "Requirement already satisfied: aiokafka>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (0.8.0)\n",
      "Requirement already satisfied: confluent-kafka>=1.9.2 in /usr/local/lib/python3.8/dist-packages (from fastkafka==0.1.0rc1) (2.0.2)\n",
      "Requirement already satisfied: kafka-python>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from aiokafka>=0.8.0->fastkafka==0.1.0rc1) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from aiokafka>=0.8.0->fastkafka==0.1.0rc1) (23.0)\n",
      "Requirement already satisfied: async-timeout in /usr/local/lib/python3.8/dist-packages (from aiokafka>=0.8.0->fastkafka==0.1.0rc1) (4.0.2)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from asyncer>=0.0.2->fastkafka==0.1.0rc1) (3.6.2)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.8/dist-packages (from fastapi>=0.85.1->fastkafka==0.1.0rc1) (1.10.4)\n",
      "Requirement already satisfied: starlette<0.24.0,>=0.22.0 in /usr/local/lib/python3.8/dist-packages (from fastapi>=0.85.1->fastkafka==0.1.0rc1) (0.23.1)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (from fastcore>=1.5.27->fastkafka==0.1.0rc1) (22.0.4)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx>=0.23.0->fastkafka==0.1.0rc1) (2022.12.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.8/dist-packages (from httpx>=0.23.0->fastkafka==0.1.0rc1) (1.3.0)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from httpx>=0.23.0->fastkafka==0.1.0rc1) (0.16.3)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.8/dist-packages (from httpx>=0.23.0->fastkafka==0.1.0rc1) (1.5.0)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.8/dist-packages (from ilock>=1.0.3->fastkafka==0.1.0rc1) (2.7.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=8.0->fastkafka==0.1.0rc1) (5.3.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=8.0->fastkafka==0.1.0rc1) (5.7.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=8.0->fastkafka==0.1.0rc1) (7.9.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=8.0->fastkafka==0.1.0rc1) (3.0.5)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=8.0->fastkafka==0.1.0rc1) (4.0.5)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from python-multipart>=0.0.5->fastkafka==0.1.0rc1) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->fastkafka==0.1.0rc1) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->fastkafka==0.1.0rc1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->fastkafka==0.1.0rc1) (2.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.8/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx>=0.23.0->fastkafka==0.1.0rc1) (0.14.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets>=8.0->fastkafka==0.1.0rc1) (6.0.4)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets>=8.0->fastkafka==0.1.0rc1) (6.1.12)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0->fastkafka==0.1.0rc1) (2.0.10)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0->fastkafka==0.1.0rc1) (2.6.1)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0->fastkafka==0.1.0rc1) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0->fastkafka==0.1.0rc1) (57.4.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0->fastkafka==0.1.0rc1) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0->fastkafka==0.1.0rc1) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0->fastkafka==0.1.0rc1) (0.18.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0->fastkafka==0.1.0rc1) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi>=0.85.1->fastkafka==0.1.0rc1) (4.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython>=6.1.0->ipywidgets>=8.0->fastkafka==0.1.0rc1) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0->fastkafka==0.1.0rc1) (0.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=8.0->fastkafka==0.1.0rc1) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=8.0->fastkafka==0.1.0rc1) (5.2.0)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=8.0->fastkafka==0.1.0rc1) (23.2.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython>=6.1.0->ipywidgets>=8.0->fastkafka==0.1.0rc1) (0.7.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel>=4.5.1->ipywidgets>=8.0->fastkafka==0.1.0rc1) (2.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastkafka==0.1.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.8/dist-packages (0.20.0)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.8/dist-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.8/dist-packages (1.5.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install nest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LocalKafkaBroker\n",
    "\n",
    "To be able to test and demonstrate the use of FastKafka, we have developed a python wrapper for Zookeeper and Kafka broker which is demonstrated here and used later in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkafka.testing import LocalKafkaBroker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start the LocalKafkaBroker\n",
    "\n",
    "When LocalKafkaBroker is started, it checks if there are Java and Kafka installed on the system, if not, it will install them and export them to path as it is necessary for it to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka.testing: LocalKafkaBroker.start(): entering...\n",
      "[WARNING] fastkafka.testing: LocalKafkaBroker.start(): RuntimeError raised for loop (<_UnixSelectorEventLoop running=True closed=False debug=False>): This event loop is already running\n",
      "[WARNING] fastkafka.testing: LocalKafkaBroker.start(): calling nest_asyncio.apply()\n",
      "[INFO] fastkafka.testing: Java is already installed.\n",
      "[INFO] fastkafka.testing: Kafka is already installed.\n",
      "[INFO] fastkafka.testing: But not exported to PATH, exporting...\n",
      "[INFO] fastkafka.testing: Starting zookeeper...\n",
      "[INFO] fastkafka.testing: Zookeeper started, sleeping for 5 seconds...\n",
      "[INFO] fastkafka.testing: Starting Kafka broker...\n",
      "[INFO] fastkafka.testing: Kafka broker started, sleeping for 5 seconds...\n",
      "[INFO] fastkafka.testing: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] fastkafka.testing: <class 'fastkafka.testing.LocalKafkaBroker'>.start(): returning 127.0.0.1:9092\n",
      "[INFO] fastkafka.testing: LocalKafkaBroker.start(): exited.\n",
      "127.0.0.1:9092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/fastkafka/testing.py:824: RuntimeWarning: coroutine 'LocalKafkaBroker._start' was never awaited\n",
      "  return retval\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "local_broker = LocalKafkaBroker()\n",
    "bootstrap_server = local_broker.start()\n",
    "print(bootstrap_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if there are any topics in our fresh Kafka broker. If everything is okay, there should be none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "! kafka-topics.sh --list --bootstrap-server {bootstrap_server}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now create a topic, list it, and describe it to see that our LocalKafkaBroker is really running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic quickstart-events.\n"
     ]
    }
   ],
   "source": [
    "! kafka-topics.sh --create --topic quickstart-events --bootstrap-server {bootstrap_server}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quickstart-events\n"
     ]
    }
   ],
   "source": [
    "! kafka-topics.sh --list --bootstrap-server {bootstrap_server}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: quickstart-events\tTopicId: tu1DO7aUTo2kZDwaTNpaqQ\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: flush.ms=1000,segment.bytes=1073741824,flush.messages=10000,retention.bytes=1073741824\n",
      "\tTopic: quickstart-events\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n"
     ]
    }
   ],
   "source": [
    "! kafka-topics.sh --describe --topic quickstart-events --bootstrap-server {bootstrap_server}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can stop the broker as it is no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka.testing: LocalKafkaBroker.stop(): entering...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 13018...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 13018 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 12626...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 12626 terminated.\n",
      "[INFO] fastkafka.testing: LocalKafkaBroker.stop(): exited.\n"
     ]
    }
   ],
   "source": [
    "local_broker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LocalKafkaBroker can also be used as a context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka.testing: LocalKafkaBroker.start(): entering...\n",
      "[INFO] fastkafka.testing: Java is already installed.\n",
      "[INFO] fastkafka.testing: Kafka is already installed.\n",
      "[INFO] fastkafka.testing: Starting zookeeper...\n",
      "[INFO] fastkafka.testing: Zookeeper started, sleeping for 5 seconds...\n",
      "[INFO] fastkafka.testing: Starting Kafka broker...\n",
      "[INFO] fastkafka.testing: Kafka broker started, sleeping for 5 seconds...\n",
      "[INFO] fastkafka.testing: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] fastkafka.testing: LocalKafkaBroker.start(): returning 127.0.0.1:9092\n",
      "[INFO] fastkafka.testing: LocalKafkaBroker.start(): exited.\n",
      "127.0.0.1:9092\n",
      "[INFO] fastkafka.testing: LocalKafkaBroker.stop(): entering...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 15319...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 15319 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 14937...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 14937 terminated.\n",
      "[INFO] fastkafka.testing: LocalKafkaBroker.stop(): exited.\n"
     ]
    }
   ],
   "source": [
    "with LocalKafkaBroker() as bootstrap_server:\n",
    "  print(bootstrap_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastKafka minimal demo\n",
    "\n",
    "We will first create one simple fastkafka producer and one consumer to demonstrate their capabiltiy to communicate over Kafka queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, model the data that will be communicated between producer and consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic.main import BaseModel\n",
    "\n",
    "from pydantic import BaseModel, Field, NonNegativeInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(BaseModel):\n",
    "    data: NonNegativeInt = Field(..., example=202020, description=\"Sample data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define our FastKafka consumer app and wrap it in a coroutine to be able to run it in the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from fastkafka.application import FastKafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fastkafka_consumer(msgs_count: int, topic: str, bootstrap_servers: str):\n",
    "  with tqdm(total=msgs_count, desc=f\"consuming from '{topic}'\") as consume_pbar:\n",
    "    kafka_app = FastKafka(bootstrap_servers = bootstrap_servers)\n",
    "\n",
    "    @kafka_app.consumes(topic=topic)\n",
    "    async def consume(msg: Data):\n",
    "      consume_pbar.update(1)\n",
    "\n",
    "    async with kafka_app: \n",
    "      while(True):\n",
    "        await asyncio.sleep(0.1)\n",
    "        if consume_pbar.n >= consume_pbar.total:\n",
    "          break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same preparation for the FastKafka producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fastkafka_producer(msgs: List[Data], topic: str, bootstrap_servers: str):\n",
    "  with tqdm(total=len(msgs), desc=f\"producing to '{topic}'\") as produce_pbar:\n",
    "    kafka_app = FastKafka(bootstrap_servers = bootstrap_servers)\n",
    "\n",
    "    @kafka_app.produces(topic=topic)\n",
    "    def produce(msg: Data):\n",
    "      return msg\n",
    "\n",
    "    async with kafka_app: \n",
    "      for msg in msgs:\n",
    "        produce(msg)\n",
    "        produce_pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets run the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncer\n",
    "from time import sleep\n",
    "\n",
    "from fastkafka.helpers import create_missing_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating messages: 100%|██████████| 1000/1000 [00:00<00:00, 165429.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka.testing: Java is already installed.\n",
      "[INFO] fastkafka.testing: Kafka is already installed.\n",
      "[INFO] fastkafka.testing: Starting zookeeper...\n",
      "[INFO] fastkafka.testing: Zookeeper started, sleeping for 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka.testing: Starting Kafka broker...\n",
      "[INFO] fastkafka.testing: Kafka broker started, sleeping for 5 seconds...\n",
      "[INFO] fastkafka.testing: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] fastkafka.helpers: create_missing_topics(['test_data']): new_topics = [NewTopic(topic=test_data,num_partitions=1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "consuming from 'test_data':   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'test_data'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'test_data'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'test_data': 1}. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "producing to 'test_data':   0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka.application: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "consuming from 'test_data':  97%|█████████▋| 969/1000 [00:10<00:00, 350.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "producing to 'test_data': 100%|██████████| 1000/1000 [00:05<00:00, 172.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "consuming from 'test_data': 100%|██████████| 1000/1000 [00:10<00:00, 91.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 16157...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 16157 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 15775...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 15775 terminated.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the messages for the producer to send\n",
    "msgs = [\n",
    "  Data(data=i)\n",
    "  for i in trange(1_000, desc=\"generating messages\")\n",
    "]\n",
    "\n",
    "# Start the broker\n",
    "async with LocalKafkaBroker() as bootstrap_server:\n",
    "  create_missing_topics(\n",
    "        [\"test_data\"],\n",
    "        bootstrap_servers=bootstrap_server,\n",
    "        num_partitions=1,\n",
    "    )\n",
    "  \n",
    "  async with asyncer.create_task_group() as tg:\n",
    "    # Start the consumer FastKafka app\n",
    "    tg.soonify(fastkafka_consumer)(\n",
    "      msgs_count=len(msgs), topic=\"test_data\", bootstrap_servers=bootstrap_server\n",
    "    )\n",
    "\n",
    "    # Give the consumer some time to connect to Kafka topic before we start sending\n",
    "    await asyncio.sleep(5)\n",
    "\n",
    "    # Start the producer FastKafka app  \n",
    "    tg.soonify(fastkafka_producer)(\n",
    "      msgs=msgs, topic=\"test_data\", bootstrap_servers=bootstrap_server\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "We have created a simple FastKafka consumer that updates the progress when a message is received, a producer that sends the messages to the Kafka queue.\n",
    "\n",
    "In the previous cell we have started the LocalKafkaBroker instance and connected our producer and consumer to it. \n",
    "Finall, after the producer started producing the messages, we have consumed them with our consumer and when all of the messages have been consumed the consumer and producer stopped and our kafka broker has exited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastKafka model predictions demo\n",
    "\n",
    "Now we will create a more fleshed out application containing a Model that will ingest data samples from one Kafka topic (input_data) and produce predictions to another Kafka topic (predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the demo model\n",
    "\n",
    "First we will prepare our model with the Iris dataset so that we can demonstrate the preditions using FastKafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "model = LogisticRegression(random_state=0, max_iter=500).fit(X, y)\n",
    "model.predict(X[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to model the input and prediction messages that will be sent to the Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import NonNegativeFloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisInputData(BaseModel):\n",
    "  sepal_length: NonNegativeFloat = Field(..., example=0.5, description=\"Sepal length in cm\"),\n",
    "  sepal_width: NonNegativeFloat = Field(..., example=0.5, description=\"Sepal width in cm\"),\n",
    "  petal_length: NonNegativeFloat = Field(..., example=0.5, description=\"Petal length in cm\"),\n",
    "  petal_width: NonNegativeFloat = Field(..., example=0.5, description=\"Petal width in cm\")\n",
    "\n",
    "class IrisPredictionData(BaseModel):\n",
    "  species: str = Field(..., example=\"Iris-setosa\", description=\"Predicted species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets prepare our prediction FastKafka app.\n",
    "\n",
    "This time, we will demonstrate the `fastkafka run` CLI functionality so we will export the app_script to .py file and run it using `run_script_and_cancel` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_script = \"\"\"\n",
    "from pydantic import BaseModel, NonNegativeFloat, Field\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from fastkafka.application import FastKafka\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "model = LogisticRegression(random_state=0, max_iter=500).fit(X, y)\n",
    "\n",
    "class IrisInputData(BaseModel):\n",
    "  sepal_length: NonNegativeFloat = Field(..., example=0.5, description=\"Sepal length in cm\"),\n",
    "  sepal_width: NonNegativeFloat = Field(..., example=0.5, description=\"Sepal width in cm\"),\n",
    "  petal_length: NonNegativeFloat = Field(..., example=0.5, description=\"Petal length in cm\"),\n",
    "  petal_width: NonNegativeFloat = Field(..., example=0.5, description=\"Petal width in cm\")\n",
    "\n",
    "class IrisPredictionData(BaseModel):\n",
    "  species: str = Field(..., example=\"Iris-setosa\", description=\"Predicted species\")\n",
    "\n",
    "kafka_app = FastKafka(\n",
    "    bootstrap_servers={bootstrap_servers},\n",
    ")\n",
    "\n",
    "iris_species = {\n",
    "    0: \"Iris-setosa\",\n",
    "    1: \"Iris-versicolor\",\n",
    "    2: \"Iris-virginica\"\n",
    "}\n",
    "\n",
    "@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\", group_id=\"my_group\")\n",
    "async def on_input_data(msg: IrisInputData):\n",
    "    global model\n",
    "    species_class = model.predict([\n",
    "          [msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]\n",
    "        ])[0]\n",
    "    \n",
    "    to_predictions(species_class)\n",
    "\n",
    "\n",
    "@kafka_app.produces(topic=\"predictions\")\n",
    "def to_predictions(species_class: int) -> IrisPredictionData:\n",
    "    prediction = IrisPredictionData(species=iris_species[species_class])\n",
    "    return prediction\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write a new consumer that will print out the predictions received and stop after a given number of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fastkafka_consumer(msgs_count: int, topic: str, bootstrap_servers: str):\n",
    "  kafka_app = FastKafka(bootstrap_servers = bootstrap_servers)\n",
    "\n",
    "  consumed = {\"total_consumed\": 0}\n",
    "\n",
    "  def increment_consumed(consumed = consumed):\n",
    "    consumed[\"total_consumed\"] = consumed[\"total_consumed\"] + 1\n",
    "\n",
    "  @kafka_app.consumes(topic=topic)\n",
    "  async def consume(msg: IrisPredictionData):\n",
    "    print(f\"Got prediction: {msg.species}\")\n",
    "    increment_consumed()\n",
    "\n",
    "  async with kafka_app: \n",
    "    while(True):\n",
    "      await asyncio.sleep(0.1)\n",
    "      if consumed[\"total_consumed\"] >= msgs_count:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write a producer app that will send the iris samples to our LocalKafkaBroker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fastkafka_producer(msgs: List[Data], topic: str, bootstrap_servers: str):\n",
    "  kafka_app = FastKafka(bootstrap_servers = bootstrap_servers)\n",
    "\n",
    "  @kafka_app.produces(topic=topic)\n",
    "  def produce(msg: IrisInputData):\n",
    "    print(f\"Producing data: {msg}\")\n",
    "    return msg\n",
    "\n",
    "  async with kafka_app: \n",
    "    for msg in msgs:\n",
    "      produce(msg)\n",
    "      await asyncio.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets run the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "from fastkafka.testing import run_script_and_cancel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka.testing: Java is already installed.\n",
      "[INFO] fastkafka.testing: Kafka is already installed.\n",
      "[INFO] fastkafka.testing: Starting zookeeper...\n",
      "[INFO] fastkafka.testing: Zookeeper started, sleeping for 5 seconds...\n",
      "[INFO] fastkafka.testing: Starting Kafka broker...\n",
      "[INFO] fastkafka.testing: Kafka broker started, sleeping for 5 seconds...\n",
      "[INFO] fastkafka.testing: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] fastkafka.helpers: create_missing_topics(['input_data', 'predictions']): new_topics = [NewTopic(topic=input_data,num_partitions=1), NewTopic(topic=predictions,num_partitions=1)]\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'predictions'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'predictions'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'predictions': 1}. \n",
      "[INFO] fastkafka.application: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "Producing data: sepal_length=5.8 sepal_width=2.8 petal_length=5.1 petal_width=2.4\n",
      "Got prediction: Iris-virginica\n",
      "Producing data: sepal_length=6.0 sepal_width=2.2 petal_length=4.0 petal_width=1.0\n",
      "Got prediction: Iris-versicolor\n",
      "Producing data: sepal_length=5.5 sepal_width=4.2 petal_length=1.4 petal_width=0.2\n",
      "Got prediction: Iris-setosa\n",
      "Producing data: sepal_length=7.3 sepal_width=2.9 petal_length=6.3 petal_width=1.8\n",
      "Got prediction: Iris-virginica\n",
      "Producing data: sepal_length=5.0 sepal_width=3.4 petal_length=1.5 petal_width=0.2\n",
      "Got prediction: Iris-setosa\n",
      "Producing data: sepal_length=6.3 sepal_width=3.3 petal_length=6.0 petal_width=2.5\n",
      "Got prediction: Iris-virginica\n",
      "Producing data: sepal_length=5.0 sepal_width=3.5 petal_length=1.3 petal_width=0.3\n",
      "Got prediction: Iris-setosa\n",
      "Producing data: sepal_length=6.7 sepal_width=3.1 petal_length=4.7 petal_width=1.5\n",
      "Got prediction: Iris-versicolor\n",
      "Producing data: sepal_length=6.8 sepal_width=2.8 petal_length=4.8 petal_width=1.4\n",
      "Got prediction: Iris-versicolor\n",
      "Producing data: sepal_length=6.1 sepal_width=2.8 petal_length=4.0 petal_width=1.3\n",
      "Got prediction: Iris-versicolor\n",
      "Producing data: sepal_length=6.1 sepal_width=2.6 petal_length=5.6 petal_width=1.4\n",
      "Got prediction: Iris-virginica\n",
      "Producing data: sepal_length=6.4 sepal_width=3.2 petal_length=4.5 petal_width=1.5\n",
      "Got prediction: Iris-versicolor\n",
      "Producing data: sepal_length=6.1 sepal_width=2.8 petal_length=4.7 petal_width=1.2\n",
      "Got prediction: Iris-versicolor\n",
      "Producing data: sepal_length=6.5 sepal_width=2.8 petal_length=4.6 petal_width=1.5\n",
      "Got prediction: Iris-versicolor\n",
      "Producing data: sepal_length=6.1 sepal_width=2.9 petal_length=4.7 petal_width=1.4\n",
      "Got prediction: Iris-versicolor\n",
      "Producing data: sepal_length=4.9 sepal_width=3.6 petal_length=1.4 petal_width=0.1\n",
      "Got prediction: Iris-setosa\n",
      "Producing data: sepal_length=6.0 sepal_width=2.9 petal_length=4.5 petal_width=1.5\n",
      "Got prediction: Iris-versicolor\n",
      "Producing data: sepal_length=5.5 sepal_width=2.6 petal_length=4.4 petal_width=1.2\n",
      "Got prediction: Iris-versicolor\n",
      "Producing data: sepal_length=4.8 sepal_width=3.0 petal_length=1.4 petal_width=0.3\n",
      "Got prediction: Iris-setosa\n",
      "Producing data: sepal_length=5.4 sepal_width=3.9 petal_length=1.3 petal_width=0.4\n",
      "Got prediction: Iris-setosa\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 17067...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 17067 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 16685...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 16685 terminated.\n"
     ]
    }
   ],
   "source": [
    "msgs = [\n",
    "  IrisInputData(     \n",
    "        sepal_length = iris[0],\n",
    "        sepal_width = iris[1],\n",
    "        petal_length = iris[2],\n",
    "        petal_width = iris[3]\n",
    "      )\n",
    "  for iris in shuffle(X, n_samples=20, random_state=0)\n",
    "]\n",
    "\n",
    "async with LocalKafkaBroker() as bootstrap_server:\n",
    "\n",
    "  create_missing_topics(\n",
    "        [\"input_data\", \"predictions\"],\n",
    "        bootstrap_servers=bootstrap_server,\n",
    "        num_partitions=1,\n",
    "    )\n",
    "  \n",
    "  async with asyncer.create_task_group() as tg:\n",
    "    output = tg.soonify(run_script_and_cancel)(\n",
    "            script=app_script.replace(\n",
    "                \"{bootstrap_servers}\",\n",
    "                f'\"{bootstrap_server}\"'\n",
    "            ),\n",
    "            script_file=f\"server.py\",\n",
    "            cmd=\"fastkafka run --num-workers=1 server:kafka_app\",\n",
    "            cancel_after=30,\n",
    "    )\n",
    "\n",
    "    # Start the consumer FastKafka app\n",
    "    consumer_task = tg.soonify(fastkafka_consumer)(\n",
    "      msgs_count=len(msgs), topic=\"predictions\", bootstrap_servers=bootstrap_server\n",
    "    )\n",
    "\n",
    "    # Give the consumer and our app some time to connect to Kafka topic before we start sending\n",
    "    await asyncio.sleep(15)\n",
    "\n",
    "    # Start the producer FastKafka app  \n",
    "    tg.soonify(fastkafka_producer)(\n",
    "      msgs=msgs, topic=\"input_data\", bootstrap_servers=bootstrap_server\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
