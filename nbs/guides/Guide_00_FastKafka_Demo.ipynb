{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastKafka\n",
    "\n",
    "This notebook will demonstrate the capabilities and developed functionalities in FastKafka project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/airtai/fastkafka/blob/64-colab-based-tutorial/nbs/guides/Guide_00_FastKafka_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing fastkafka library\n",
    "\n",
    "To install fastkafka, run: `pip install fastkafka` in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import fastkafka\n",
    "except ImportError:\n",
    "    #!pip install fastkafka==0.1.0\n",
    "    !pip install \"fastkafka @ git+https://github.com/airtai/fastkafka@62aad0a5415b4ddd6bff945a48df235f7ae59af0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastKafka demo\n",
    "\n",
    "Now we will create a fastkafka application containing a Model that will ingest data samples from one Kafka topic (input_data) and produce predictions to another Kafka topic (predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the demo model\n",
    "\n",
    "First we will prepare our model with the Iris dataset so that we can demonstrate the preditions using FastKafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "model = LogisticRegression(random_state=0, max_iter=500).fit(X, y)\n",
    "x = X[[0, 55, -1]]\n",
    "print(x)\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to model the input and prediction messages that will be sent to the Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, NonNegativeFloat, Field\n",
    "\n",
    "\n",
    "class IrisInputData(BaseModel):\n",
    "    sepal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal length in cm\"\n",
    "    )\n",
    "    sepal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal width in cm\"\n",
    "    )\n",
    "    petal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal length in cm\"\n",
    "    )\n",
    "    petal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal width in cm\"\n",
    "    )\n",
    "\n",
    "\n",
    "class IrisPredictionData(BaseModel):\n",
    "    species: str = Field(..., example=\"setosa\", description=\"Predicted species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets prepare our prediction FastKafka app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkafka.application import FastKafka\n",
    "\n",
    "kafka_app = FastKafka()\n",
    "\n",
    "iris_species = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "\n",
    "@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\")\n",
    "async def on_input_data(msg: IrisInputData):\n",
    "    global model\n",
    "    species_class = model.predict(\n",
    "        [[msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]]\n",
    "    )[0]\n",
    "\n",
    "    to_predictions(species_class)\n",
    "\n",
    "\n",
    "@kafka_app.produces(topic=\"predictions\")\n",
    "def to_predictions(species_class: int) -> IrisPredictionData:\n",
    "    prediction = IrisPredictionData(species=iris_species[species_class])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the test by sending a message to the running app that now encapsulates the Iris classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._testing.local_broker: Java is already installed.\n",
      "[INFO] fastkafka._testing.local_broker: But not exported to PATH, exporting...\n",
      "[INFO] fastkafka._testing.local_broker: Kafka is already installed.\n",
      "[INFO] fastkafka._testing.local_broker: But not exported to PATH, exporting...\n",
      "[INFO] fastkafka._testing.local_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.local_broker: zookeeper started, sleeping for 5 seconds...\n",
      "[INFO] fastkafka._testing.local_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.local_broker: kafka started, sleeping for 5 seconds...\n",
      "[INFO] fastkafka._testing.local_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'latest', 'max_poll_records': 100}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'input_data'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'input_data'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'input_data': 1}. \n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'predictions'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'predictions'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'predictions': 1}. \n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 369524...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 369524 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 369158...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 369158 terminated.\n"
     ]
    }
   ],
   "source": [
    "from fastkafka.application import Tester\n",
    "\n",
    "msg = IrisInputData(\n",
    "    sepal_length=0.1,\n",
    "    sepal_width=0.2,\n",
    "    petal_length=0.3,\n",
    "    petal_width=0.4,\n",
    ")\n",
    "\n",
    "# Start Tesr app and create local Kafka broker for testing\n",
    "async with Tester(kafka_app) as tester:\n",
    "    # Send IrisInputData message to input_data topic\n",
    "    await tester.to_input_data(msg)\n",
    "    # Assert that the kafka_app responded with IrisPedictionData in predictions topic\n",
    "    await tester.awaited_mocks.on_predictions.assert_awaited_with(\n",
    "        IrisPredictionData(species=\"setosa\"), timeout=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "We have created a Iris classification model and encapulated it into our fastkafka application.\n",
    "The app will consume the IrisInputData from the `input_data` topic and produce the predictions to `predictions` topic.\n",
    "\n",
    "To test the app we have:\n",
    "1. Created the app\n",
    "1. Started the LocalKafkaBroker\n",
    "2. Started our Tester class which mirrors the developed app topics for testing purpuoses\n",
    "3. Sent IrisInputData message to `input_data` topic\n",
    "4. Asserted and checked that the developed iris classification service has reacted to IrisInputData message "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "The kafka app comes with builtin documentation generation, let's demonstrate that.\n",
    "\n",
    "To generate the documentation programatically you just need to call `.generate_docs()` method of your app. This will generate the *asyncapi* folder in relative path where all your documentation will be saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mcode\u001b[0m EACCESming\u001b[0m \u001b[35midealTree\u001b[0m Completed in 996ms\u001b[0m\u001b[K\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35msyscall\u001b[0m mkdir\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mpath\u001b[0m /usr/lib/node_modules/n\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35merrno\u001b[0m -13\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m Error: EACCES: permission denied, mkdir '/usr/lib/node_modules/n'\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m  [Error: EACCES: permission denied, mkdir '/usr/lib/node_modules/n'] {\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m   errno: -13,\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m   code: 'EACCES',\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m   syscall: 'mkdir',\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m   path: '/usr/lib/node_modules/n'\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m }\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m \n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m The operation was rejected by your operating system.\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m It is likely you do not have the permissions to access this file as the current user\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m \n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m If you believe this might be a permissions issue, please double-check the\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m permissions of the file and its containing directories, or try running\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m the command again as root/Administrator.\n",
      "\u001b[0m\n",
      "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m A complete log of this run can be found in:\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m     /home/tvrtko/.npm/_logs/2023-03-03T17_10_43_175Z-debug-0.log\n",
      "\u001b[0m/bin/bash: line 1: n: command not found\n",
      "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mcode\u001b[0m EACCESming\u001b[0m \u001b[35midealTree\u001b[0m Completed in 38ms\u001b[0m\u001b[K[0m\u001b[K\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35msyscall\u001b[0m rename\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mpath\u001b[0m /usr/lib/node_modules/npm\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35mdest\u001b[0m /usr/lib/node_modules/.npm-qUIFSsiV\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m \u001b[0m\u001b[35merrno\u001b[0m -13\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m Error: EACCES: permission denied, rename '/usr/lib/node_modules/npm' -> '/usr/lib/node_modules/.npm-qUIFSsiV'\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m  [Error: EACCES: permission denied, rename '/usr/lib/node_modules/npm' -> '/usr/lib/node_modules/.npm-qUIFSsiV'] {\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m   errno: -13,\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m   code: 'EACCES',\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m   syscall: 'rename',\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m   path: '/usr/lib/node_modules/npm',\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m   dest: '/usr/lib/node_modules/.npm-qUIFSsiV'\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m }\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m \n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m The operation was rejected by your operating system.\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m It is likely you do not have the permissions to access this file as the current user\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m \n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m If you believe this might be a permissions issue, please double-check the\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m permissions of the file and its containing directories, or try running\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m the command again as root/Administrator.\n",
      "\u001b[0m\n",
      "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m A complete log of this run can be found in:\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[31;40mERR!\u001b[0m\u001b[35m\u001b[0m     /home/tvrtko/.npm/_logs/2023-03-03T17_10_44_871Z-debug-0.log\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! npm install -g n\n",
    "! n lts\n",
    "! npm install -g npm@latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.asyncapi: AsyncAPI generator installed\r\n"
     ]
    }
   ],
   "source": [
    "! fastkafka docs install_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_app_source = \"\"\"\n",
    "from pydantic import BaseModel, NonNegativeFloat, Field\n",
    "\n",
    "class IrisInputData(BaseModel):\n",
    "    sepal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal length in cm\"\n",
    "    )\n",
    "    sepal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal width in cm\"\n",
    "    )\n",
    "    petal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal length in cm\"\n",
    "    )\n",
    "    petal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal width in cm\"\n",
    "    )\n",
    "\n",
    "\n",
    "class IrisPredictionData(BaseModel):\n",
    "    species: str = Field(..., example=\"setosa\", description=\"Predicted species\")\n",
    "    \n",
    "from fastkafka.application import FastKafka\n",
    "\n",
    "kafka_app = FastKafka()\n",
    "\n",
    "iris_species = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\")\n",
    "async def on_input_data(msg: IrisInputData):\n",
    "    global model\n",
    "    species_class = model.predict([\n",
    "          [msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]\n",
    "        ])[0]\n",
    "\n",
    "    to_predictions(species_class)\n",
    "\n",
    "\n",
    "@kafka_app.produces(topic=\"predictions\")\n",
    "def to_predictions(species_class: int) -> IrisPredictionData:\n",
    "    prediction = IrisPredictionData(species=iris_species[species_class])\n",
    "    return prediction\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"application.py\", \"w\") as source:\n",
    "    source.write(kafka_app_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.asyncapi: New async specifications generated at: '/work/fastkafka/nbs/asyncapi/spec/asyncapi.yml'\n",
      "[INFO] fastkafka._components.asyncapi: Async docs generated at 'asyncapi/docs'\n",
      "[INFO] fastkafka._components.asyncapi: Output of '$ npx -y -p @asyncapi/generator ag asyncapi/spec/asyncapi.yml @asyncapi/html-template -o asyncapi/docs --force-write'\u001b[32m\n",
      "\n",
      "Done! ✨\u001b[0m\n",
      "\u001b[33mCheck out your shiny new generated files at \u001b[0m\u001b[35m/work/fastkafka/nbs/asyncapi/docs\u001b[0m\u001b[33m.\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! fastkafka docs generate application:kafka_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs  spec\r\n"
     ]
    }
   ],
   "source": [
    "! ls asyncapi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In docs folder you will find the servable static html file of your documentation. This can also be served using our `fastkafka docs serve` CLI command (more on that in our guides).\n",
    "\n",
    "In spec folder you will find a asyncapi.yml file containing the async API specification of your application. \n",
    "Lets view the generated docs now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkafka.testing import display_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"700\"\n",
       "            src=\"http://localhost:4000\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 370721...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 370721 terminated.\n"
     ]
    }
   ],
   "source": [
    "await display_docs(docs_path=\"asyncapi/docs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
