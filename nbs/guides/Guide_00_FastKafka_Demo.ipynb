{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastKafka\n",
    "\n",
    "This notebook will demonstrate the capabilities and developed functionalities in FastKafka project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/airtai/fastkafka/blob/64-colab-based-tutorial/nbs/guides/Guide_00_FastKafka_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing fastkafka library\n",
    "\n",
    "To install fastkafka, run: `pip install fastkafka` in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import fastkafka\n",
    "except ImportError:\n",
    "    #!pip install fastkafka==0.1.0\n",
    "    !pip install \"fastkafka @ git+https://github.com/airtai/fastkafka@8929c430ef058103390cc59e2864b195890784e7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LocalKafkaBroker\n",
    "\n",
    "To be able to test and demonstrate the use of FastKafka, we have developed a python wrapper for Zookeeper and Kafka broker which is demonstrated here and used later in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkafka.testing import LocalKafkaBroker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start the LocalKafkaBroker\n",
    "\n",
    "When LocalKafkaBroker is started, it checks if there are Java and Kafka installed on the system, if not, it will install them and export them to path as it is necessary for it to function.\n",
    "\n",
    "Note: We use `apply_nest_asyncio=True` when creating the broker in the notebook to enable it to run in a nested async loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_broker = LocalKafkaBroker(apply_nest_asyncio=True)\n",
    "bootstrap_server = local_broker.start()\n",
    "print(bootstrap_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if there are any topics in our fresh Kafka broker. If everything is okay, there should be none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kafka-topics.sh --list --bootstrap-server {bootstrap_server}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now create a topic, list it, and describe it to see that our LocalKafkaBroker is really running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kafka-topics.sh --create --topic quickstart-events --bootstrap-server {bootstrap_server}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kafka-topics.sh --list --bootstrap-server {bootstrap_server}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kafka-topics.sh --describe --topic quickstart-events --bootstrap-server {bootstrap_server}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can stop the broker as it is no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_broker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LocalKafkaBroker can also be used as a context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with LocalKafkaBroker(apply_nest_asyncio=True) as bootstrap_server:\n",
    "    print(bootstrap_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastKafka minimal demo\n",
    "\n",
    "We will first create one simple fastkafka producer and one consumer to demonstrate their capabiltiy to communicate over Kafka queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, model the data that will be communicated between producer and consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic.main import BaseModel\n",
    "\n",
    "from pydantic import BaseModel, Field, NonNegativeInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(BaseModel):\n",
    "    data: NonNegativeInt = Field(..., example=202020, description=\"Sample data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define our FastKafka consumer app and wrap it in a coroutine to be able to run it in the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from fastkafka.application import FastKafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fastkafka_consumer(msgs_count: int, topic: str, bootstrap_servers: str):\n",
    "    with tqdm(total=msgs_count, desc=f\"consuming from '{topic}'\") as consume_pbar:\n",
    "        kafka_app = FastKafka(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "        @kafka_app.consumes(topic=topic)\n",
    "        async def consume(msg: Data):\n",
    "            consume_pbar.update(1)\n",
    "\n",
    "        async with kafka_app:\n",
    "            while True:\n",
    "                await asyncio.sleep(0.1)\n",
    "                if consume_pbar.n >= consume_pbar.total:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same preparation for the FastKafka producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fastkafka_producer(msgs: List[Data], topic: str, bootstrap_servers: str):\n",
    "    with tqdm(total=len(msgs), desc=f\"producing to '{topic}'\") as produce_pbar:\n",
    "        kafka_app = FastKafka(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "        @kafka_app.produces(topic=topic)\n",
    "        def produce(msg: Data):\n",
    "            return msg\n",
    "\n",
    "        async with kafka_app:\n",
    "            for msg in msgs:\n",
    "                produce(msg)\n",
    "                produce_pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets run the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncer\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the messages for the producer to send\n",
    "msgs = [Data(data=i) for i in trange(1_000, desc=\"generating messages\")]\n",
    "\n",
    "# Start the broker\n",
    "async with LocalKafkaBroker(topics=[\"test_data\"], apply_nest_asyncio=True) as bootstrap_server:\n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        # Start the consumer FastKafka app\n",
    "        tg.soonify(fastkafka_consumer)(\n",
    "            msgs_count=len(msgs), topic=\"test_data\", bootstrap_servers=bootstrap_server\n",
    "        )\n",
    "\n",
    "        # Give the consumer some time to connect to Kafka topic before we start sending\n",
    "        await asyncio.sleep(5)\n",
    "\n",
    "        # Start the producer FastKafka app\n",
    "        tg.soonify(fastkafka_producer)(\n",
    "            msgs=msgs, topic=\"test_data\", bootstrap_servers=bootstrap_server\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "We have created a simple FastKafka consumer that updates the progress when a message is received, a producer that sends the messages to the Kafka queue.\n",
    "\n",
    "In the previous cell we have started the LocalKafkaBroker instance and connected our producer and consumer to it. \n",
    "Finall, after the producer started producing the messages, we have consumed them with our consumer and when all of the messages have been consumed the consumer and producer stopped and our kafka broker has exited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastKafka model predictions demo\n",
    "\n",
    "Now we will create a more fleshed out application containing a Model that will ingest data samples from one Kafka topic (input_data) and produce predictions to another Kafka topic (predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the demo model\n",
    "\n",
    "First we will prepare our model with the Iris dataset so that we can demonstrate the preditions using FastKafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "model = LogisticRegression(random_state=0, max_iter=500).fit(X, y)\n",
    "model.predict(X[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to model the input and prediction messages that will be sent to the Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import NonNegativeFloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisInputData(BaseModel):\n",
    "    sepal_length: NonNegativeFloat = (\n",
    "        Field(..., example=0.5, description=\"Sepal length in cm\"),\n",
    "    )\n",
    "    sepal_width: NonNegativeFloat = (\n",
    "        Field(..., example=0.5, description=\"Sepal width in cm\"),\n",
    "    )\n",
    "    petal_length: NonNegativeFloat = (\n",
    "        Field(..., example=0.5, description=\"Petal length in cm\"),\n",
    "    )\n",
    "    petal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal width in cm\"\n",
    "    )\n",
    "\n",
    "\n",
    "class IrisPredictionData(BaseModel):\n",
    "    species: str = Field(..., example=\"Iris-setosa\", description=\"Predicted species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets prepare our prediction FastKafka app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_app():\n",
    "    kafka_app = FastKafka()\n",
    "\n",
    "    iris_species = {\n",
    "        0: \"Iris-setosa\",\n",
    "        1: \"Iris-versicolor\",\n",
    "        2: \"Iris-virginica\"\n",
    "    }\n",
    "\n",
    "    @kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\", group_id=\"my_group\")\n",
    "    async def on_input_data(msg: IrisInputData):\n",
    "        global model\n",
    "        species_class = model.predict([\n",
    "              [msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]\n",
    "            ])[0]\n",
    "\n",
    "        to_predictions(species_class)\n",
    "\n",
    "\n",
    "    @kafka_app.produces(topic=\"predictions\")\n",
    "    def to_predictions(species_class: int) -> IrisPredictionData:\n",
    "        prediction = IrisPredictionData(species=iris_species[species_class])\n",
    "        return prediction\n",
    "    return kafka_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets run the test by sending a message to the running app that now encapsulates the Iris classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkafka.application import Tester\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = create_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Start the broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = LocalKafkaBroker(topics=[\"input_data\", \"predictions\"], apply_nest_asyncio=True)\n",
    "bootstrap_server = broker.start()\n",
    "app.set_bootstrap_servers(bootstrap_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Started our Tester class which mirrors the developed app topics for testing purpuoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = Tester(app)\n",
    "await tester.__aenter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Send a message and see what we get at the predictions topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = IrisInputData(\n",
    "    sepal_length=X[0][0],\n",
    "    sepal_width=X[0][1],\n",
    "    petal_length=X[0][2],\n",
    "    petal_width=X[0][3],\n",
    ")\n",
    "\n",
    "await tester.to_input_data(msg)\n",
    "await tester.awaited_mocks.on_predictions.assert_awaited(timeout=2)\n",
    "print(f\"Received prediction: {tester.mocks.on_predictions.call_args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. To keep everything clean, close the broker and tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await tester.__aexit__(None, None, None)\n",
    "broker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When condensed into one cell, the test looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = create_app()\n",
    "msg = IrisInputData(\n",
    "    sepal_length=X[0][0],\n",
    "    sepal_width=X[0][1],\n",
    "    petal_length=X[0][2],\n",
    "    petal_width=X[0][3],\n",
    ")\n",
    "with LocalKafkaBroker(\n",
    "    topics=[\"input_data\", \"predictions\"], apply_nest_asyncio=True\n",
    ") as bootstrap_servers:\n",
    "    app.set_bootstrap_servers(bootstrap_servers=bootstrap_servers)\n",
    "    tester = Tester(app)\n",
    "    async with tester:\n",
    "        await tester.to_input_data(msg)\n",
    "        await tester.awaited_mocks.on_predictions.assert_awaited(timeout=2)\n",
    "        prediction = tester.mocks.on_predictions.call_args\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(f\"Sent data: {msg}\")\n",
    "print(f\"Received prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "We have created a Iris classification model and encapulated it into our fastkafka application.\n",
    "The app will consume the IrisInputData from the `input_data` topic and produce the predictions to `predictions` topic.\n",
    "\n",
    "To test the app we have:\n",
    "1. Created the app\n",
    "1. Started the LocalKafkaBroker\n",
    "2. Started our Tester class which mirrors the developed app topics for testing purpuoses\n",
    "3. Sent IrisInputData message to `input_data` topic\n",
    "4. Asserted and checked that the developed iris classification service has reacted to IrisInputData message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
