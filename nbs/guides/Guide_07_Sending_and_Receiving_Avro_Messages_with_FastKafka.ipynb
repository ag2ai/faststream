{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eecb8b6",
   "metadata": {},
   "source": [
    "# Sending and Receiving Avro Messages with FastKafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ef914",
   "metadata": {},
   "source": [
    "## What is Avro?\n",
    "\n",
    "Avro is a row-oriented remote procedure call and data serialization framework developed within Apache's Hadoop project. It uses JSON for defining data types and protocols, and serializes data in a compact binary format. To learn more about the Apache Avro, please check out the [docs](https://avro.apache.org/docs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8c300",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "\n",
    "1. A basic knowledge of `FastKafka` is needed to proceed with this guide. If you are not familiar with `FastKafka`, please go through the [tutorial](/docs#tutorial) first.\n",
    "2. `FastKafka` with dependencies for Apache Avro installed is needed. Please install `FastKafka` with Avro support using the command - `pip install FastKafka[avro]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada1754",
   "metadata": {},
   "source": [
    "## Defining Avro Schema Using Pydantic Models\n",
    "\n",
    "\n",
    "By default, you can use Pydantic model to define your message schemas. FastKafka internally takes care of encoding and decoding avro messages, based on the Pydantic models.\n",
    "\n",
    "So, similar to the [tutorial](/docs#tutorial), the message schema will remain as it is.\n",
    "\n",
    "```python\n",
    "# Define Pydantic models for Avro messages\n",
    "from pydantic import BaseModel, NonNegativeFloat, Field\n",
    "\n",
    "class IrisInputData(BaseModel):\n",
    "    sepal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal length in cm\"\n",
    "    )\n",
    "    sepal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal width in cm\"\n",
    "    )\n",
    "    petal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal length in cm\"\n",
    "    )\n",
    "    petal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal width in cm\"\n",
    "    )\n",
    "\n",
    "\n",
    "class IrisPrediction(BaseModel):\n",
    "    species: str = Field(..., example=\"setosa\", description=\"Predicted species\")\n",
    "```\n",
    "\n",
    "No need to change anything to support avro. You can use existing Pydantic models as is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd3c20",
   "metadata": {},
   "source": [
    "## Reusing existing avro schema\n",
    "\n",
    "\n",
    "If you are using some other library to send and receive avro encoded messages, it is highly likely that you already have an Avro schema defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e42789",
   "metadata": {},
   "source": [
    "### Building pydantic models from avro schema dictionary\n",
    "\n",
    "\n",
    "Let's modify the above example and let's assume we have schemas already for `IrisInputData` and `IrisPrediction` which will look like below:\n",
    "\n",
    "```python\n",
    "iris_input_data_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"namespace\": \"IrisInputData\",\n",
    "    \"name\": \"IrisInputData\",\n",
    "    \"fields\": [\n",
    "        {\"doc\": \"Sepal length in cm\", \"type\": \"double\", \"name\": \"sepal_length\"},\n",
    "        {\"doc\": \"Sepal width in cm\", \"type\": \"double\", \"name\": \"sepal_width\"},\n",
    "        {\"doc\": \"Petal length in cm\", \"type\": \"double\", \"name\": \"petal_length\"},\n",
    "        {\"doc\": \"Petal width in cm\", \"type\": \"double\", \"name\": \"petal_width\"},\n",
    "    ],\n",
    "}\n",
    "iris_prediction_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"namespace\": \"IrisPrediction\",\n",
    "    \"name\": \"IrisPrediction\",\n",
    "    \"fields\": [{\"doc\": \"Predicted species\", \"type\": \"string\", \"name\": \"species\"}],\n",
    "}\n",
    "```\n",
    "\n",
    "We can easily construct pydantic models from avro schema using `avsc_to_pydantic` function which is included as part of `FastKafka` itself.\n",
    "\n",
    "```python\n",
    "from fastkafka._components.encoder.avro import avsc_to_pydantic\n",
    "\n",
    "IrisInputData = avsc_to_pydantic(iris_input_data_schema)\n",
    "print(IrisInputData.__fields__)\n",
    "\n",
    "IrisPrediction = avsc_to_pydantic(iris_prediction_schema)\n",
    "print(IrisPrediction.__fields__)\n",
    "```\n",
    "\n",
    "The above code will convert avro schema to pydantic models and will print pydantic models' fields. The output of the above is:\n",
    "\n",
    "```txt\n",
    "{'sepal_length': ModelField(name='sepal_length', type=float, required=True),\n",
    " 'sepal_width': ModelField(name='sepal_width', type=float, required=True),\n",
    " 'petal_length': ModelField(name='petal_length', type=float, required=True),\n",
    " 'petal_width': ModelField(name='petal_width', type=float, required=True)}\n",
    " \n",
    " {'species': ModelField(name='species', type=str, required=True)}\n",
    "```\n",
    "\n",
    "This is exactly same as manually defining the pydantic models ourselves. You don't have to worry about not making any mistakes while converting avro schema to pydantic models manually. You can easily and automatically accomplish it by using `avsc_to_pydantic` function as demonstrated above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f4a6b",
   "metadata": {},
   "source": [
    "### Building pydantic models from `.avsc` file\n",
    "\n",
    "Not all cases will have avro schema conveniently defined as a python dictionary. You may have it stored as the proprietary `.avsc` files in filesystem. Let's see how to convert those `.avsc` files to pydantic models.\n",
    "\n",
    "Let's assume our avro files are stored in files called `iris_input_data_schema.avsc` and `iris_prediction_schema.avsc`. In that case, following code converts the schema to pydantic models:\n",
    "\n",
    "```python\n",
    "import json\n",
    "from fastkafka._components.encoder.avro import avsc_to_pydantic\n",
    "\n",
    "\n",
    "with open(\"iris_input_data_schema.avsc\", \"rb\") as f:\n",
    "    iris_input_data_schema = json.load(f)\n",
    "    \n",
    "with open(\"iris_prediction_schema.avsc\", \"rb\") as f:\n",
    "    iris_prediction_schema = json.load(f)\n",
    "    \n",
    "\n",
    "IrisInputData = avsc_to_pydantic(iris_input_data_schema)\n",
    "print(IrisInputData.__fields__)\n",
    "\n",
    "IrisPrediction = avsc_to_pydantic(iris_prediction_schema)\n",
    "print(IrisPrediction.__fields__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ab3f8",
   "metadata": {},
   "source": [
    "## Consume/Produce avro messages with FastKafka\n",
    "\n",
    "\n",
    "`FastKafka` provides `@consumes` and `@produces` methods to consume/produces messages to/from a `Kafka` topic. This is explained in [tutorial](/docs#function-decorators).\n",
    "\n",
    "The `@consumes` and `@produces` methods accepts a parameter called `decoder`/`encoder` to decode/encode avro messages.\n",
    "\n",
    "```python\n",
    "@kafka_app.consumes(topic=\"input_data\", encoder=\"avro\")\n",
    "async def on_input_data(msg: IrisInputData):\n",
    "    global model\n",
    "    species_class = model.predict(\n",
    "        [[msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]]\n",
    "    )[0]\n",
    "\n",
    "    await to_predictions(species_class)\n",
    "\n",
    "\n",
    "@kafka_app.produces(topic=\"predictions\", decoder=\"avro\")\n",
    "async def to_predictions(species_class: int) -> IrisPrediction:\n",
    "    iris_species = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "    prediction = IrisPrediction(species=iris_species[species_class])\n",
    "    return prediction\n",
    "```\n",
    "\n",
    "In the above example, in `@consumes` and `@produces` methods, we explicitly instruct FastKafka to `decode` and `encode` messages using the `avro` `decoder`/`encoder` instead of the default `json` `decoder`/`encoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fed866",
   "metadata": {},
   "source": [
    "## Assembling it all together\n",
    "\n",
    "Let's rewrite the sample code found in [tutorial](/docs#running-the-service) to use `avro` to `decode` and `encode` messages:\n",
    "\n",
    "```python\n",
    "# content of the \"application.py\" file\n",
    "\n",
    "iris_input_data_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"namespace\": \"IrisInputData\",\n",
    "    \"name\": \"IrisInputData\",\n",
    "    \"fields\": [\n",
    "        {\"doc\": \"Sepal length in cm\", \"type\": \"double\", \"name\": \"sepal_length\"},\n",
    "        {\"doc\": \"Sepal width in cm\", \"type\": \"double\", \"name\": \"sepal_width\"},\n",
    "        {\"doc\": \"Petal length in cm\", \"type\": \"double\", \"name\": \"petal_length\"},\n",
    "        {\"doc\": \"Petal width in cm\", \"type\": \"double\", \"name\": \"petal_width\"},\n",
    "    ],\n",
    "}\n",
    "iris_prediction_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"namespace\": \"IrisPrediction\",\n",
    "    \"name\": \"IrisPrediction\",\n",
    "    \"fields\": [{\"doc\": \"Predicted species\", \"type\": \"string\", \"name\": \"species\"}],\n",
    "}\n",
    "# Or load schema from avsc files\n",
    "\n",
    "from fastkafka._components.encoder.avro import avsc_to_pydantic\n",
    "\n",
    "IrisInputData = avsc_to_pydantic(iris_input_data_schema)\n",
    "IrisPrediction = avsc_to_pydantic(iris_prediction_schema)\n",
    "\n",
    "    \n",
    "from fastkafka import FastKafka\n",
    "\n",
    "kafka_brokers = {\n",
    "    \"localhost\": {\n",
    "        \"url\": \"localhost\",\n",
    "        \"description\": \"local development kafka broker\",\n",
    "        \"port\": 9092,\n",
    "    },\n",
    "    \"production\": {\n",
    "        \"url\": \"kafka.airt.ai\",\n",
    "        \"description\": \"production kafka broker\",\n",
    "        \"port\": 9092,\n",
    "        \"protocol\": \"kafka-secure\",\n",
    "        \"security\": {\"type\": \"plain\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "kafka_app = FastKafka(\n",
    "    title=\"Iris predictions\",\n",
    "    kafka_brokers=kafka_brokers,\n",
    ")\n",
    "\n",
    "iris_species = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "@kafka_app.consumes(topic=\"input_data\", encoder=\"avro\")\n",
    "async def on_input_data(msg: IrisInputData):\n",
    "    global model\n",
    "    species_class = model.predict(\n",
    "        [[msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]]\n",
    "    )[0]\n",
    "\n",
    "    await to_predictions(species_class)\n",
    "\n",
    "\n",
    "@kafka_app.produces(topic=\"predictions\", decoder=\"avro\")\n",
    "async def to_predictions(species_class: int) -> IrisPrediction:\n",
    "    iris_species = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "    prediction = IrisPrediction(species=iris_species[species_class])\n",
    "    return prediction\n",
    "```\n",
    "\n",
    "\n",
    "The above code is a sample implementation of using FastKafka to consume and produce Avro-encoded messages from/to a Kafka topic. The code defines two Avro schemas for the input data and the prediction result. It then uses the `avsc_to_pydantic` function from the FastKafka library to convert the Avro schema into Pydantic models, which will be used to decode and encode Avro messages.\n",
    "\n",
    "The `FastKafka` class is then instantiated with the broker details, and two functions decorated with `@kafka_app.consumes` and `@kafka_app.produces` are defined to consume messages from the \"input_data\" topic and produce messages to the \"predictions\" topic, respectively. The functions uses the decoder=\"avro\" and encoder=\"avro\" parameters to decode and encode the Avro messages.\n",
    "\n",
    "In summary, the above code demonstrates a straightforward way to use Avro-encoded messages with FastKafka to build a message processing pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
