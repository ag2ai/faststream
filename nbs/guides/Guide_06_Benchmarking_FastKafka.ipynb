{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b1b12fd",
   "metadata": {},
   "source": [
    "# Benchmarking FastKafka app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e80d6",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To benchmark a `FastKafka` project, you will need the following:\n",
    "\n",
    "1. A library built with `FastKafka`.\n",
    "2. A running `Kafka` instance to benchmark the FastKafka application against."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a63ab0",
   "metadata": {},
   "source": [
    "### Creating FastKafka Code\n",
    "\n",
    "Let's create a `FastKafka`-based application and write it to the `application.py` file based on the [tutorial](/#tutorial).\n",
    "\n",
    "```python\n",
    "# content of the \"application.py\" file\n",
    "\n",
    "from pydantic import BaseModel, NonNegativeFloat, Field\n",
    "\n",
    "class IrisInputData(BaseModel):\n",
    "    sepal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal length in cm\"\n",
    "    )\n",
    "    sepal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal width in cm\"\n",
    "    )\n",
    "    petal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal length in cm\"\n",
    "    )\n",
    "    petal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal width in cm\"\n",
    "    )\n",
    "\n",
    "\n",
    "class IrisPrediction(BaseModel):\n",
    "    species: str = Field(..., example=\"setosa\", description=\"Predicted species\")\n",
    "\n",
    "from fastkafka import FastKafka\n",
    "\n",
    "kafka_brokers = {\n",
    "    \"localhost\": {\n",
    "        \"url\": \"localhost\",\n",
    "        \"description\": \"local development kafka broker\",\n",
    "        \"port\": 9092,\n",
    "    },\n",
    "    \"production\": {\n",
    "        \"url\": \"kafka.airt.ai\",\n",
    "        \"description\": \"production kafka broker\",\n",
    "        \"port\": 9092,\n",
    "        \"protocol\": \"kafka-secure\",\n",
    "        \"security\": {\"type\": \"plain\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "kafka_app = FastKafka(\n",
    "    title=\"Iris predictions\",\n",
    "    kafka_brokers=kafka_brokers,\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    ")\n",
    "\n",
    "iris_species = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\")\n",
    "async def on_input_data(msg: IrisInputData):\n",
    "    global model\n",
    "    species_class = model.predict([\n",
    "          [msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]\n",
    "        ])[0]\n",
    "\n",
    "    await to_predictions(species_class)\n",
    "\n",
    "\n",
    "@kafka_app.produces(topic=\"predictions\")\n",
    "async def to_predictions(species_class: int) -> IrisPrediction:\n",
    "    prediction = IrisPrediction(species=iris_species[species_class])\n",
    "    return prediction\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "`FastKafka` has a decorator for benchmarking which is appropriately called as `benchmark`.\n",
    "Let's edit our `application.py` file and add the `benchmark` decorator to the consumes method.\n",
    "\n",
    "\n",
    "```python\n",
    "# content of the \"application.py\" file with benchmark\n",
    "\n",
    "from pydantic import BaseModel, NonNegativeFloat, Field\n",
    "\n",
    "class IrisInputData(BaseModel):\n",
    "    sepal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal length in cm\"\n",
    "    )\n",
    "    sepal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal width in cm\"\n",
    "    )\n",
    "    petal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal length in cm\"\n",
    "    )\n",
    "    petal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal width in cm\"\n",
    "    )\n",
    "\n",
    "\n",
    "class IrisPrediction(BaseModel):\n",
    "    species: str = Field(..., example=\"setosa\", description=\"Predicted species\")\n",
    "\n",
    "from fastkafka import FastKafka\n",
    "\n",
    "kafka_brokers = {\n",
    "    \"localhost\": {\n",
    "        \"url\": \"localhost\",\n",
    "        \"description\": \"local development kafka broker\",\n",
    "        \"port\": 9092,\n",
    "    },\n",
    "    \"production\": {\n",
    "        \"url\": \"kafka.airt.ai\",\n",
    "        \"description\": \"production kafka broker\",\n",
    "        \"port\": 9092,\n",
    "        \"protocol\": \"kafka-secure\",\n",
    "        \"security\": {\"type\": \"plain\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "kafka_app = FastKafka(\n",
    "    title=\"Iris predictions\",\n",
    "    kafka_brokers=kafka_brokers,\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    ")\n",
    "\n",
    "iris_species = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\")\n",
    "@kafka_app.benchmark(interval=1, sliding_window_size=5)\n",
    "async def on_input_data(msg: IrisInputData):\n",
    "    global model\n",
    "    species_class = model.predict([\n",
    "          [msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]\n",
    "        ])[0]\n",
    "\n",
    "    await to_predictions(species_class)\n",
    "\n",
    "\n",
    "@kafka_app.produces(topic=\"predictions\")\n",
    "async def to_predictions(species_class: int) -> IrisPrediction:\n",
    "    prediction = IrisPrediction(species=iris_species[species_class])\n",
    "    return prediction\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Here we are conducting a benchmark of a function that consumes data from the `input_data` topic with an interval of 1 second and a sliding window size of 5. This benchmark method uses the `interval` parameter to calculate the results over a specific time period, and the `sliding_window_size parameter` to determine the maximum number of results to use in calculating the average throughput and standard deviation. This benchmark is important to ensure that the function is performing optimally and to identify any areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a460d8d",
   "metadata": {},
   "source": [
    "### Starting Kafka\n",
    "\n",
    "If you already have a `Kafka` running somewhere, then you can skip this step. Please keep in mind that your benchmarking results may be affected by bottlenecks such as network, CPU cores in the Kafka machine, or even the Kafka configuration itself.\n",
    "\n",
    "#### Installing Java and Kafka\n",
    "We need a working `Kafka`instance to benchmark our `FastKafka` app, and to run `Kafka` we need `Java`. Thankfully, `FastKafka` comes with a CLI to install both `Java` and `Kafka` on our machine.\n",
    "So, let's install `Java` and `Kafka` by executing the following command.\n",
    "\n",
    "```cmd\n",
    "fastkafka testing install_deps\n",
    "```\n",
    "\n",
    "The above command will extract `Kafka` scripts at the location \"$HOME/.local/kafka_2.13-3.3.2\" on your machine. \n",
    "\n",
    "\n",
    "#### Creating configuration for Zookeeper and Kafka\n",
    "Now we need to start `Zookeeper` and `Kafka` separately, and to start them we need `zookeeper.properties` and `kafka.properties` files.\n",
    "\n",
    "Let's create a folder inside the folder where `Kafka` scripts were extracted and change directory into it.\n",
    "\n",
    "```cmd\n",
    "mkdir $HOME/.local/kafka_2.13-3.3.2/data_dir && cd $HOME/.local/kafka_2.13-3.3.2/data_dir\n",
    "```\n",
    "\n",
    "Let's create a file called `zookeeper.properties` and write the following content to the file:\n",
    "\n",
    "```txt\n",
    "dataDir=$HOME/.local/kafka_2.13-3.3.2/data_dir/zookeeper\n",
    "clientPort=2181\n",
    "maxClientCnxns=0\n",
    "```\n",
    "\n",
    "Similarly, let's create a file called `kafka.properties` and write the following content to the file:\n",
    "\n",
    "```txt\n",
    "broker.id=0\n",
    "listeners=PLAINTEXT://:9092\n",
    "\n",
    "num.network.threads=3\n",
    "num.io.threads=8\n",
    "socket.send.buffer.bytes=102400\n",
    "socket.receive.buffer.bytes=102400\n",
    "socket.request.max.bytes=104857600\n",
    "\n",
    "num.partitions=1\n",
    "num.recovery.threads.per.data.dir=1\n",
    "offsets.topic.replication.factor=1\n",
    "transaction.state.log.replication.factor=1\n",
    "transaction.state.log.min.isr=1\n",
    "\n",
    "log.dirs=/work/fastkafka/kafka_config/kafka_logs\n",
    "log.flush.interval.messages=10000\n",
    "log.flush.interval.ms=1000\n",
    "log.retention.hours=168\n",
    "log.retention.bytes=1073741824\n",
    "log.segment.bytes=1073741824\n",
    "log.retention.check.interval.ms=300000\n",
    "\n",
    "\n",
    "zookeeper.connect=localhost:2181\n",
    "zookeeper.connection.timeout.ms=18000\n",
    "```\n",
    "\n",
    "\n",
    "#### Starting Zookeeper and Kafka\n",
    "\n",
    "We need two different terminals to run `Zookeeper` in one and `Kafka` in another.\n",
    "Let's open a new terminal and run the following command to start `Zookeeper`\n",
    "\n",
    "```cmd\n",
    "cd $HOME/.local/kafka_2.13-3.3.2/bin && ./zookeeper-server-start.sh ../data_dir/zookeeper.properties\n",
    "```\n",
    "\n",
    "\n",
    "Once `Zookeeper` is up and running, open a new terminal and execute the follwing command to start `Kafka`:\n",
    "```cmd\n",
    "cd $HOME/.local/kafka_2.13-3.3.2/bin && ./kafka-server-start.sh ../data_dir/kafka.properties\n",
    "```\n",
    "\n",
    "Now we have both `Zookeeper` and `Kafka` up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac21518",
   "metadata": {},
   "source": [
    "### Benchmarking FastKafka\n",
    "\n",
    "\n",
    "Once `Zookeeper` and `Kafka` are ready then benchmarking `FastKafka` app is as simple as running the `fastkafka run` command\n",
    "\n",
    "```cmd\n",
    "fastkafka run --num-workers 1 --kafka-broker localhost application:kafka_app\n",
    "```\n",
    "\n",
    "This will start the `FastKafka` app and will start consuming messages from `Kafka` which we spun up earlier. \n",
    "Also the same command will output all the benchmark throughputs based on `interval` and `sliding_window_size`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
