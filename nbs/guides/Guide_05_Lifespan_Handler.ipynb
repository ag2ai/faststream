{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b490915d",
   "metadata": {},
   "source": [
    "# Lifespan Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305342b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "from fastkafka._components.helpers import _import_from_string, change_dir\n",
    "from fastkafka.testing import Tester, LocalKafkaBroker\n",
    "from fastkafka._documentation.helpers import _run_example_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85b1d1",
   "metadata": {},
   "source": [
    "Did you know that you can define some special code that runs before and after your Kafka application? This code will be executed just once, but it covers the whole lifespan of your app! :rocket:\n",
    "\n",
    "Lets break it down:\n",
    "\n",
    "You can define logic (code) that should be executed before the application starts up. This is like a warm-up for your app, getting it ready to consume and produce messages.\n",
    "\n",
    "Similarly, you can define logic (code) that should be executed when the application is shutting down. This is like a cool-down for your app, making sure everything is properly closed and cleaned up.\n",
    "\n",
    "By executing code before consuming and after producing, you cover the entire lifecycle of your application :tada:\n",
    "\n",
    "This is super handy for setting up shared resources that are needed across consumers and producers, like a database connection pool or a machine learning model. And the best part? You can clean up these resources when the app is shutting down!\n",
    "\n",
    "So lets give it a try and see how it can make your Kafka app even more awesome! :muscle:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7acf3cf",
   "metadata": {},
   "source": [
    "## Lifespan example - Iris prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47606657",
   "metadata": {},
   "source": [
    "Let's dive into an example to see how you can leverage the lifecycle handler to solve a common use case. Imagine that you have some machine learning models that need to consume incoming messages and produce response/prediction messages. These models are shared among consumers and producers, which means you don't want to load them for every message.\n",
    "\n",
    "Here's where the lifecycle handler comes to the rescue! By loading the model before the messages are consumed and produced, but only right before the application starts receiving messages, you can ensure that the model is ready to use without compromising the performance of your tests. In the upcoming sections, we'll walk you through how to initialize an Iris species prediction model and use it in your developed application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f7f3b7",
   "metadata": {},
   "source": [
    "### Lifespan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca909ff",
   "metadata": {},
   "source": [
    "You can define this startup and shutdown logic using the lifespan parameter of the FastKafka app, and an async context manager.\n",
    "\n",
    "Let's start with an example and then see it in detail.\n",
    "\n",
    "We create an async function lifespan() with yield like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6a994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from sklearn.datasets import load_iris\n",
       "from sklearn.linear_model import LogisticRegression\n",
       "from contextlib import asynccontextmanager\n",
       "\n",
       "ml_models = {}\n",
       "\n",
       "@asynccontextmanager\n",
       "async def lifespan():\n",
       "    # Load the ML model\n",
       "    X, y = load_iris(return_X_y=True)\n",
       "    ml_models[\"iris_predictor\"] = LogisticRegression(random_state=0, max_iter=500).fit(X, y)\n",
       "    yield\n",
       "    # Clean up the ML models and release the resources\n",
       "    ml_models.clear()\n",
       "    \n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifespan = \"\"\"from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "ml_models = {}\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan():\n",
    "    # Load the ML model\n",
    "    X, y = load_iris(return_X_y=True)\n",
    "    ml_models[\"iris_predictor\"] = LogisticRegression(random_state=0, max_iter=500).fit(X, y)\n",
    "    yield\n",
    "    # Clean up the ML models and release the resources\n",
    "    ml_models.clear()\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "md(f\"```python\\n{lifespan}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebc906",
   "metadata": {},
   "source": [
    "The first thing to notice, is that we are defining an async function with `yield`. This is very similar to Dependencies with `yield`.\n",
    "\n",
    "The first part of the function, before the `yield`, will be executed **before** the application starts.\n",
    "And the part after the `yield` will be executed **after** the application has finished.\n",
    "\n",
    "This lifespan will create an iris_prediction model on application startup and cleanup the references after the app is shutdown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74c00f",
   "metadata": {},
   "source": [
    "### Async context manager\n",
    "\n",
    "Context managers can be used in `with` blocks, our lifespan, for example colud be used like this:\n",
    "\n",
    "```python\n",
    "ml_models = {}\n",
    "async with lifespan():\n",
    "    print(ml_models)\n",
    "```\n",
    "\n",
    "When you create a context manager or an async context manager, what it does is that, before entering the `with` block, it will execute the code before the `yield`, and after exiting the `with` block, it will execute the code after the `yield`.\n",
    "\n",
    "If you want to learn more about context managers and contextlib decorators, please visit [Python official docs](https://docs.python.org/3/library/contextlib.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d3323",
   "metadata": {},
   "source": [
    "## App demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f4163",
   "metadata": {},
   "source": [
    "Lets now create our application using the created lifespan handler.\n",
    "\n",
    "Notice how we passed our lifespan handler to the app when constructing it trough the `lifespan` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ed2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from fastkafka import FastKafka\n",
       "\n",
       "kafka_brokers = {\n",
       "    \"localhost\": {\n",
       "        \"url\": \"localhost\",\n",
       "        \"description\": \"local development kafka broker\",\n",
       "        \"port\": 9092,\n",
       "    },\n",
       "}\n",
       "\n",
       "kafka_app = FastKafka(\n",
       "    title=\"Iris predictions\",\n",
       "    kafka_brokers=kafka_brokers,\n",
       "    lifespan=lifespan,\n",
       ")\n",
       "\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = \"\"\"from fastkafka import FastKafka\n",
    "\n",
    "kafka_brokers = {\n",
    "    \"localhost\": {\n",
    "        \"url\": \"localhost\",\n",
    "        \"description\": \"local development kafka broker\",\n",
    "        \"port\": 9092,\n",
    "    },\n",
    "}\n",
    "\n",
    "kafka_app = FastKafka(\n",
    "    title=\"Iris predictions\",\n",
    "    kafka_brokers=kafka_brokers,\n",
    "    lifespan=lifespan,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "md(f\"```python\\n{app}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ebb39",
   "metadata": {},
   "source": [
    "Lets model the Iris data for our app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea24631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "from pydantic import BaseModel, Field, NonNegativeFloat\n",
       "\n",
       "class IrisInputData(BaseModel):\n",
       "    sepal_length: NonNegativeFloat = Field(\n",
       "        ..., example=0.5, description=\"Sepal length in cm\"\n",
       "    )\n",
       "    sepal_width: NonNegativeFloat = Field(\n",
       "        ..., example=0.5, description=\"Sepal width in cm\"\n",
       "    )\n",
       "    petal_length: NonNegativeFloat = Field(\n",
       "        ..., example=0.5, description=\"Petal length in cm\"\n",
       "    )\n",
       "    petal_width: NonNegativeFloat = Field(\n",
       "        ..., example=0.5, description=\"Petal width in cm\"\n",
       "    )\n",
       "\n",
       "\n",
       "class IrisPrediction(BaseModel):\n",
       "    species: str = Field(..., example=\"setosa\", description=\"Predicted species\")\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model = \"\"\"\n",
    "from pydantic import BaseModel, Field, NonNegativeFloat\n",
    "\n",
    "class IrisInputData(BaseModel):\n",
    "    sepal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal length in cm\"\n",
    "    )\n",
    "    sepal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal width in cm\"\n",
    "    )\n",
    "    petal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal length in cm\"\n",
    "    )\n",
    "    petal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal width in cm\"\n",
    "    )\n",
    "\n",
    "\n",
    "class IrisPrediction(BaseModel):\n",
    "    species: str = Field(..., example=\"setosa\", description=\"Predicted species\")\n",
    "\"\"\"\n",
    "\n",
    "md(f\"```python\\n{data_model}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab2c5c",
   "metadata": {},
   "source": [
    "Lets create a consumer and producer for our app that will generate predictions from input iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aee2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\")\n",
       "async def on_input_data(msg: IrisInputData):\n",
       "    species_class = ml_models[\"iris_predictor\"].predict(\n",
       "        [[msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]]\n",
       "    )[0]\n",
       "\n",
       "    to_predictions(species_class)\n",
       "\n",
       "\n",
       "@kafka_app.produces(topic=\"predictions\")\n",
       "def to_predictions(species_class: int) -> IrisPrediction:\n",
       "    iris_species = [\"setosa\", \"versicolor\", \"virginica\"]\n",
       "\n",
       "    prediction = IrisPrediction(species=iris_species[species_class])\n",
       "    return prediction\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producers_and_consumers = \"\"\"@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\")\n",
    "async def on_input_data(msg: IrisInputData):\n",
    "    species_class = ml_models[\"iris_predictor\"].predict(\n",
    "        [[msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]]\n",
    "    )[0]\n",
    "\n",
    "    to_predictions(species_class)\n",
    "\n",
    "\n",
    "@kafka_app.produces(topic=\"predictions\")\n",
    "def to_predictions(species_class: int) -> IrisPrediction:\n",
    "    iris_species = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "    prediction = IrisPrediction(species=iris_species[species_class])\n",
    "    return prediction\n",
    "\"\"\"\n",
    "\n",
    "md(f\"```python\\n{producers_and_consumers}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47123e0",
   "metadata": {},
   "source": [
    "The final app looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5969a922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "from pydantic import BaseModel, Field, NonNegativeFloat\n",
       "\n",
       "class IrisInputData(BaseModel):\n",
       "    sepal_length: NonNegativeFloat = Field(\n",
       "        ..., example=0.5, description=\"Sepal length in cm\"\n",
       "    )\n",
       "    sepal_width: NonNegativeFloat = Field(\n",
       "        ..., example=0.5, description=\"Sepal width in cm\"\n",
       "    )\n",
       "    petal_length: NonNegativeFloat = Field(\n",
       "        ..., example=0.5, description=\"Petal length in cm\"\n",
       "    )\n",
       "    petal_width: NonNegativeFloat = Field(\n",
       "        ..., example=0.5, description=\"Petal width in cm\"\n",
       "    )\n",
       "\n",
       "\n",
       "class IrisPrediction(BaseModel):\n",
       "    species: str = Field(..., example=\"setosa\", description=\"Predicted species\")\n",
       "from sklearn.datasets import load_iris\n",
       "from sklearn.linear_model import LogisticRegression\n",
       "from contextlib import asynccontextmanager\n",
       "\n",
       "ml_models = {}\n",
       "\n",
       "@asynccontextmanager\n",
       "async def lifespan():\n",
       "    # Load the ML model\n",
       "    X, y = load_iris(return_X_y=True)\n",
       "    ml_models[\"iris_predictor\"] = LogisticRegression(random_state=0, max_iter=500).fit(X, y)\n",
       "    yield\n",
       "    # Clean up the ML models and release the resources\n",
       "    ml_models.clear()\n",
       "    \n",
       "from fastkafka import FastKafka\n",
       "\n",
       "kafka_brokers = {\n",
       "    \"localhost\": {\n",
       "        \"url\": \"localhost\",\n",
       "        \"description\": \"local development kafka broker\",\n",
       "        \"port\": 9092,\n",
       "    },\n",
       "}\n",
       "\n",
       "kafka_app = FastKafka(\n",
       "    title=\"Iris predictions\",\n",
       "    kafka_brokers=kafka_brokers,\n",
       "    lifespan=lifespan,\n",
       ")\n",
       "\n",
       "@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\")\n",
       "async def on_input_data(msg: IrisInputData):\n",
       "    species_class = ml_models[\"iris_predictor\"].predict(\n",
       "        [[msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]]\n",
       "    )[0]\n",
       "\n",
       "    to_predictions(species_class)\n",
       "\n",
       "\n",
       "@kafka_app.produces(topic=\"predictions\")\n",
       "def to_predictions(species_class: int) -> IrisPrediction:\n",
       "    iris_species = [\"setosa\", \"versicolor\", \"virginica\"]\n",
       "\n",
       "    prediction = IrisPrediction(species=iris_species[species_class])\n",
       "    return prediction\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_app = data_model + lifespan + app + producers_and_consumers\n",
    "md(f\"```python\\n{complete_app}\\n```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b77444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n",
      "[INFO] fastkafka._testing.local_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.local_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.local_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'latest', 'max_poll_records': 100}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'input_data'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'input_data'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'input_data': 1}. \n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'predictions'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'predictions'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'predictions': 1}. \n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 225332...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 225332 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 224971...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 224971 terminated.\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "from pydantic import BaseModel, Field, NonNegativeFloat\n",
    "\n",
    "\n",
    "class IrisInputData(BaseModel):\n",
    "    sepal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal length in cm\"\n",
    "    )\n",
    "    sepal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Sepal width in cm\"\n",
    "    )\n",
    "    petal_length: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal length in cm\"\n",
    "    )\n",
    "    petal_width: NonNegativeFloat = Field(\n",
    "        ..., example=0.5, description=\"Petal width in cm\"\n",
    "    )\n",
    "\n",
    "\n",
    "class IrisPrediction(BaseModel):\n",
    "    species: str = Field(..., example=\"setosa\", description=\"Predicted species\")\n",
    "        \n",
    "msg = IrisInputData(\n",
    "    sepal_length=0.1,\n",
    "    sepal_width=0.2,\n",
    "    petal_length=0.3,\n",
    "    petal_width=0.4,\n",
    ")\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    src_path = Path(d) / \"main.py\"\n",
    "    with open(src_path, \"w\") as source:\n",
    "        source.write(complete_app)\n",
    "    with change_dir(d):\n",
    "        kafka_app = _import_from_string(f\"{src_path.stem}:kafka_app\")\n",
    "        \n",
    "async with Tester(kafka_app) as tester:\n",
    "    # Send IrisInputData message to input_data topic\n",
    "    await tester.to_input_data(msg)\n",
    "\n",
    "    # Assert that the kafka_app responded with IrisPrediction in predictions topic\n",
    "    await tester.awaited_mocks.on_predictions.assert_awaited_with(\n",
    "        IrisPrediction(species=\"setosa\"), timeout=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2509682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Now we can run the app with your custom lifespan handler. Copy the code above in lifespan_example.py and run it by running\n",
       "```shell\n",
       "fastkafka run --num-workers=1 --kafka-broker=localhost lifespan_example:kafka_app\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_file = \"lifespan_example.py\"\n",
    "cmd = \"fastkafka run --num-workers=1 --kafka-broker=localhost lifespan_example:kafka_app\"\n",
    "md(\n",
    "    f\"Now we can run the app with your custom lifespan handler. Copy the code above in lifespan_example.py and run it by running\\n```shell\\n{cmd}\\n```\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a53cc7",
   "metadata": {},
   "source": [
    "When you run the app, you should see a simmilar output to the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6a823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._testing.local_broker: LocalKafkaBroker.start(): entering...\n",
      "[WARNING] fastkafka._testing.local_broker: LocalKafkaBroker.start(): (<_UnixSelectorEventLoop running=True closed=False debug=False>) is already running!\n",
      "[WARNING] fastkafka._testing.local_broker: LocalKafkaBroker.start(): calling nest_asyncio.apply()\n",
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._testing.local_broker: Starting zookeeper...\n",
      "[INFO] fastkafka._testing.local_broker: Starting kafka...\n",
      "[INFO] fastkafka._testing.local_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] fastkafka._testing.local_broker: <class 'fastkafka.testing.LocalKafkaBroker'>.start(): returning 127.0.0.1:9092\n",
      "[INFO] fastkafka._testing.local_broker: LocalKafkaBroker.start(): exited.\n",
      "[INFO] fastkafka._testing.local_broker: LocalKafkaBroker.stop(): entering...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 226814...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 226814 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 226454...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 226454 terminated.\n",
      "[INFO] fastkafka._testing.local_broker: LocalKafkaBroker.stop(): exited.\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "with LocalKafkaBroker(\n",
    "    topicas=[\"hello_world\"], apply_nest_asyncio=True\n",
    ") as bootstrap_server:\n",
    "    exit_code, output = await _run_example_app(\n",
    "        app_example=complete_app, bootstrap_server=bootstrap_server, script_file=script_file, cmd=cmd\n",
    "    )\n",
    "    assert exit_code == 0, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e5eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[227234]: [INFO] fastkafka._application.app: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'\n",
      "[227234]: [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': 'localhost:9092'}'\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'localhost:9092', 'auto_offset_reset': 'latest', 'max_poll_records': 100}\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[227234]: [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'input_data'})\n",
      "[227234]: [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'input_data'}\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[227234]: [WARNING] aiokafka.cluster: Topic input_data is not available during auto-create initialization\n",
      "[227234]: [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'input_data': 0}. \n",
      "Starting process cleanup, this may take a few seconds...\n",
      "[INFO] fastkafka._server: terminate_asyncio_process(): Terminating the process 227234...\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[227234]: [INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._server: terminate_asyncio_process(): Process 227234 terminated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192a4f5",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "In this guide we have defined a lifespan handler and passed to our FastKafka app.\n",
    "\n",
    "Some important points are:\n",
    "1. Lifespan handler is implemented as [AsyncContextManager](https://docs.python.org/3/library/contextlib.html#contextlib.asynccontextmanager)\n",
    "2. Code **before** yield in lifespan will be executed **before** application **startup**\n",
    "3. Code **after** yield in lifespan will be executed **after** application **shutdown**\n",
    "4. You can pass your lifespan handler to FastKafka app on initialisation by passing a `lifespan` argument"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
